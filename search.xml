<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Understanding Fuzz Testing</title>
      <link href="/2024/07/Java/FuzzTest/"/>
      <url>/2024/07/Java/FuzzTest/</url>
      
        <content type="html"><![CDATA[<h2 id="Understanding-Fuzz-Testing"><a href="#Understanding-Fuzz-Testing" class="headerlink" title="Understanding Fuzz Testing"></a>Understanding Fuzz Testing</h2><p>Recently, I learned about a new concept - <strong>Fuzz Testing</strong>. The main idea of fuzzing is to create unexpected inputs to check if the program handles exception cases correctly.</p><h3 id="Why-did-I-learn-about-this-keyword"><a href="#Why-did-I-learn-about-this-keyword" class="headerlink" title="Why did I learn about this keyword?"></a>Why did I learn about this keyword?</h3><p>This stems from a situation where I had a piece of code attempting to pass an input (a string) to create an object:</p><pre><code class="java">new javax.mail.internet.InternetAddress(str)</code></pre><p>and it threw an error with the message “Domain contains illegal character”.</p><p>The problem is that before I passed the value <code>str</code>, there were basic validation logics already in place. Basic cases would definitely be filtered out and fail before being passed to <code>InternetAddress</code>. However, on production, I still encountered this error. The task now is to find out what value of <code>str</code> caused this error.</p><h3 id="The-tool-I-used"><a href="#The-tool-I-used" class="headerlink" title="The tool I used"></a>The tool I used</h3><p>I used the following tool: <a href="https://github.com/CodeIntelligenceTesting/jazzer">jazzer</a></p><p>In my opinion, this tool takes some time for the first experience. The way to run it is simple, but understanding how to run it is complicated.</p><h3 id="How-to-run"><a href="#How-to-run" class="headerlink" title="How to run"></a>How to run</h3><h4 id="1-Declare-Maven-dependency"><a href="#1-Declare-Maven-dependency" class="headerlink" title="1. Declare Maven dependency"></a>1. Declare Maven dependency</h4><pre><code class="xml">&lt;dependency&gt;    &lt;groupId&gt;com.code-intelligence&lt;/groupId&gt;    &lt;artifactId&gt;jazzer-junit&lt;/artifactId&gt;    &lt;version&gt;0.22.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><h4 id="2-Write-the-test-code"><a href="#2-Write-the-test-code" class="headerlink" title="2. Write the test code"></a>2. Write the test code</h4><pre><code class="java">import jakarta.mail.internet.AddressException;import jakarta.mail.internet.InternetAddress;import com.code_intelligence.jazzer.api.FuzzedDataProvider;public class MyFuzzTest &#123;    public static void fuzzerTestOneInput(FuzzedDataProvider data) &#123;        String input = data.consumeRemainingAsString();        if (input == null || input.isEmpty()) &#123;            return;        &#125;        MailAddress m;        try &#123;            m = new MailAddress(input);        &#125; catch (AddressException e) &#123;            return;        &#125;        try &#123;            new InternetAddress(input);        &#125; catch (Exception e) &#123;            if (e.getMessage().contains(&quot;Domain contains illegal character&quot;)) &#123;                System.out.println(&quot;====&quot;);                System.out.println(input);                System.out.println(&quot;====&quot;);                throw new RuntimeException(e);            &#125;        &#125;    &#125;&#125;</code></pre><h4 id="3-Download-Jazzer"><a href="#3-Download-Jazzer" class="headerlink" title="3. Download Jazzer"></a>3. Download Jazzer</h4><p>First, download Jazzer from: <a href="https://github.com/CodeIntelligenceTesting/jazzer/releases">jazzer releases</a></p><p>For example:</p><pre><code class="sh">wget https://github.com/CodeIntelligenceTesting/jazzer/releases/download/v0.22.1/jazzer-macos.tar.gztar -zxvf jazzer-macos.tar.gzchmod +x jazzer-macos.tar.gz</code></pre><h4 id="4-Run-Fuzz-Test"><a href="#4-Run-Fuzz-Test" class="headerlink" title="4. Run Fuzz Test"></a>4. Run Fuzz Test</h4><p>Use one of the following commands to run:</p><pre><code class="sh">./jazzer --cp=$(cat classpath.txt):target/classes:target/test-classes --target_class=org.apache.james.core.MyFuzzTest./jazzer --cp=target/classes:target/test-classes --target_class=org.apache.james.core.MyFuzzTest</code></pre><h3 id="Experience-with-other-tools"><a href="#Experience-with-other-tools" class="headerlink" title="Experience with other tools"></a>Experience with other tools</h3><p>Before successfully experiencing jazzer, I used another tool called Berkeley CS JQF. I thought that the tools would be similar and there wouldn’t be much difference. However, when running with the JQF Maven plugin, I spent many days and my code still didn’t produce any result, even when I left it running on a server.</p><p>But with Jazzer, it really surprised me. It only took about ten minutes to produce the first result.</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> fuzz </tag>
            
            <tag> testing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis sentinel note</title>
      <link href="/2024/07/Redis/RedisSentinelNote/"/>
      <url>/2024/07/Redis/RedisSentinelNote/</url>
      
        <content type="html"><![CDATA[<h1 id="Redis-Sentinel"><a href="#Redis-Sentinel" class="headerlink" title="Redis Sentinel"></a>Redis Sentinel</h1><p>Redis Sentinel is another model of Redis, and as of now, I am aware of at least the following Redis models:</p><ul><li><strong>Standalone</strong>: The simplest model with just one Redis node.</li><li><strong>Master-Replica</strong>: More complex, with one master node and N replica nodes. This model mainly reduces the load on the master node as clients can be configured to read from any node. However, this model does not support failover, meaning if the master node goes down, the system cannot write data.</li><li><strong>Redis Cluster</strong>: Modern and advanced but requires a minimum of six nodes (3 masters and 3 replicas). It supports failover.</li><li><strong>Sentinel</strong>: Possibly an older model compared to Redis Cluster. It requires independent Sentinel nodes acting as coordinators, helping clients discover the system and designating a new master node if the old one fails.</li></ul><p>I am still debating whether to choose Redis Cluster or Redis Sentinel. From an end-user perspective, they seem quite similar, but Redis Cluster feels more “trendy”. It reminds me of the Kafka architecture I experienced a few years ago. Initially, Kafka had a node called Zookeeper (or bootstrapper, broker, etc.) acting as a coordinator, but later Kafka evolved, and this node was removed or integrated directly into the Kafka worker nodes.</p><p>For a production environment, I think deploying one Sentinel node, one master node, and one replica might be sufficient for a small setup. However, the Redis official documentation suggests a simple architecture should include three Sentinel nodes, one master, and two replicas. Their reasoning includes concerns about voting algorithms and the fact that “the more nodes, the better the failover”.</p><h2 id="Deployment-Experience"><a href="#Deployment-Experience" class="headerlink" title="Deployment Experience"></a>Deployment Experience</h2><p>Recently, I had an experience deploying Redis Sentinel for a client. I set up a simple Docker Compose lab to run locally. The sample source code is available here:</p><p><a href="https://github.com/vttranlina/james-project/blob/149595da247dfb915ecb60d239edf627616916ae/server/mailet/rate-limiter-redis/docker-compose-sample/docker-compose-with-redis-sentinel.yml">Sample Docker Compose with Redis Sentinel</a></p><h3 id="Key-Points"><a href="#Key-Points" class="headerlink" title="Key Points"></a>Key Points</h3><ul><li>When mounting <code>sentinel.conf</code> and <code>redis.conf</code> files into Redis, it’s best to do it as shown in the Docker entrypoint file. If mounted directly into the volume, Redis will encounter permission issues when it attempts to rewrite these config files during failover events.</li></ul><h2 id="Redis-URI-Syntax-with-Lettuce"><a href="#Redis-URI-Syntax-with-Lettuce" class="headerlink" title="Redis URI Syntax with Lettuce"></a>Redis URI Syntax with Lettuce</h2><p>Refer to the Redis URI syntax when using the Lettuce library:<br><a href="https://github.com/redis/lettuce/wiki/Redis-URI-and-connection-details">Redis URI and Connection Details</a></p><p>There are essentially two types of URLs: <code>redis:</code> and <code>redis-sentinel:</code>, meaning there is no special URL for clusters or master-replica setups. In some of my labs, clusters and master-replica setups require multiple <code>redis:</code> URLs.</p><p>Here is an example Redis URL for Sentinel:</p><pre><code>redisURL=redis-sentinel://secret1@sentinel-1:26379,sentinel-2:26379,sentinel-3:26379?sentinelMasterId=mymaster</code></pre><h3 id="Configuration-Example"><a href="#Configuration-Example" class="headerlink" title="Configuration Example"></a>Configuration Example</h3><p>Sample <code>sentinel.conf</code> can be found here: <a href="https://download.redis.io/redis-stable/sentinel.conf">Sentinel Configuration</a></p><h3 id="Verification-Commands"><a href="#Verification-Commands" class="headerlink" title="Verification Commands"></a>Verification Commands</h3><p>To verify the Redis setup:</p><h4 id="Master-Node"><a href="#Master-Node" class="headerlink" title="Master Node"></a>Master Node</h4><p>To verify the master node setup, run the following command:</p><pre><code class="bash">redis-cli INFO replication</code></pre><p>Expected output:</p><pre><code class="angular2html">role:masterconnected_slaves:2</code></pre><h4 id="Replica-Nodes"><a href="#Replica-Nodes" class="headerlink" title="Replica Nodes"></a>Replica Nodes</h4><p>To verify the replica nodes setup, run the following command on each replica node:<br><code>redis-cli -a $&#123;REDIS_PASS&#125; INFO replication</code><br>Expected output:</p><pre><code class="angular2html">role:slavemaster_host:$&#123;REDIS_MASTER_HOST&#125;master_port:$&#123;REDIS_MASTER_PORT&#125;master_link_status:up</code></pre><h4 id="Sentinel-Nodes"><a href="#Sentinel-Nodes" class="headerlink" title="Sentinel Nodes"></a>Sentinel Nodes</h4><p>To verify the sentinel nodes setup, monitor the logs on the Redis master and replica nodes (enable debug level logging for more details). Then, run the following command on any sentinel node:</p><p><code>redis-cli -p 26379 -a $&#123;REDIS_PASS&#125; SENTINEL masters</code></p><p>The sentinel will rewrite the <code>/usr/local/etc/redis/sentinel.conf</code> file when discovering the replicas&#x2F;other sentinel nodes. Then we can check the configuration file to see the discovered nodes.</p><p>Run command: <code>cat /usr/local/etc/redis/sentinel.conf</code></p><p>The end of the file should look like this:</p><pre><code>sentinel known-replica mymaster 172.19.0.5 6379sentinel known-replica mymaster 172.19.0.3 6379sentinel known-sentinel mymaster 172.19.0.7 26379 bdfe331304c1dc737226deff780a18d38e4f9b55sentinel known-sentinel mymaster 172.19.0.6 26379 c8ba86a48859ef6329538f71eadcaed30aba638b</code></pre>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> redis sentinel </tag>
            
            <tag> sentinel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis cluster note</title>
      <link href="/2024/04/Redis/RedisClusterNote/"/>
      <url>/2024/04/Redis/RedisClusterNote/</url>
      
        <content type="html"><![CDATA[<h1 id="Redis-cluster"><a href="#Redis-cluster" class="headerlink" title="Redis cluster"></a>Redis cluster</h1><ul><li>docker compose lab: <a href="https://github.com/vttranlina/redis-cluster-demo">https://github.com/vttranlina/redis-cluster-demo</a></li><li>We need to pay attention to the <code>cluster-node-timeout</code> parameter in the configuration file when starting redis-cluster.<br>For example: <code>cluster-node-timeout = 60000</code>, meaning when a node in the cluster goes down, it takes up to 60 seconds for the remaining nodes to confirm that the entire cluster is down. Within 1-60 seconds after node 1 goes down, the remaining nodes still have a normal status.</li></ul><h3 id="1-redis-cluster-3-node-master-0-node-replicas"><a href="#1-redis-cluster-3-node-master-0-node-replicas" class="headerlink" title="1. redis cluster: 3 node master, 0 node replicas"></a>1. redis cluster: 3 node master, 0 node replicas</h3><p>(a requirement for building a cluster is to have a minimum of 3 master nodes):<br>Example before any node go down:</p><ul><li><code>key1</code> is stored on <code>node1</code></li><li><code>key2</code> -&gt; <code>node2</code></li><li><code>key3</code> on <code>node3</code>.</li></ul><p>When <code>node1</code> goes down:</p><ul><li>Within the first 1-60 seconds after going down:<ul><li>Clients cannot read or write <code>key1</code> (waiting).</li><li>Clients can read and write <code>key2</code> and <code>key3</code> normally.</li><li>When client writes new <code>key4</code>:<ul><li>If the “Client-Side Sharding” algorithm on the client side calculates that key4 should be stored on node1, it will be waiting.</li><li>Otherwise, if it calculates that key4 should be stored on node2 or node3, it will be successfully written (reading afterwards is also successful).</li></ul></li></ul></li><li>After 60 seconds, nodes 2 and 3 confirm that the entire cluster is down. At this point, no data can be read or written.</li></ul><h3 id="2-redis-cluster-3-node-master-3-node-replicas"><a href="#2-redis-cluster-3-node-master-3-node-replicas" class="headerlink" title="2. redis cluster: 3 node master, 3 node replicas"></a>2. redis cluster: 3 node master, 3 node replicas</h3><p>Scenario sample: </p><pre><code>Node1 (master) - Node4 (replica)Node2 (master) - Node5 (replica)Node3 (master) - Node6 (replica)</code></pre><ul><li>When node 1 goes down:<ul><li>first 1-60 seconds: similar to the scenario of 3 master nodes above.</li><li>After 60 seconds: node4 automatically becomes the master. Reading and writing any data return to normal.</li></ul></li></ul><p>During this time, monitoring the Redis logs, there will be logs like:</p><pre><code>Cluster state changed: fail....Cluster state changed: ok</code></pre>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> redis cluster </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker - Healthcheck &amp; Graceful Shutdown</title>
      <link href="/2023/08/Docker/Docker_Healthcheck/"/>
      <url>/2023/08/Docker/Docker_Healthcheck/</url>
      
        <content type="html"><![CDATA[<h1 id="Healthcheck"><a href="#Healthcheck" class="headerlink" title="Healthcheck"></a>Healthcheck</h1><h2 id="Why"><a href="#Why" class="headerlink" title="Why?"></a>Why?</h2><ul><li>monitor</li><li>zero downtime deployment</li><li>trigger something base on health status</li></ul><h2 id="Healthcheck-in-Dockerfile"><a href="#Healthcheck-in-Dockerfile" class="headerlink" title="Healthcheck in Dockerfile"></a>Healthcheck in Dockerfile</h2><p>example</p><pre><code class="Dockerfile">FROM nginx:1.17.7RUN apt-get update &amp;&amp; apt-get install -y curlHEALTHCHECK --interval=2s --timeout=2s CMD curl -f http://james:8000/domains || exit 1</code></pre><p>ref: <a href="https://docs.docker.com/engine/reference/builder/#healthcheck">https://docs.docker.com/engine/reference/builder/#healthcheck</a></p><pre><code>The options that can appear before CMD are:--interval=DURATION (default: 30s)--timeout=DURATION (default: 30s)--start-period=DURATION (default: 0s)--retries=N (default: 3)</code></pre><ul><li>The command’s exit status indicates the health status of the container. The possible values are:</li></ul><pre><code>0: success - the container is healthy and ready for use1: unhealthy - the container is not working correctly2: reserved - do not use this exit code</code></pre><h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo:"></a>Demo:</h3><p>  <a href="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/1DockerfileHealthcheckExample.mp4"><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/1DockerfileHealthcheckExample.png" alt="height:500px"></a></p><h2 id="Healthcheck-in-docker-compose"><a href="#Healthcheck-in-docker-compose" class="headerlink" title="Healthcheck in docker-compose"></a>Healthcheck in docker-compose</h2><ul><li>Declare: <img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/2DockercomposeHealthcheck.png" alt="h:600px bg right"></li><li>More example:</li></ul><pre><code class="yaml">    healthcheck:      test: wget --quiet --tries=1 --spider http://localhost:$&#123;PORT&#125; || exit 1z      interval: 30s      timeout: 10s      retries: 5</code></pre><pre><code class="yaml">    healthcheck:      test: echo &#39;db.runCommand(&quot;ping&quot;).ok&#39; | mongo db:27017/speech-api --quiet</code></pre><pre><code class="yaml">    healthcheck:      test: [&quot;CMD&quot;, &quot;redis-cli&quot;,&quot;ping&quot;]</code></pre><ul><li><code>docker compose ps</code>:<br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/2DockercomposePs.png"></li><li>Get healthcheck log by inspect docker:<br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/2CommandGetLogHealthCheck.png"></li></ul><h3 id="Demo-1"><a href="#Demo-1" class="headerlink" title="Demo"></a>Demo</h3><ul><li>Demo:<br><a href="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/2DockerComposeHealthCheck.mp4"><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/2DockerComposeHealthCheck.png" alt="height:500px"></a></li></ul><h1 id="Graceful-shutdown"><a href="#Graceful-shutdown" class="headerlink" title="Graceful shutdown"></a>Graceful shutdown</h1><h2 id="Why-1"><a href="#Why-1" class="headerlink" title="Why?"></a>Why?</h2><ul><li>Allows the service to complete any in-progress tasks</li><li>Clean up any resources it is using</li><li>Notify other services that it is going offline.</li><li>Help prevent data loss</li><li>Reduce downtime</li><li>Avoid cascading failures that could affect other services</li></ul><h2 id="Mechanism"><a href="#Mechanism" class="headerlink" title="Mechanism"></a>Mechanism</h2><ul><li><p>When you kill a PID on linux. Your application will receive a <code>signal</code></p></li><li><p>Base on each <code>signal</code>, Your application should handler it</p></li><li><p>3 Signal we should care:</p><ul><li>SIGTERM: The SIGTERM signal requests a process to stop running. The process is given time to gracefully shut down.</li><li>SIGKILL: The SIGKILL signal forces the process to stop executing immediately. The program cannot ignore this signal.</li><li>SIGINT: The SIGINT signal is the same as pressing ctrl-c. On some systems, “delete” + “break” sends the same signal to the process. The process is given time to gracefully shut down.</li></ul></li><li><p>Total 64 signal types:<br><img src="https://www.saintlad.com/wp-content/uploads/2022/11/image4-13.png" alt="h:300"></p></li><li><p>For more info: <a href="https://www.linux.org/threads/kill-commands-and-signals.8881/">https://www.linux.org/threads/kill-commands-and-signals.8881/</a></p></li><li><p>Docker And The PID <code>1</code>: it will receive termination signals</p></li><li><p><code>docker stop</code> command sends <code>SIGTERM</code> signal to the <code>PID 1</code>. The <code>PID 1</code> is given 30 seconds to shut down, if it does not shut down in 30 seconds, then docker sends a <code>SIGKILL</code> signal which stops the process immediately. By default, stop waits 10 seconds for the container to exit before sending <code>SIGKILL</code>.</p></li><li><p>Snapshoot from checking pid on <code>apache/james:memory-latest</code> container:<br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/3GracefulPid1Check.png"><br>&#x3D;&gt; java app run on PID &#x3D; <code>1</code> &#x3D;&gt; correct way</p></li><li><p>Notice when creating a Dockerfile<br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/3Ref_DockerPid1_NodeVsNPM.png"></p></li></ul><h2 id="James"><a href="#James" class="headerlink" title="James"></a>James</h2><p>Handler graceful in James:<br>  <img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/3JamesHanderGraceful.png"></p><ul><li>Log from James when stopping James container: &#x3D;&gt; James try to “dispose” something before it finally stopped</li></ul><pre><code>03:45:29.722 [INFO ] o.a.j.m.s.JMXServer - JMX server stopped2023-06-28T03:45:29.722681185Z 03:45:29.722 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose Manage Sieve Service2023-06-28T03:45:29.722689712Z 03:45:29.722 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose Manage Sieve Service done2023-06-28T03:45:29.725654700Z 03:45:29.725 [INFO ] o.a.j.w.WebAdminServer - Web admin server stopped2023-06-28T03:45:29.731379540Z 03:45:29.731 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose SMTP Service2023-06-28T03:45:31.744946195Z 03:45:31.744 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose SMTP Service done2023-06-28T03:45:31.744969930Z 03:45:31.744 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose SMTP Service2023-06-28T03:45:33.748734899Z 03:45:33.748 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose SMTP Service done2023-06-28T03:45:33.748754646Z 03:45:33.748 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose SMTP Service2023-06-28T03:45:35.752316430Z 03:45:35.752 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose SMTP Service done2023-06-28T03:45:35.752336378Z 03:45:35.752 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose POP3 Service2023-06-28T03:45:35.752339383Z 03:45:35.752 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose POP3 Service done2023-06-28T03:45:35.752344243Z 03:45:35.752 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose IMAP Service2023-06-28T03:45:37.755951232Z 03:45:37.755 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose IMAP Service done2023-06-28T03:45:37.755973675Z 03:45:37.755 [INFO ] o.a.j.p.l.n.AbstractConfigurableAsyncServer - Dispose IMAP Service</code></pre><ul><li>Question?<ul><li>What happens if the application needs more time to complete the task than the grace period of Docker?</li></ul></li><li>add <code>addShutdownHook</code> to <code>MemoryJamesServerMain</code> for watch &#x3D;&gt; app shutdown after <code>10seconds</code><br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/4JamesMemoryShutDownHook.png"></li><li>Docker compose support configure <code>stop_grace_period</code> and <code>stop_signal</code><br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/3IncreaseGracefulShutDowntime_Ref.png"></li><li>Result after changing <code>stop_grace_period</code> (default -&gt; 1 minute)<br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/3IncreaseGracefulShutDowntime.png"></li><li>Demo:<br><a href="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/3GracefulShutDown60s.mp4"><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/3GracefulShutDown60s.png" alt="height:500px"></a></li></ul><h2 id="K8s"><a href="#K8s" class="headerlink" title="K8s"></a>K8s</h2><ul><li><code>terminationGracePeriodSeconds</code>: 30s<br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/5k8sTermialGraceShutdown.png"></li><li>K8s log when stop tmail pod<br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/5k8sGracefulShutdownLog.png"></li><li>Healthcheck<br><code>livenessProbe</code><br><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/image/5k8sLivenessProbeConfig.png"></li><li>Demo:<br><a href="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/4k8s.mp4"><img src="https://s3.ap-east-1.amazonaws.com/aws-s3.tungexplorer.me/docker_healthcheck_graceful_slide/4k8s.png" alt="height:500px"></a></li></ul>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> healthcheck </tag>
            
            <tag> graceful shutdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Netty In Action</title>
      <link href="/2022/12/Java/Netty_In_Action/"/>
      <url>/2022/12/Java/Netty_In_Action/</url>
      
        <content type="html"><![CDATA[<p>All Netty servers require the following:<br>    - At least one ChannelHandler —This component implements the server’s process-<br>    ing of data received from the client—its business logic.<br>    - Bootstrapping—This is the startup code that configures the server. At a minimum,<br>it binds the server to the port on which it will listen for connection requests    </p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> netty </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gatling maven sample</title>
      <link href="/2022/05/Other/GatlingMavenSample/"/>
      <url>/2022/05/Other/GatlingMavenSample/</url>
      
        <content type="html"><![CDATA[<h1 id="Gatling-maven-sample"><a href="#Gatling-maven-sample" class="headerlink" title="Gatling maven sample"></a>Gatling maven sample</h1><h2 id="Maven"><a href="#Maven" class="headerlink" title="Maven"></a>Maven</h2><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;me.tungexplorer&lt;/groupId&gt;    &lt;artifactId&gt;gatling-maven-sample&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;11&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;11&lt;/maven.compiler.target&gt;        &lt;scala-maven-plugin.version&gt;4.6.0&lt;/scala-maven-plugin.version&gt;        &lt;gatling-maven-plugin.version&gt;4.1.5&lt;/gatling-maven-plugin.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;io.gatling.highcharts&lt;/groupId&gt;            &lt;artifactId&gt;gatling-charts-highcharts&lt;/artifactId&gt;            &lt;version&gt;3.7.6&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;version&gt;3.2.0&lt;/version&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;                &lt;version&gt;$&#123;scala-maven-plugin.version&#125;&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;goals&gt;                            &lt;goal&gt;testCompile&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;args&gt;                                &lt;arg&gt;-target:jvm-11&lt;/arg&gt;                                &lt;arg&gt;-deprecation&lt;/arg&gt;                                &lt;arg&gt;-feature&lt;/arg&gt;                                &lt;arg&gt;-unchecked&lt;/arg&gt;                                &lt;arg&gt;-language:implicitConversions&lt;/arg&gt;                                &lt;arg&gt;-language:postfixOps&lt;/arg&gt;                                &lt;arg&gt;-Xlog-implicits&lt;/arg&gt;                                &lt;arg&gt;-explaintypes&lt;/arg&gt;                            &lt;/args&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;io.gatling&lt;/groupId&gt;                &lt;artifactId&gt;gatling-maven-plugin&lt;/artifactId&gt;                &lt;version&gt;$&#123;gatling-maven-plugin.version&#125;&lt;/version&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><h2 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h2><pre><code class="java">import io.gatling.core.Predef._import io.gatling.core.scenario.Simulationimport io.gatling.core.structure.ScenarioBuilderimport io.gatling.http.Predef._import io.gatling.http.protocol.HttpProtocolBuilderimport io.gatling.http.request.builder.HttpRequestBuilderimport scala.concurrent.duration._import scala.language.postfixOpsimport scala.util.Randomclass SampleSimulation extends Simulation &#123;  def httpCall(): HttpRequestBuilder = http(&quot;Request to endpoint /test&quot;)    .get(&quot;/test&quot;)    .queryParam(&quot;greetingEmail&quot;, &quot;$&#123;email&#125;&quot;)    .queryParam(&quot;secondParam&quot;, StringBody(session =&gt; s&quot;&quot;&quot;&#123; &quot;orderReference&quot;: &quot;$&#123;generateSecondParam()&#125;&quot; &#125;&quot;&quot;&quot;))  def generateSecondParam(): String = Random.alphanumeric.take(20).mkString  val feeder: Iterator[Map[String, String]] = Iterator.continually &#123;    Map(&quot;email&quot; -&gt; s&quot;$&#123;Random.alphanumeric.take(20).mkString&#125;@foo.com&quot;)  &#125;  def generate(): ScenarioBuilder =    scenario(&quot;Call http test&quot;)      .exec(feed(feeder)        .exec(httpCall()          .check(status.is(200))))  def httpProtocol: HttpProtocolBuilder = http    .baseUrl(&quot;https://webhook.site/456eed1d-a188-407d-a0d7-38d820366234&quot;)    .acceptHeader(&quot;application/json&quot;)    .contentTypeHeader(&quot;application/json; charset=UTF-8&quot;)  setUp(generate()    .inject((rampUsersPerSec(1).to(2) during (2 minutes)).randomized)    .protocols(httpProtocol)  )&#125;</code></pre><h2 id="Runner"><a href="#Runner" class="headerlink" title="Runner"></a>Runner</h2><pre><code class="java">import io.gatling.app.Gatlingimport io.gatling.core.config.GatlingPropertiesBuilderobject GatlingRunnerSample extends App &#123;  val simulationClass: String = &quot;me.tungexplorer.SampleSimulation&quot;  val props: GatlingPropertiesBuilder = new GatlingPropertiesBuilder  props.resourcesDirectory(&quot;src/main/scala&quot;)  props.binariesDirectory(&quot;target/scala/classes&quot;)  props.resultsDirectory(&quot;/tmp/gatling-result&quot;)  props.simulationClass(simulationClass)  Gatling.fromMap(props.build)&#125;</code></pre><h2 id="Git-project"><a href="#Git-project" class="headerlink" title="Git project"></a>Git project</h2><ul><li><a href="https://github.com/tungtv202/gatling-maven-sample">https://github.com/tungtv202/gatling-maven-sample</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> maven </tag>
            
            <tag> performance test </tag>
            
            <tag> gatling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - How many max threads we can create?</title>
      <link href="/2022/04/Java/HowManyMaxThreadsInJava/"/>
      <url>/2022/04/Java/HowManyMaxThreadsInJava/</url>
      
        <content type="html"><![CDATA[<h1 id="How-many-max-threads-we-can-create-in-Java"><a href="#How-many-max-threads-we-can-create-in-Java" class="headerlink" title="How many max threads we can create in Java?"></a>How many max threads we can create in Java?</h1><h2 id="My-PC-info"><a href="#My-PC-info" class="headerlink" title="My PC info"></a>My PC info</h2><pre><code>OS: Ubuntu 20.04.4 LTS x86_64 Kernel: 5.13.0-39-generic Packages: 2837 (dpkg), 18 (snap) Shell: zsh 5.8 DE: GNOME WM: Mutter CPU: AMD Ryzen 5 5600G with Radeon Graphics (12) @ 3.900GHz GPU: AMD ATI 04:00.0 Device 1638 Memory: 6860MiB / 27949MiB </code></pre><h2 id="Java-code"><a href="#Java-code" class="headerlink" title="Java code"></a>Java code</h2><pre><code class="java">import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit;public class HowManyMaxThreadWeCanCreate &#123;    public static void main(String[] args) throws InterruptedException &#123;        int numberOfTasks = 2000000;        System.out.println(&quot;HeapSize start maxMemory &quot; + formatSize(Runtime.getRuntime().maxMemory()));        CountDownLatch latch = new CountDownLatch(numberOfTasks);        for (int i = 1; i &lt;= numberOfTasks; i++) &#123;            int finalI = i;            new Thread(() -&gt; &#123;                System.out.println(finalI + &quot; task is started in : &quot; + Thread.currentThread().getName());                try &#123;                    TimeUnit.SECONDS.sleep(1000);                &#125; catch (InterruptedException e) &#123;                    throw new RuntimeException(e);                &#125;                System.out.println(finalI + &quot; task is finished in : &quot; + Thread.currentThread().getName());                latch.countDown();            &#125;).start();        &#125;        latch.await();    &#125;    public static String formatSize(long v) &#123;        if (v &lt; 1024) return v + &quot; B&quot;;        int z = (63 - Long.numberOfLeadingZeros(v)) / 10;        return String.format(&quot;%.1f %sB&quot;, (double) v / (1L &lt;&lt; (z * 10)), &quot; KMGTPE&quot;.charAt(z));    &#125;&#125;</code></pre><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><ul><li><code>32571</code> threads has been created, after that I got a error</li></ul><pre><code class="log">[7,278s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 16384 bytes for committing reserved memory.# An error report file with more information is saved as:# /home/tungtv/workplace/1_STUDY/Study101/hs_err_pid78874.log[7,279s][warning][os,thread] Attempt to deallocate stack guard pages failed (0x00007f5ac2584000-0x00007f5ac2588000).[thread 78875 also had an error]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f50f74b9000, 16384, 0) failed; error=&#39;Not enough space&#39; (errno=12)Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached    at java.base/java.lang.Thread.start0(Native Method)    at java.base/java.lang.Thread.start(Thread.java:798)    at me.tungexplorer.study.reactor.HowManyMaxThreadWeCanCreate.main(HowManyMaxThreadWeCanCreate.java:30)OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5ac2584000, 16384, 0) failed; error=&#39;Not enough space&#39; (errno=12)[7,827s][warning][os,thread] Attempt to deallocate stack guard pages failed (0x00007f568e9f7000-0x00007f568e9fb000).</code></pre><ul><li>I tried to increase heap size (by setting Xmx), but maybe it is not a reason</li><li>How can I check threads running in the system (note: os threads, NOT machine threads)</li></ul><p>bash: </p><pre><code class="bash">ps -eo nlwp | tail -n +2 | awk &#39;&#123; num_threads += $1 &#125; END &#123; print num_threads &#125;&#39;</code></pre><pre><code>- When normal, thread count from 1000 -&gt; 2000. - When I tried to run the Java app, the thread increased to + ~30k. After Java exit, the threads count came back to normal</code></pre><ul><li>Maximum number of threads per process in Linux?</li></ul><pre><code class="bash">cat /proc/sys/kernel/threads-max# (My Computer is `223043`, my friend computer is `126578`)</code></pre><ul><li>How Much Memory Does a Java Thread Take?<ul><li>~1KB</li></ul></li></ul><pre><code class="bash">java -XX:+PrintFlagsFinal -version | grep ThreadStackSize       intx CompilerThreadStackSize                  = 1024                                   &#123;pd product&#125; &#123;default&#125;     intx ThreadStackSize                          = 1024                                   &#123;pd product&#125; &#123;default&#125;     intx VMThreadStackSize                        = 1024                                   &#123;pd product&#125; &#123;default&#125;openjdk version &quot;11.0.14.1&quot; 2022-02-08OpenJDK Runtime Environment (build 11.0.14.1+1-Ubuntu-0ubuntu1.20.04)OpenJDK 64-Bit Server VM (build 11.0.14.1+1-Ubuntu-0ubuntu1.20.04, mixed mode, sharing)</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> thread </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch - Contact Autocomplete Feature</title>
      <link href="/2022/04/Opensearch/Autocomplete_contact_feature/"/>
      <url>/2022/04/Opensearch/Autocomplete_contact_feature/</url>
      
        <content type="html"><![CDATA[<h1 id="Contact-Autocomplete-Feature"><a href="#Contact-Autocomplete-Feature" class="headerlink" title="Contact Autocomplete Feature"></a>Contact Autocomplete Feature</h1><h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><ul><li>Build an autocomplete feature (search feature).</li><li>Data index:</li></ul><pre><code class="json">[&#123;    &quot;contactId&quot;: 1,    &quot;email&quot;: &quot;tungtv202@gmail.com&quot;,    &quot;firstname&quot;: &quot;Tung&quot;,    &quot;surname&quot;: &quot;Tran Van&quot;&#125;, ...]</code></pre><ul><li>Search with any text query. Example “tungtv202”. The results should be returned exactly as expected.</li></ul><h2 id="1-Create-Index"><a href="#1-Create-Index" class="headerlink" title="1. Create Index"></a>1. Create Index</h2><ul><li><code>PUT /contact</code></li></ul><pre><code class="json">&#123;    &quot;settings&quot;: &#123;        &quot;number_of_shards&quot;: 5,        &quot;number_of_replicas&quot;: 1,        &quot;index.write.wait_for_active_shards&quot;: 1,        &quot;index&quot;: &#123;            &quot;max_ngram_diff&quot;: 30        &#125;,        &quot;analysis&quot;: &#123;            &quot;analyzer&quot;: &#123;                &quot;search_analyzer&quot;: &#123;                    &quot;tokenizer&quot;: &quot;uax_url_email&quot;                &#125;,                &quot;ngram_filter_analyzer&quot;: &#123;                    &quot;tokenizer&quot;: &quot;uax_url_email&quot;,                    &quot;filter&quot;: [                        &quot;ngram_filter&quot;                    ]                &#125;,                &quot;edge_ngram_filter_analyzer&quot;: &#123;                    &quot;tokenizer&quot;: &quot;standard&quot;,                    &quot;filter&quot;: [                        &quot;edge_ngram_filter&quot;,                        &quot;lowercase&quot;                    ]                &#125;            &#125;,            &quot;filter&quot;: &#123;                &quot;ngram_filter&quot;: &#123;                    &quot;type&quot;: &quot;ngram&quot;,                    &quot;min_gram&quot;: 3,                    &quot;max_gram&quot;: 30                &#125;,                &quot;edge_ngram_filter&quot;: &#123;                    &quot;type&quot;: &quot;edge_ngram&quot;,                    &quot;min_gram&quot;: 3,                    &quot;max_gram&quot;: 30                &#125;            &#125;        &#125;    &#125;,    &quot;mappings&quot;: &#123;        &quot;properties&quot;: &#123;            &quot;contactId&quot;: &#123;                &quot;type&quot;: &quot;keyword&quot;            &#125;,            &quot;email&quot;: &#123;                &quot;type&quot;: &quot;text&quot;,                &quot;analyzer&quot;: &quot;ngram_filter_analyzer&quot;            &#125;,            &quot;firstname&quot;: &#123;                &quot;type&quot;: &quot;text&quot;,                &quot;analyzer&quot;: &quot;edge_ngram_filter_analyzer&quot;            &#125;,            &quot;surname&quot;: &#123;                &quot;type&quot;: &quot;text&quot;,                &quot;analyzer&quot;: &quot;edge_ngram_filter_analyzer&quot;            &#125;        &#125;    &#125;&#125;</code></pre><h3 id="Why-email-Uses-ngram-filter-analyzer-Instead-of-edge-ngram-filter-analyzer"><a href="#Why-email-Uses-ngram-filter-analyzer-Instead-of-edge-ngram-filter-analyzer" class="headerlink" title="Why email Uses ngram_filter_analyzer Instead of edge_ngram_filter_analyzer?"></a>Why <code>email</code> Uses <code>ngram_filter_analyzer</code> Instead of <code>edge_ngram_filter_analyzer</code>?</h3><ul><li>Using <code>edge_ngram_filter_analyzer</code> on <code>email</code> would result in tokens like these:</li></ul><pre><code class="bash">GET /_analyze&#123;  &quot;tokenizer&quot;: &quot;uax_url_email&quot;,  &quot;filter&quot;: [&#123;&quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 3, &quot;max_gram&quot;: 30 &#125;],  &quot;text&quot;: &quot;tranvantung@gmail.com&quot;&#125;</code></pre><p>Tokens result:</p><pre><code>[  &quot;tra&quot;,  &quot;tran&quot;,  &quot;tranv&quot;,  &quot;tranva&quot;,  &quot;tranvan&quot;,  &quot;tranvant&quot;,  &quot;tranvantu&quot;,  &quot;tranvantun&quot;,  &quot;tranvantung&quot;,  &quot;tranvantung@&quot;,  &quot;tranvantung@g&quot;,  &quot;tranvantung@gm&quot;,  &quot;tranvantung@gma&quot;,  &quot;tranvantung@gmai&quot;,  &quot;tranvantung@gmail&quot;,  &quot;tranvantung@gmail.&quot;,  &quot;tranvantung@gmail.c&quot;,  &quot;tranvantung@gmail.co&quot;,  &quot;tranvantung@gmail.com&quot;]</code></pre><p>No token contains <code>tung</code>.</p><ul><li>Using <code>ngram_filter_analyzer</code> instead:</li></ul><pre><code class="bash">GET /_analyze&#123;  &quot;tokenizer&quot;: &quot;uax_url_email&quot;,  &quot;filter&quot;: [&#123;&quot;type&quot;: &quot;ngram&quot;, &quot;min_gram&quot;: 3, &quot;max_gram&quot;: 4 &#125;],  &quot;text&quot;: &quot;tranvantung@gmail.com&quot;&#125;</code></pre><p>Tokens result:</p><pre><code>[  &quot;tra&quot;,  &quot;tran&quot;,  &quot;ran&quot;,  &quot;ranv&quot;,  &quot;anv&quot;,  &quot;anva&quot;,  &quot;nva&quot;,  &quot;nvan&quot;,  &quot;van&quot;,  &quot;vant&quot;,  &quot;ant&quot;,  &quot;antu&quot;,  &quot;ntu&quot;,  &quot;ntun&quot;,  &quot;tun&quot;,  &quot;tung&quot;,  &quot;ung&quot;,  &quot;ung@&quot;,  &quot;ng@&quot;,  &quot;ng@g&quot;,  &quot;g@g&quot;,  &quot;g@gm&quot;,  &quot;@gm&quot;,  &quot;@gma&quot;,  &quot;gma&quot;,  &quot;gmai&quot;,  &quot;mai&quot;,  &quot;mail&quot;,  &quot;ail&quot;,  &quot;ail.&quot;,  &quot;il.&quot;,  &quot;il.c&quot;,  &quot;l.c&quot;,  &quot;l.co&quot;,  &quot;.co&quot;,  &quot;.com&quot;,  &quot;com&quot;]</code></pre><p>Includes the token <code>tung</code>.</p><h3 id="Why-firstname-and-surname-Use-edge-ngram-filter-analyzer"><a href="#Why-firstname-and-surname-Use-edge-ngram-filter-analyzer" class="headerlink" title="Why firstname and surname Use edge_ngram_filter_analyzer?"></a>Why <code>firstname</code> and <code>surname</code> Use <code>edge_ngram_filter_analyzer</code>?</h3><ul><li>When searching for contact names, users typically type progressively (e.g., T…Tu…Tun…Tung).</li></ul><pre><code class="bash">GET /_analyze&#123;  &quot;tokenizer&quot;: &quot;standard&quot;,  &quot;filter&quot;: [&#123;&quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 3, &quot;max_gram&quot;: 30 &#125;],  &quot;text&quot;: &quot;Tran Van Tung&quot;&#125;</code></pre><p>Tokens result:</p><pre><code>[  &quot;Tra&quot;,  &quot;Tran&quot;,  &quot;Van&quot;,  &quot;Tun&quot;,  &quot;Tung&quot;]</code></pre><h3 id="Why-Use-tokenizer-uax-url-email"><a href="#Why-Use-tokenizer-uax-url-email" class="headerlink" title="Why Use tokenizer: uax_url_email?"></a>Why Use <code>tokenizer</code>: <code>uax_url_email</code>?</h3><ul><li>The <code>uax_url_email</code> tokenizer is designed to tokenize email addresses and URLs effectively.</li></ul><p>Reference: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-uaxurlemail-tokenizer.html">Elasticsearch UAX URL Email Tokenizer</a></p><h3 id="Why-Use-search-analyzer"><a href="#Why-Use-search-analyzer" class="headerlink" title="Why Use search_analyzer?"></a>Why Use <code>search_analyzer</code>?</h3><ul><li>The <code>search_analyzer</code> is used during search queries for the <code>email</code> field. It can be set as the default search analyzer or assigned in the query:</li></ul><pre><code class="json">&quot;email&quot;: &#123;    &quot;type&quot;: &quot;text&quot;,    &quot;analyzer&quot;: &quot;ngram_filter_analyzer&quot;,    &quot;search_analyzer&quot;: &quot;search_analyzer&quot;&#125;</code></pre><h2 id="2-Search-Query"><a href="#2-Search-Query" class="headerlink" title="2. Search Query"></a>2. Search Query</h2><ul><li><code>GET /contact/_search</code></li></ul><pre><code class="json">&#123;  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;should&quot;: [        &#123;          &quot;multi_match&quot;: &#123;            &quot;query&quot;: &quot;nobita&quot;,            &quot;fields&quot;: [&quot;email&quot;],            &quot;analyzer&quot;: &quot;search_analyzer&quot;          &#125;        &#125;,        &#123;          &quot;multi_match&quot;: &#123;            &quot;query&quot;: &quot;nobita&quot;,            &quot;fields&quot;: [&quot;firstname&quot;, &quot;surname&quot;]          &#125;        &#125;      ],      &quot;minimum_should_match&quot;: 1    &#125;  &#125;&#125;</code></pre><h3 id="More-Complex-Query"><a href="#More-Complex-Query" class="headerlink" title="More Complex Query"></a>More Complex Query</h3><pre><code class="json">&#123;  &quot;from&quot;: 0,  &quot;size&quot;: 10,  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;must&quot;: [        &#123;          &quot;bool&quot;: &#123;            &quot;should&quot;: [              &#123;                &quot;multi_match&quot;: &#123;                  &quot;query&quot;: &quot;nobita&quot;,                  &quot;fields&quot;: [&quot;email&quot;],                  &quot;analyzer&quot;: &quot;search_analyzer&quot;                &#125;              &#125;,              &#123;                &quot;multi_match&quot;: &#123;                  &quot;query&quot;: &quot;nobita&quot;,                  &quot;fields&quot;: [&quot;firstname&quot;, &quot;surname&quot;]                &#125;              &#125;            ],            &quot;minimum_should_match&quot;: 1          &#125;        &#125;,        &#123;          &quot;bool&quot;: &#123;            &quot;must&quot;: [              &#123;                &quot;term&quot;: &#123;                  &quot;domain&quot;: &quot;linagora.com&quot;                &#125;              &#125;            ]          &#125;        &#125;      ]    &#125;  &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
            <tag> search contact </tag>
            
            <tag> autocomplete </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch - Contact autocomplete feature</title>
      <link href="/2022/04/Other/Autocomplete_contact_feature/"/>
      <url>/2022/04/Other/Autocomplete_contact_feature/</url>
      
        <content type="html"><![CDATA[<h1 id="Contact-autocomplete-feature"><a href="#Contact-autocomplete-feature" class="headerlink" title="Contact autocomplete feature"></a>Contact autocomplete feature</h1><h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><ul><li>Build a autocomplete feature (search feature).</li><li>Data index:</li></ul><pre><code class="json">[&#123;    &quot;contactId&quot; : 1,    &quot;email&quot; : &quot;tungtv202@gmail.com&quot;,    &quot;firstname&quot; : &quot;Tung&quot;,    &quot;surname&quot; : &quot;Tran Van&quot;&#125;, ....]</code></pre><ul><li>Search with any text query. Example “tungtv202”. The results should be returned exactly expected.</li></ul><h2 id="1-Create-index"><a href="#1-Create-index" class="headerlink" title="1. Create index"></a>1. Create index</h2><ul><li><code>PUT /contact</code></li></ul><pre><code class="json">&#123;    &quot;settings&quot;: &#123;        &quot;number_of_shards&quot;: 5,        &quot;number_of_replicas&quot;: 1,        &quot;index.write.wait_for_active_shards&quot;: 1,        &quot;index&quot;: &#123;            &quot;max_ngram_diff&quot;: 30        &#125;,        &quot;analysis&quot;: &#123;            &quot;analyzer&quot;: &#123;                &quot;search_analyzer&quot;: &#123;                    &quot;tokenizer&quot;: &quot;uax_url_email&quot;                &#125;,                &quot;ngram_filter_analyzer&quot;: &#123;                    &quot;tokenizer&quot;: &quot;uax_url_email&quot;,                    &quot;filter&quot;: [                        &quot;ngram_filter&quot;                    ]                &#125;,                &quot;edge_ngram_filter_analyzer&quot;: &#123;                    &quot;tokenizer&quot;: &quot;standard&quot;,                    &quot;filter&quot;: [                        &quot;edge_ngram_filter&quot;,                        &quot;lowercase&quot;                    ]                &#125;            &#125;,            &quot;filter&quot;: &#123;                &quot;ngram_filter&quot;: &#123;                    &quot;type&quot;: &quot;ngram&quot;,                    &quot;min_gram&quot;: 3,                    &quot;max_gram&quot;: 30                &#125;,                &quot;edge_ngram_filter&quot;: &#123;                    &quot;type&quot;: &quot;edge_ngram&quot;,                    &quot;min_gram&quot;: 3,                    &quot;max_gram&quot;: 30                &#125;            &#125;        &#125;    &#125;,    &quot;mapping&quot;: &#123;        &quot;properties&quot;: &#123;            &quot;contactId&quot;: &#123;                &quot;type&quot;: &quot;keyword&quot;            &#125;,            &quot;email&quot;: &#123;                &quot;type&quot;: &quot;text&quot;,                &quot;analyzer&quot;: &quot;ngram_filter_analyzer&quot;            &#125;,            &quot;firstname&quot;: &#123;                &quot;type&quot;: &quot;text&quot;,                &quot;analyzer&quot;: &quot;edge_ngram_filter_analyzer&quot;            &#125;,            &quot;surname&quot;: &#123;                &quot;type&quot;: &quot;text&quot;,                &quot;analyzer&quot;: &quot;edge_ngram_filter_analyzer&quot;            &#125;        &#125;    &#125;&#125;</code></pre><ol><li>Why <code>email</code> has NOT index analyzers is <code>edge_ngram_filter_analyzer</code>?<ul><li>“<a href="mailto:&#x74;&#x72;&#x61;&#x6e;&#118;&#97;&#x6e;&#116;&#117;&#110;&#x67;&#x40;&#x67;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#109;">&#x74;&#x72;&#x61;&#x6e;&#118;&#97;&#x6e;&#116;&#117;&#110;&#x67;&#x40;&#x67;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#109;</a>“ will NOT in contain of results when we searching “tung”</li></ul></li></ol><pre><code class="bash">GET /_analyze&#123;  &quot;tokenizer&quot;: &quot;uax_url_email&quot;,  &quot;filter&quot; : [&#123;&quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 3, &quot;max_gram&quot;: 30 &#125;],  &quot;text&quot;: &quot;tranvantung@gmail.com&quot;&#125;</code></pre><p>Tokens result:</p><pre><code>[  &quot;tra&quot;,  &quot;tran&quot;,  &quot;tranv&quot;,  &quot;tranva&quot;,  &quot;tranvan&quot;,  &quot;tranvant&quot;,  &quot;tranvantu&quot;,  &quot;tranvantun&quot;,  &quot;tranvantung&quot;,  &quot;tranvantung@&quot;,  &quot;tranvantung@g&quot;,  &quot;tranvantung@gm&quot;,  &quot;tranvantung@gma&quot;,  &quot;tranvantung@gmai&quot;,  &quot;tranvantung@gmail&quot;,  &quot;tranvantung@gmail.&quot;,  &quot;tranvantung@gmail.c&quot;,  &quot;tranvantung@gmail.co&quot;,  &quot;tranvantung@gmail.com&quot;]</code></pre><p>We can see, no any token has value <code>tung</code>.<br>Now, let try with <code>ngram</code></p><pre><code class="bash">GET /_analyze&#123;  &quot;tokenizer&quot;: &quot;uax_url_email&quot;,  &quot;filter&quot; : [&#123;&quot;type&quot;: &quot;ngram&quot;, &quot;min_gram&quot;: 3, &quot;max_gram&quot;: 4 &#125;],  &quot;text&quot;: &quot;tranvantung@gmail.com&quot;&#125;</code></pre><p>tokens result:</p><pre><code>[  &quot;tra&quot;,  &quot;tran&quot;,  &quot;ran&quot;,  &quot;ranv&quot;,  &quot;anv&quot;,  &quot;anva&quot;,  &quot;nva&quot;,  &quot;nvan&quot;,  &quot;van&quot;,  &quot;vant&quot;,  &quot;ant&quot;,  &quot;antu&quot;,  &quot;ntu&quot;,  &quot;ntun&quot;,  &quot;tun&quot;,  &quot;tung&quot;,  &quot;ung&quot;,  &quot;ung@&quot;,  &quot;ng@&quot;,  &quot;ng@g&quot;,  &quot;g@g&quot;,  &quot;g@gm&quot;,  &quot;@gm&quot;,  &quot;@gma&quot;,  &quot;gma&quot;,  &quot;gmai&quot;,  &quot;mai&quot;,  &quot;mail&quot;,  &quot;ail&quot;,  &quot;ail.&quot;,  &quot;il.&quot;,  &quot;il.c&quot;,  &quot;l.c&quot;,  &quot;l.co&quot;,  &quot;.co&quot;,  &quot;.com&quot;,  &quot;com&quot;]</code></pre><p>Has token “tung”.</p><ol start="2"><li>Why <code>firstname/surname</code> has index analyzers is <code>edge_ngram_filter_analyzer</code>?</li></ol><ul><li>When you search contact by contactName, you will not search “ung” for “Tung” contact?<br>you will type: T…Tu…Tun…Tung</li></ul><pre><code class="bash">GET /_analyze&#123;  &quot;tokenizer&quot;: &quot;standard&quot;,  &quot;filter&quot; : [&#123;&quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 3, &quot;max_gram&quot;: 30 &#125;],  &quot;text&quot;: &quot;Tran Van Tung&quot;&#125;</code></pre><p>tokens result:</p><pre><code>[  &quot;Tra&quot;,  &quot;Tran&quot;,  &quot;Van&quot;,  &quot;Tun&quot;,  &quot;Tung&quot;]</code></pre><ol start="3"><li>Why <code>&quot;tokenizer&quot;: &quot;uax_url_email&quot;</code>?</li></ol><ul><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-uaxurlemail-tokenizer.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-uaxurlemail-tokenizer.html</a></li></ul><ol start="4"><li>Why we need <code>search_analyzer</code>?</li></ol><ul><li>It will be used when searching, for field “email”.</li><li>We can setting search analyzer for email by default</li></ul><pre><code>            &quot;email&quot;: &#123;                &quot;type&quot;: &quot;text&quot;,                &quot;analyzer&quot;: &quot;ngram_filter_analyzer&quot;,                &quot;search_analyzer&quot;: &quot;search_analyzer&quot;            &#125;,</code></pre><p>or we can assign it in query when searching</p><h2 id="2-Search-query"><a href="#2-Search-query" class="headerlink" title="2. Search query"></a>2. Search query</h2><ul><li><code>GET /contact/_search</code></li></ul><pre><code class="json">&#123;  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;should&quot;: [        &#123;          &quot;multi_match&quot;: &#123;            &quot;query&quot;: &quot;nobita&quot;,            &quot;fields&quot;: [              &quot;email&quot;            ],            &quot;analyzer&quot;: &quot;search_analyzer&quot;          &#125;        &#125;,        &#123;          &quot;multi_match&quot;: &#123;            &quot;query&quot;: &quot;nobita&quot;,            &quot;fields&quot;: [              &quot;firstname&quot;,              &quot;surname&quot;            ]          &#125;        &#125;      ],      &quot;minimum_should_match&quot;: 1    &#125;  &#125;&#125;</code></pre><p>&#x2F;&#x2F; Bonus more complex query</p><pre><code class="json">&#123;  &quot;from&quot;: 0,  &quot;size&quot;: 10,  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;must&quot;: [        &#123;          &quot;bool&quot;: &#123;            &quot;should&quot;: [              &#123;                &quot;multi_match&quot;: &#123;                  &quot;query&quot;: &quot;nobita&quot;,                  &quot;fields&quot;: [                    &quot;email&quot;                  ],                  &quot;analyzer&quot;: &quot;search_analyzer&quot;                &#125;              &#125;,              &#123;                &quot;multi_match&quot;: &#123;                  &quot;query&quot;: &quot;nobita&quot;,                  &quot;fields&quot;: [                    &quot;firstname&quot;,                    &quot;surname&quot;                  ]                &#125;              &#125;            ],            &quot;minimum_should_match&quot;: 1          &#125;        &#125;,        &#123;          &quot;bool&quot;: &#123;            &quot;must&quot;: [              &#123;                &quot;term&quot;: &#123;                  &quot;domain&quot;: &quot;linagora.com&quot;                &#125;              &#125;            ]          &#125;        &#125;      ]    &#125;  &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
            <tag> search contact </tag>
            
            <tag> autocomplete </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch - Search as you type</title>
      <link href="/2022/04/Opensearch/Search_as_you_type_ElasticSearch/"/>
      <url>/2022/04/Opensearch/Search_as_you_type_ElasticSearch/</url>
      
        <content type="html"><![CDATA[<h1 id="Search-as-you-type-Other-related"><a href="#Search-as-you-type-Other-related" class="headerlink" title="Search as you type &amp; Other related"></a>Search as you type &amp; Other related</h1><h2 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h2><p>Example:</p><pre><code class="bash">PUT movies&#123;   &quot;mappings&quot;: &#123;       &quot;properties&quot;: &#123;           &quot;title&quot;: &#123;               &quot;type&quot;: &quot;search_as_you_type&quot;           &#125;,           &quot;genre&quot;: &#123;               &quot;type&quot;: &quot;search_as_you_type&quot;           &#125;       &#125;   &#125;&#125;</code></pre><p><code>search_as_you_type = field + field._2gram + field._3gram + field._index_prefix</code></p><p>Input:</p><pre><code>Star Wars: Episode VII - The Force Awakens</code></pre><table><thead><tr><th>Field</th><th align="left">Example Output</th></tr></thead><tbody><tr><td>movie_title</td><td align="left">[“star”, “wars”, “episode”, “vii”, “the”, “force”, “awakens”]</td></tr><tr><td>movie_title._2gram</td><td align="left">[“Star Wars”, “Wars Episode”, “Episode VII”, “VII The”, “The Force”, “Force Awakens”]</td></tr><tr><td>movie_title._3gram</td><td align="left">[“Star Wars”, “Star Wars Episode”, “Wars Episode”, “Wars Episode VII”, “Episode VII”, …]</td></tr><tr><td>movie_title._index_prefix</td><td align="left">[“S”, “St”, “Sta”, “Star”]</td></tr></tbody></table><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><ul><li>Run Elasticsearch</li></ul><pre><code class="bash">docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.10.2</code></pre><p>(No need auth)</p><pre><code class="bash">// CREATE NEW INDEX `movies`POST _bulk&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;135569&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;135569&quot;, &quot;title&quot; : &quot;Star Trek Beyond&quot;, &quot;year&quot;:2016 , &quot;genre&quot;:[&quot;Action&quot;, &quot;Adventure&quot;, &quot;Sci-Fi&quot;] &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;122886&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;122886&quot;, &quot;title&quot; : &quot;Star Wars: Episode VII - The Force Awakens&quot;, &quot;year&quot;:2015 , &quot;genre&quot;:[&quot;Action&quot;, &quot;Adventure&quot;, &quot;Fantasy&quot;, &quot;Sci-Fi&quot;, &quot;IMAX&quot;] &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;109487&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;109487&quot;, &quot;title&quot; : &quot;Interstellar&quot;, &quot;year&quot;:2014 , &quot;genre&quot;:[&quot;Sci-Fi&quot;, &quot;IMAX&quot;] &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;58559&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;58559&quot;, &quot;title&quot; : &quot;Dark Knight, The&quot;, &quot;year&quot;:2008 , &quot;genre&quot;:[&quot;Action&quot;, &quot;Crime&quot;, &quot;Drama&quot;, &quot;IMAX&quot;] &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;1924&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;1924&quot;, &quot;title&quot; : &quot;Plan 9 from Outer Space&quot;, &quot;year&quot;:1959 , &quot;genre&quot;:[&quot;Horror&quot;, &quot;Sci-Fi&quot;] &#125;// CREATE NEW INDEX `autocomplete`PUT autocomplete&#123;   &quot;mappings&quot;: &#123;       &quot;properties&quot;: &#123;           &quot;title&quot;: &#123;               &quot;type&quot;: &quot;search_as_you_type&quot;           &#125;,           &quot;genre&quot;: &#123;               &quot;type&quot;: &quot;search_as_you_type&quot;           &#125;       &#125;   &#125;&#125;// REINDEX DATA FROM movies -&gt; autocompletePOST _reindex&#123;  &quot;source&quot;: &#123;    &quot;index&quot;: &quot;movies&quot;  &#125;,  &quot;dest&quot;: &#123;    &quot;index&quot;: &quot;autocomplete&quot;  &#125;&#125;// CHECK MAPPINGGET /autocomplete/_mapping// SEARCHGET /autocomplete/_search&#123;  &quot;size&quot;: 5,  &quot;query&quot;: &#123;    &quot;multi_match&quot;: &#123;      &quot;query&quot;: &quot;Sta&quot;,      &quot;type&quot;: &quot;bool_prefix&quot;,      &quot;fields&quot;: [        &quot;title&quot;,        &quot;title._2gram&quot;,        &quot;title._3gram&quot;      ]    &#125;  &#125;&#125;</code></pre><h2 id="Other-related"><a href="#Other-related" class="headerlink" title="Other related"></a>Other related</h2><ul><li><code>ngram</code>: A sequence of n characters.</li><li>Analyzers are composed of a single Tokenizer and zero or more TokenFilters. The tokenizer may be preceded by one or more CharFilters.</li><li>An analyzer consists of three things:<ul><li>Character filter</li><li>Tokenizer</li><li>Token filter</li></ul></li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com//other/elasticsearch/TOKENIZER_charfilter_tokenFilter.png" alt="TOKENIZER_charfilter_tokenFilter"></p><ul><li><p>Sometimes the two approaches: <code>ngram tokenizer</code> vs <code>ngram token filter</code> are equivalent. (Depending on the circumstances, one approach may be better than the other)<br>Example:</p><ul><li><p><code>ngram token filter</code></p><pre><code class="json">&#123;    &quot;analysis&quot;: &#123;        &quot;filter&quot;: &#123;            &quot;ngram_filter&quot;: &#123;                &quot;type&quot;: &quot;nGram&quot;,                &quot;min_gram&quot;: 4,                &quot;max_gram&quot;: 4            &#125;        &#125;,        &quot;analyzer&quot;: &#123;            &quot;ngram_filter_analyzer&quot;: &#123;                &quot;type&quot;: &quot;custom&quot;,                &quot;tokenizer&quot;: &quot;standard&quot;,                &quot;filter&quot;: [                    &quot;lowercase&quot;,                    &quot;ngram_filter&quot;                ]            &#125;        &#125;    &#125;&#125;</code></pre></li><li><p><code>ngram tokenizer</code></p><pre><code class="json">&#123;    &quot;analysis&quot;: &#123;        &quot;tokenizer&quot;: &#123;            &quot;ngram_tokenizer&quot;: &#123;                &quot;type&quot;: &quot;nGram&quot;,                &quot;min_gram&quot;: 4,                &quot;max_gram&quot;: 4,                &quot;token_chars&quot;: [                    &quot;letter&quot;,                    &quot;digit&quot;                ]            &#125;        &#125;,        &quot;analyzer&quot;: &#123;            &quot;ngram_tokenizer_analyzer&quot;: &#123;                &quot;type&quot;: &quot;custom&quot;,                &quot;tokenizer&quot;: &quot;ngram_tokenizer&quot;,                &quot;filter&quot;: [                    &quot;lowercase&quot;                ]            &#125;        &#125;    &#125;&#125;</code></pre></li></ul></li><li><p>Test an <code>analyzer</code>: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/test-analyzer.html">Elasticsearch Test Analyzer</a></p></li><li><p><code>ngram</code></p></li></ul><pre><code class="bash">GET _analyze&#123;  &quot;tokenizer&quot;: &quot;standard&quot;,  &quot;filter&quot;: [&#123;&quot;type&quot;: &quot;ngram&quot;, &quot;min_gram&quot;: 3, &quot;max_gram&quot;: 3 &#125;],  &quot;text&quot;: &quot;search&quot;&#125;</code></pre><p>Result:</p><pre><code>[&quot;sea&quot;, &quot;ear&quot;, &quot;arc&quot;, &quot;rch&quot;]</code></pre><ul><li><code>edge_ngram</code></li></ul><pre><code class="bash">GET _analyze&#123; &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [&#123;&quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 1, &quot;max_gram&quot;: 10 &#125;], &quot;text&quot;: &quot;search&quot;&#125;</code></pre><p>Result:</p><pre><code>[&quot;s&quot;, &quot;se&quot;, &quot;sea&quot;, &quot;sear&quot;, &quot;searc&quot;, &quot;search&quot;]</code></pre><h3 id="index-analyzer-vs-search-analyzer"><a href="#index-analyzer-vs-search-analyzer" class="headerlink" title="index_analyzer vs search_analyzer"></a>index_analyzer vs search_analyzer</h3><p>Why do we need two analyzers?<br>This comes from ES mechanism work. Let’s imagine through the following example:</p><ul><li>We have a document with content “XXXXXXXX” (1).</li><li>When “index time”, (1) has been indexed (<code>index_analyzer</code>) to tokens: [A, B, C, D].</li><li>When we search text “XXXX” (2), this string will be analyzed (<code>search_analyzer</code>) to tokens: [A, B, E].</li><li>In order to detect results, ES will compare tokens of documents from “index time” with tokens of “query text” from “search time”. Example: A&#x3D;A, B&#x3D;B.</li></ul><p>Why do we use single analyzers for both index and search?. Let’s take an example:</p><ul><li>We have document “elasticsearch”.</li><li>Document has been indexed by <code>edge_ngram</code> (min_gram&#x3D;3, max_gram&#x3D;20) (Analyzer A1). &#x3D;&gt; Tokens: [“ela”, “elas”, “elast”, “elasti”, “elastic”, “elastics”, “elasticse”, “elasticsea”, “elasticsear”, “elasticsearc”, “elasticsearch”].</li><li>We search with text query “elapsed”. With Analyzer A1, tokens will be: [“ela”, “elap”, “elaps”, “elapse”, “elapsed”].</li><li>We can see “ela”&#x3D;”ela”. So, when we search “elapsed”, the result will contain “elasticsearch”. Is it our expected result? NO.</li><li>If we tokenize “elapsed” by another Analyzer. Example: Analyzer A2 (tokenizer&#x3D;standard, filter&#x3D;standard). Then tokens will be [“elapsed”]. &#x3D;&gt; Then the result will NOT contain “elasticsearch”.</li></ul><p>Reference: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-index-search-time.html#different-analyzers">Elasticsearch Analysis: Index Time vs Search Time</a></p>]]></content>
      
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
            <tag> autocomplete </tag>
            
            <tag> search as you type </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch - Search as you type</title>
      <link href="/2022/04/Other/Search_as_you_type_ElasticSearch/"/>
      <url>/2022/04/Other/Search_as_you_type_ElasticSearch/</url>
      
        <content type="html"><![CDATA[<h1 id="Search-as-you-type-Other-related"><a href="#Search-as-you-type-Other-related" class="headerlink" title="Search as you type &amp; Other related"></a>Search as you type &amp; Other related</h1><h2 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h2><p>Example: </p><pre><code class="bash">PUT movies&#123;   &quot;mappings&quot;: &#123;       &quot;properties&quot;: &#123;           &quot;title&quot;: &#123;               &quot;type&quot;: &quot;search_as_you_type&quot;           &#125;,           &quot;genre&quot;: &#123;               &quot;type&quot;: &quot;search_as_you_type&quot;           &#125;       &#125;   &#125;&#125;</code></pre><p><code>search_as_you_type = field + field._2gram + field._3gram + field._index_prefix</code></p><p>Input: </p><pre><code>Star Wars: Episode VII - The Force Awakens</code></pre><table><thead><tr><th>Field</th><th align="left"></th><th>Example Output</th></tr></thead><tbody><tr><td>movie_title</td><td align="left">The “root” field is analyzed as configured in the mapping</td><td>[“star”,”wars”,”episode”,”vii”,”the”,”force”,”awakens”]</td></tr><tr><td>movie_title._2gram</td><td align="left">Splits sentence up by two words</td><td>[“Star Wars”,”Wars Episode”,”Episode VII”,”VII The”,”The Force”,”Force Awakens”]</td></tr><tr><td>movie_title._3gram</td><td align="left">Splits the sentence up by three words</td><td>[“Star Wars”,”Star Wars Episode”,”Wars Episode”,”Wars Episode VII”,”Episode VII”, … ]</td></tr><tr><td>movie_title._index_prefix</td><td align="left">This uses an edge n-gram token filter to split up each word into substrings, starting from the edge of the word</td><td>[“S”,”St”,”Sta”,”Star”]</td></tr></tbody></table><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><ul><li>Run elastic search</li></ul><pre><code>docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.10.2</code></pre><p>(No need auth)</p><pre><code class="bash">// CREATE NEW INDEX `movies`POST _bulk&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;135569&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;135569&quot;, &quot;title&quot; : &quot;Star Trek Beyond&quot;, &quot;year&quot;:2016 , &quot;genre&quot;:[&quot;Action&quot;, &quot;Adventure&quot;, &quot;Sci-Fi&quot;] &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;122886&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;122886&quot;, &quot;title&quot; : &quot;Star Wars: Episode VII - The Force Awakens&quot;, &quot;year&quot;:2015 , &quot;genre&quot;:[&quot;Action&quot;, &quot;Adventure&quot;, &quot;Fantasy&quot;, &quot;Sci-Fi&quot;, &quot;IMAX&quot;] &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;109487&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;109487&quot;, &quot;title&quot; : &quot;Interstellar&quot;, &quot;year&quot;:2014 , &quot;genre&quot;:[&quot;Sci-Fi&quot;, &quot;IMAX&quot;] &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;58559&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;58559&quot;, &quot;title&quot; : &quot;Dark Knight, The&quot;, &quot;year&quot;:2008 , &quot;genre&quot;:[&quot;Action&quot;, &quot;Crime&quot;, &quot;Drama&quot;, &quot;IMAX&quot;] &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;movies&quot;, &quot;_id&quot; : &quot;1924&quot; &#125; &#125;&#123; &quot;id&quot;: &quot;1924&quot;, &quot;title&quot; : &quot;Plan 9 from Outer Space&quot;, &quot;year&quot;:1959 , &quot;genre&quot;:[&quot;Horror&quot;, &quot;Sci-Fi&quot;] &#125;// CREATE NEW INDEX `autocomplete`PUT autocomplete&#123;   &quot;mappings&quot;: &#123;       &quot;properties&quot;: &#123;           &quot;title&quot;: &#123;               &quot;type&quot;: &quot;search_as_you_type&quot;           &#125;,           &quot;genre&quot;: &#123;               &quot;type&quot;: &quot;search_as_you_type&quot;           &#125;       &#125;   &#125;&#125;// REINDEX DATA FROM movies -&gt; autocompletePOST _reindex&#123;  &quot;source&quot;: &#123;    &quot;index&quot;: &quot;movies&quot;  &#125;,  &quot;dest&quot;: &#123;    &quot;index&quot;: &quot;autocomplete&quot;  &#125;&#125;// CHECK MAPINGGET /autocomplete/_mapping// SEARCHGET /autocomplete/_search&#123;  &quot;size&quot;: 5,  &quot;query&quot;: &#123;    &quot;multi_match&quot;: &#123;      &quot;query&quot;: &quot;Sta&quot;,      &quot;type&quot;: &quot;bool_prefix&quot;,      &quot;fields&quot;: [        &quot;title&quot;,        &quot;title._2gram&quot;,        &quot;title._3gram&quot;      ]    &#125;  &#125;&#125;</code></pre><h2 id="Other-related"><a href="#Other-related" class="headerlink" title="Other related"></a>Other related</h2><ul><li>ngram &#x3D; is a sequnce of n characters</li><li>Analyzers are composed of a single Tokenizer and zero or more TokenFilters. The tokenizer may be preceded by one or more CharFilters.</li><li>An analyzer consists of three things:<ul><li>Character filter</li><li>Tokeniser</li><li>Token filter</li></ul></li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com//other/elasticsearch/TOKENIZER_charfilter_tokenFilter.png" alt="TOKENIZER_charfilter_tokenFilter"></p><ul><li><p>Sometimes the two approaches: <code>ngram tokenizer vs ngram token filter</code> are equivalent. ( Depending on the circumstances one approach may be better than the other)<br>Example: </p><ul><li>ngram token filter</li></ul><pre><code class="json">&#123;    &quot;analysis&quot;: &#123;        &quot;filter&quot;: &#123;            &quot;ngram_filter&quot;: &#123;                &quot;type&quot;: &quot;nGram&quot;,                &quot;min_gram&quot;: 4,                &quot;max_gram&quot;: 4            &#125;        &#125;,        &quot;analyzer&quot;: &#123;            &quot;ngram_filter_analyzer&quot;: &#123;                &quot;type&quot;: &quot;custom&quot;,                &quot;tokenizer&quot;: &quot;standard&quot;,                &quot;filter&quot;: [                    &quot;lowercase&quot;,                    &quot;ngram_filter&quot;                ]            &#125;        &#125;    &#125;&#125;</code></pre><ul><li>ngram tokenizer</li></ul><pre><code class="json">&#123;    &quot;analysis&quot;: &#123;        &quot;tokenizer&quot;: &#123;            &quot;ngram_tokenizer&quot;: &#123;                &quot;type&quot;: &quot;nGram&quot;,                &quot;min_gram&quot;: 4,                &quot;max_gram&quot;: 4,                &quot;token_chars&quot;: [                    &quot;letter&quot;,                    &quot;digit&quot;                ]            &#125;        &#125;,        &quot;analyzer&quot;: &#123;            &quot;ngram_tokenizer_analyzer&quot;: &#123;                &quot;type&quot;: &quot;custom&quot;,                &quot;tokenizer&quot;: &quot;ngram_tokenizer&quot;,                &quot;filter&quot;: [                    &quot;lowercase&quot;                ]            &#125;        &#125;    &#125;&#125;</code></pre></li><li><p>Test an <code>analyzer</code>: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/test-analyzer.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/test-analyzer.html</a></p></li><li><p><code>ngram</code></p></li></ul><pre><code class="bash">GET _analyze&#123;  &quot;tokenizer&quot; : &quot;standard&quot;,  &quot;filter&quot; : [&#123;&quot;type&quot;: &quot;ngram&quot;, &quot;min_gram&quot;: 3, &quot;max_gram&quot;: 3 &#125;],  &quot;text&quot; : &quot;search&quot;&#125;</code></pre><p>result: </p><pre><code>[“sea”, “ear”, “arc”, “rch”]</code></pre><ul><li><code>edge_ngram</code></li></ul><pre><code class="bash">GET _analyze&#123; &quot;tokenizer&quot; : &quot;standard&quot;, &quot;filter&quot; : [&#123;&quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 1, &quot;max_gram&quot;: 10 &#125;], &quot;text&quot; : &quot;search&quot;&#125;</code></pre><p>result: </p><pre><code>[&quot;s&quot;, &quot;se&quot;, &quot;sea&quot;, &quot;sear&quot;, &quot;searc&quot;, &quot;search&quot;]</code></pre><h3 id="index-analyzer-vs-search-analyzer"><a href="#index-analyzer-vs-search-analyzer" class="headerlink" title="index_analyzer vs search_analyzer"></a>index_analyzer vs search_analyzer</h3><p>Why do we need two analyzers?<br>This comes from ES mechanism work. Let’s imagine through the following example:</p><ul><li>We have a document with content “XXXXXXXX” (1).  </li><li>When “index time”, (1) has been indexed (<code>index_analyzer</code>) to tokens: [A, B, C, D].</li><li>When we search text “XXXX” (2), this string will be analyzing (<code>search_analyzer</code>) to tokens: [A, B, E] </li><li>In order to detect results, ES will compare tokens of documents from “index time” with tokens of “query text” from “search time”. Example A&#x3D;A, B&#x3D;B</li></ul><p>Why do we use single analyers for both index and search?. Let example:</p><ul><li>We have document “elasticsearch”.</li><li>Document has been indexed by edge_ngram (min_gram&#x3D;3, max_gram&#x3D;20)  (Analyzer A1). &#x3D;&gt; Tokens: [“ela”, “elas”,”elast”,”elasti”,”elastic”,”elastics”,”elasticse”,”elasticsea”,”elasticsear”,”eleasticsearc” and “elasticsearch”]</li><li>We search with text query “elapsed”. With Analyzer A1, tokens will be: [“ela”, “elap”, “elaps”, “elapse”, “elapsed”]  </li><li>We can see “ela”&#x3D;”ela”. So, when we search “elapsed”, the result will contain “elasticsearch”. Is it our expected? NO</li><li>If we tokenizing “elapsed” by another Analyzer. Example: Analyzer A2 (tokenizer&#x3D;standard, filter&#x3D;standar). Then tokens will be [“elapsed” ]  &#x3D;&gt; Then the result will NOT contain “elasticsearch”</li></ul><p>Ref: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-index-search-time.html#different-analyzers">https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-index-search-time.html#different-analyzers</a></p>]]></content>
      
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
            <tag> autocomplete </tag>
            
            <tag> search as you type </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Resource leak detect by PhantomReference</title>
      <link href="/2022/03/Java/Java_LeakDetect/"/>
      <url>/2022/03/Java/Java_LeakDetect/</url>
      
        <content type="html"><![CDATA[<h1 id="Code-sample-Resource-leak-detect-by-java-lang-ref-PhantomReference"><a href="#Code-sample-Resource-leak-detect-by-java-lang-ref-PhantomReference" class="headerlink" title="Code sample - Resource leak detect by java.lang.ref.PhantomReference"></a>Code sample - Resource leak detect by java.lang.ref.PhantomReference</h1><p>Note: “resource” is not only “memory”, it can be a any method&#x2F;blockCode for “dispose&#x2F;release” somethings.</p><h2 id="PhantomReference-mechanism"><a href="#PhantomReference-mechanism" class="headerlink" title="PhantomReference mechanism"></a>PhantomReference mechanism</h2><ul><li>like <code>finalizers</code>, but more “flexible”</li><li>phantomRef.isEnqueued() method returns true which means that innerObject object has been removed from memory. When innerObject object is removed from memory, phantomRef object will be placed in the queue.</li></ul><h2 id="Code-sample"><a href="#Code-sample" class="headerlink" title="Code sample"></a>Code sample</h2><ul><li>ResourceFacade</li></ul><pre><code class="java">public interface ResourceFacade &#123;    void dispose();&#125;</code></pre><ul><li>Resource: in realword, it can be a object, that we need monitor</li></ul><pre><code class="java">import java.util.concurrent.atomic.AtomicLong;public class Resource implements ResourceFacade &#123;    public static AtomicLong GLOBAL_ALLOCATED = new AtomicLong();     public static AtomicLong GLOBAL_RELEASED = new AtomicLong();         protected boolean disposed;    private final long number;        public Resource() &#123;        number = GLOBAL_ALLOCATED.incrementAndGet();    &#125;        public synchronized void dispose() &#123;        if (!disposed) &#123;            disposed = true;            releaseResources();        &#125;    &#125;    protected void releaseResources() &#123;        GLOBAL_RELEASED.incrementAndGet();    &#125;    public long number() &#123;        return number;    &#125;&#125;</code></pre><ul><li>PhantomHandle: the purpose is wrap <code>Resource</code>, this a way to get <code>Resource</code> at <code>PhantomResourceRef</code></li></ul><pre><code class="java">public class PhantomHandle implements ResourceFacade &#123;    private final Resource resource;        public PhantomHandle(Resource resource) &#123;        this.resource = resource;    &#125;    public void dispose() &#123;        resource.dispose();    &#125;            Resource getResource() &#123;        return resource;    &#125;&#125;</code></pre><ul><li>PhantomResourceRef</li></ul><pre><code class="java">public class PhantomResourceRef extends PhantomReference&lt;PhantomHandle&gt; &#123;    static boolean isTraceEnabled = true;    private Resource resource;    private TraceRecord traceRecord;    public PhantomResourceRef(PhantomHandle referent, ReferenceQueue&lt;? super PhantomHandle&gt; q) &#123;        super(referent, q);        this.resource = referent.getResource();        if (isTraceEnabled) &#123;            traceRecord = new TraceRecord(StackWalker.getInstance().walk(s -&gt; s.collect(Collectors.toList())));        &#125;    &#125;    public void dispose() &#123;        Resource r = resource;        if (r != null) &#123;            System.out.println(this.getClass().getSimpleName() + &quot; dispose, number = &quot; + r.number());            r.dispose();        &#125;        if (isTraceEnabled) &#123;            System.out.println(traceRecord.toString());        &#125;    &#125;&#125;</code></pre><ul><li>TraceRecord: Optional, for tracing where instance has been created, but not be invoke disposed</li></ul><pre><code class="java">class TraceRecord &#123;    private final List&lt;StackWalker.StackFrame&gt; stackFrames;    TraceRecord(List&lt;StackWalker.StackFrame&gt; stackFrames) &#123;        this.stackFrames = stackFrames;    &#125;    @Override    public String toString() &#123;        StringBuilder buf = new StringBuilder();        this.stackFrames.subList(3, this.stackFrames.size()).forEach(stackFrame -&gt; &#123;            buf.append(&quot;\t&quot;);            buf.append(stackFrame.getClassName());            buf.append(&quot;#&quot;);            buf.append(stackFrame.getMethodName());            buf.append(&quot;:&quot;);            buf.append(stackFrame.getLineNumber());            buf.append(&quot;\n&quot;);        &#125;);        return buf.toString();    &#125;&#125;</code></pre><ul><li>Create resource and track, not only “log” the leak detections, we can alose “auto” dispose Resource</li></ul><pre><code class="java">    private static final Set&lt;PhantomResourceRef&gt; REFERENCES = Collections.synchronizedSet(new HashSet&lt;PhantomResourceRef&gt;());    public static final ReferenceQueue&lt;PhantomHandle&gt; REFERENCE_QUEUE = new ReferenceQueue&lt;&gt;();    public static ResourceFacade newResource() &#123;        Resource resource = new Resource();        PhantomHandle handle = new PhantomHandle(resource);        PhantomResourceRef ref = new PhantomResourceRef(handle, REFERENCE_QUEUE);        REFERENCES.add(ref);        return handle;    &#125;    public static void track() &#123;        Reference&lt;?&gt; referenceFromQueue;        while ((referenceFromQueue = REFERENCE_QUEUE.poll()) != null) &#123;            PhantomResourceRef ref = ((PhantomResourceRef) referenceFromQueue);            ref.dispose();            referenceFromQueue.clear();            REFERENCES.remove(ref);        &#125;    &#125;</code></pre><ul><li>Testing</li></ul><pre><code class="java">    @Test    public void test() throws InterruptedException &#123;        ResourceFacade rf = PhantomResourceFactory.newResource();        rf = null;        System.gc();        Thread.sleep(1000);        PhantomResourceFactory.track();    &#125;</code></pre><p>output:</p><pre><code>PhantomResourceRef dispose, number = 1</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> leak </tag>
            
            <tag> leak detect </tag>
            
            <tag> phantomreference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rabbitmq - Test performance tool</title>
      <link href="/2022/03/RabbitMQ/RabbitMQ_TestPerformance_Tool/"/>
      <url>/2022/03/RabbitMQ/RabbitMQ_TestPerformance_Tool/</url>
      
        <content type="html"><![CDATA[<h1 id="Tool-for-test-performance-RabbitMQ"><a href="#Tool-for-test-performance-RabbitMQ" class="headerlink" title="Tool for test performance RabbitMQ"></a>Tool for test performance RabbitMQ</h1><p><a href="https://github.com/rabbitmq/rabbitmq-perf-test">https://github.com/rabbitmq/rabbitmq-perf-test</a></p><p>TLDR:</p><ul><li><a href="https://rabbitmq.github.io/rabbitmq-perf-test/stable/htmlsingle">https://rabbitmq.github.io/rabbitmq-perf-test/stable/htmlsingle</a></li><li><a href="https://github.com/rabbitmq/rabbitmq-perf-test/tree/main/html#supported-scenario-parameters">https://github.com/rabbitmq/rabbitmq-perf-test/tree/main/html#supported-scenario-parameters</a></li></ul><p>Have 2 ways to run the test tool:</p><h2 id="Using-PerfTest"><a href="#Using-PerfTest" class="headerlink" title="Using PerfTest"></a>Using <code>PerfTest</code></h2><p>The spec of the scenario will be set by arguments of the command. The result is a log file or metric.</p><p>Command run the example: </p><pre><code>bin/runjava com.rabbitmq.perf.PerfTest -x 2 -y 4 -h amqp://tung:tung@localhost:5672 --queue-pattern &#39;perf-test-%d&#39;  --queue-pattern-from 1 --queue-pattern-to 10</code></pre><h2 id="Using-PerfTestMulti"><a href="#Using-PerfTestMulti" class="headerlink" title="Using PerfTestMulti"></a>Using <code>PerfTestMulti</code></h2><p>The spec will be input in single <code>.js</code> file. The result is another <code>.js</code> file and can be visualized by graph WebUI</p><p>Command run the example: </p><pre><code>bin/runjava com.rabbitmq.perf.PerfTestMulti publish-consume-spec.js publish-consume-result.js</code></pre><p>&#x3D;&gt; (2) more friendly</p><ul><li><p><code>publish-consume-spec.js</code> - format example: <a href="https://raw.githubusercontent.com/rabbitmq/rabbitmq-perf-test/main/html/examples/publish-consume-spec.js">https://raw.githubusercontent.com/rabbitmq/rabbitmq-perf-test/main/html/examples/publish-consume-spec.js</a><br>or <a href="https://raw.githubusercontent.com/rabbitmq/rabbitmq-perf-test/main/html/examples/various-spec.js">https://raw.githubusercontent.com/rabbitmq/rabbitmq-perf-test/main/html/examples/various-spec.js</a>  . More parameters: <a href="https://github.com/rabbitmq/rabbitmq-perf-test/tree/main/html#supported-scenario-parameters">https://github.com/rabbitmq/rabbitmq-perf-test/tree/main/html#supported-scenario-parameters</a></p></li><li><p><code>publish-consume-result.js</code> - result file. The tool will append JSON results here. </p></li><li><p>Copy <code>publish-consume-result.js</code> to <code>/html/examples/publish-consume-result.js</code>. Start webserver to view graph by the command <code>bin/runjava com.rabbitmq.perf.WebServer</code> then goto: <a href="http://localhost:8080/examples/sample.html">http://localhost:8080/examples/sample.html</a></p></li></ul><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><ul><li>If rabbitmq has authen: Declare uri: <code>&#39;uri&#39;: &#39;amqp://username:pass@localhost:5672&#39;</code></li></ul><h2 id="Bonus"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus</h2><ul><li>Run rabbitmq</li></ul><pre><code>docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 -e RABBITMQ_DEFAULT_USER=tung -e RABBITMQ_DEFAULT_PASS=tung rabbitmq:management</code></pre><ul><li>Tool download</li></ul><pre><code>https://github.com/rabbitmq/rabbitmq-perf-test/releases/download/v2.16.0/rabbitmq-perf-test-2.16.0-bin.tar.gz</code></pre><ul><li>Test result on my pc env</li><li>CPU: AMD 5600G - 6 core 12 threads</li><li>RabbitMQ: Docker 3.9.13 (latest)</li><li>Tool test: rabbitmq-perf-test 2.16.0 (latest)</li><li>Scenario spec</li></ul><pre><code>[&#123;&#39;name&#39;: &#39;consume&#39;, &#39;type&#39;: &#39;simple&#39;,&#39;uri&#39;: &#39;amqp://tung:tung@localhost:5672&#39;,&#39;params&#39;:    [&#123;&#39;time-limit&#39;: 300, &#39;producer-count&#39;: 2, &#39;consumer-count&#39;: 4&#125;]&#125;]</code></pre><ul><li>Result:</li></ul><p>From tool:<br><img src="https://user-images.githubusercontent.com/81145350/156550087-ea493a5d-1f99-4d37-819e-81292205d39f.png" alt="image"><br>From Rabbitmq Webadmin<br><img src="https://user-images.githubusercontent.com/81145350/156550210-1ff39dd3-e6f4-4f31-b487-1de52ff45733.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> rabbitmq </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rabbitmq </tag>
            
            <tag> test tool </tag>
            
            <tag> performance test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache James - Note</title>
      <link href="/2022/03/Other/ApacheJames/"/>
      <url>/2022/03/Other/ApacheJames/</url>
      
        <content type="html"><![CDATA[<h1 id="Apache-James-Note"><a href="#Apache-James-Note" class="headerlink" title="Apache James - Note"></a>Apache James - Note</h1><p>todo</p><pre><code class="bash">source 2_sandbox_env.txtecho &quot;USER_COUNT = $USER_COUNT&quot;echo &quot;DURATION = $DURATION&quot;echo &quot;MAX_DURATION = $MAX_DURATION&quot; cd james-gatlingif [[ &quot;$1&quot; = &quot;imap&quot; ]]; then   echo &quot;IMAP test&quot;  sbt &quot;gatling:testOnly org.apache.james.gatling.simulation.imap.PlatformValidationSimulation&quot;;fiif [[ &quot;$1&quot; = &quot;jmap&quot; ]]; then  echo &quot;JMAP test&quot;  sbt &quot;gatling:testOnly org.apache.james.gatling.simulation.jmap.rfc8621.PushPlatformValidationSimulation&quot;;  fi</code></pre><h2 id="Alternative"><a href="#Alternative" class="headerlink" title="Alternative"></a>Alternative</h2><ol><li>sudo su &#x3D;&gt; vào quyền root</li><li>cd &#x2F;root&#x2F;upn&#x2F;james&#x2F;james-gatling<br>  sbt &#x3D;&gt; chạy simulation cho imap và jmap<br>  imap:<br>  source imap_env.sh<br>  gatling:testOnly org.apache.james.gatling.simulation.imap.PlatformValidationSimulation<br>  jmap-draft:<br>  gatling:testOnly org.apache.james.gatling.simulation.jmap.draft.PlatformValidationSimulation<br>  jmap-rfc:<br>  gatling:testOnly org.apache.james.gatling.simulation.jmap.rfc8621.PushPlatformValidationSimulation</li></ol><ul><li>remember to warmup James 10-20m<br>  Normal run (release perf test): 2 pods, 10k user imap&#x2F;jmap-draft, 6k users jmap rfc</li></ul>]]></content>
      
      
      <categories>
          
          <category> apache james </category>
          
      </categories>
      
      
        <tags>
            
            <tag> apache james </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rate Limiting Algorithm</title>
      <link href="/2022/02/Other/RateLimitingAgorithm/"/>
      <url>/2022/02/Other/RateLimitingAgorithm/</url>
      
        <content type="html"><![CDATA[<h1 id="Rate-Limiting-Algorithm"><a href="#Rate-Limiting-Algorithm" class="headerlink" title="Rate Limiting Algorithm"></a>Rate Limiting Algorithm</h1><ol><li>Token Bucket</li><li>Leaking Bucket</li><li>Fixed Window counter</li><li>Sliding window log</li><li>Sliding window counter</li></ol><h2 id="Token-bucket"><a href="#Token-bucket" class="headerlink" title="Token bucket"></a>Token bucket</h2><p>Idea:</p><ul><li>There are a few tokens in a bucket.</li><li>When a request comes, a token has to be taken from the bucket for it to be processed. If there is no token available in the bucket, the request will be rejected and the requester has to retry later.</li><li>The token bucket is also refilled per time unit.</li></ul><p>2 part:</p><ul><li>refiller: interval put new tokens to bucket<br>(condition: current tokens in bucket + add tokens &lt;&#x3D; bucket capacity)</li><li>request processor: checking “enough tokens” for requests</li></ul><p>Ex:</p><ul><li>request rate: 10 request&#x2F;hour</li><li>refill rate: 1 request&#x2F;min</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rate_limiting/tokenBucketEx1.png" alt="TokenBucket"></p><p>Problem?</p><ul><li>rate limit &#x3D;  1000&#x2F;hour, refill rate &#x3D; 10&#x2F;second</li><li>consumers can has a 1000 accept requests&#x2F;second when bucket full. But then request 1001th is rejected</li><li>If we want to stable outflow rate is needed?</li></ul><h2 id="Leaky-Bucket"><a href="#Leaky-Bucket" class="headerlink" title="Leaky Bucket"></a>Leaky Bucket</h2><ul><li>similar to the Token Bucket</li><li>except that requests are processed at a fixed rate</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rate_limiting/LeakyBucketEx1.png" alt="Leaky Bucket"></p><p>Problem<br>Q: What happens if queues full and bucket still NOT full?<br>A : Requests are discarded (or leaked).</p><h3 id="Compare-Leaky-Bucket-vs-Token-Bucket"><a href="#Compare-Leaky-Bucket-vs-Token-Bucket" class="headerlink" title="Compare Leaky Bucket vs Token Bucket:"></a>Compare Leaky Bucket vs Token Bucket:</h3><ul><li>Token Bucket:<ul><li>Can send Large bursts can faster rate</li><li>Not suitable for some use cases (stable outflow rate is needed)</li></ul></li><li>Leaky Bucket:<ul><li>Requests are processed at a fixed rate (suitable for use cases that a stable outflow rate is needed)</li><li>There are two parameters in the algorithm (queue + bucket). It might not be easy to tune them properly</li></ul></li></ul><h2 id="Fixed-Window-counter"><a href="#Fixed-Window-counter" class="headerlink" title="Fixed Window counter"></a>Fixed Window counter</h2><ul><li>divides the timeline into fix-sized time windows</li><li>assign a counter for each window</li><li>each incoming request increments the counter for the window</li><li>once the counter reaches the pre-defined threshold,<br>new requests are dropped until a new time window starts</li></ul><p>Ex:</p><ul><li>window size such as 60 or 600 seconds…</li><li>the windows are typically defined by the floor of the current timestamp,<br>so 10:00:06 with a 60 second window length, would be in the 10:00:00 windo</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rate_limiting/FixedWindowCounter.png" alt="Fixed Window Counter"></p><p>Problem?</p><ul><li>rate limit &#x3D;  10 request&#x2F;min</li><li>send 10 request at 2:00:59 → Acceptable</li><li>send 10 request at 2:01:01 → Acceptable</li><li>Duration(2:01:01, 2:00:59) &#x3D; 2 seconds</li><li>Finally, 10+10&#x3D;20 request&#x2F; 2 seconds has been acceptable</li></ul><p>&#x3D;&gt; Edge problem</p><p>Cons?</p><ul><li>Spike in traffic at the edges of a window could cause more requests than the allowed quota to go though</li><li>if many consumers wait for a reset window, they may stampede your API at the same time at the top of the hour.</li></ul><h2 id="Sliding-window-log"><a href="#Sliding-window-log" class="headerlink" title="Sliding window log"></a>Sliding window log</h2><ul><li>The algorithm keeps track of request timestamps. (log for each consumer request)</li><li>When a new request comes in, remove all the outdated timestamps<br>(Outdated timestamps are defined as those older than the start of the current time window )</li><li>Add timestamp of the new request to the log</li><li>If the log size is the equal or lower than the allowed count, a request is accepted.<br>Otherwise, it is rejected</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rate_limiting/SlideWindowLog1.png" alt="Sliding window log"></p><p>Pros?</p><ul><li>very accurate</li><li>not suffer from the boundary conditions of fixed windows</li></ul><p>Cons?</p><ul><li>consumes a lot of memory (even if a request is rejected, its timestamp might still be stored in memory)</li><li>expensive to compute (add&#x2F;remove log, sort…)</li></ul><h2 id="Sliding-window-counter"><a href="#Sliding-window-counter" class="headerlink" title="Sliding window counter"></a>Sliding window counter</h2><ul><li>A hybrid approach that combines the low processing cost of the fixed window algorithm,<br>and the improved boundary conditions of the sliding log algorithm.</li><li>Sliding Window tries to fix “edge problem” by taking the previous counter into account</li></ul><p>Explain it with an example:</p><ul><li>rate limit &#x3D; 10&#x2F;minute</li><li>9 requests in window [00:00, 00:01]</li><li>5 requests in window [00:01, 00:02]</li><li>For a request 6th arrives at 00:01:15, which is at 25% position of window [00:01, 00:02]</li><li>Calculate the request count by the formula: 9 x (1 - 25%) + 5 &#x3D; 11.75 &gt; 10 &#x3D;&gt; reject</li><li>If 6th request arrives at 00:01:30<br>9x50% + 5 &#x3D; 9.5 &lt; 10 &#x3D;&gt; accept</li></ul><p>Note:</p><ul><li>ec &#x3D; previous counter * ((time unit - time into the current counter) &#x2F; time unit) + current counter</li><li>This is still not accurate because it assumes that the distribution of requests in previous window is even, which may not be true. (But better than Fixed Window)</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rate_limiting/SlidingWindowCounter1.png" alt="Sliding window counter"></p><h2 id="Bonus"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus</h2><ul><li><a href="https://github.com/mokies/ratelimitj">https://github.com/mokies/ratelimitj</a></li><li><a href="https://www.quinbay.com/blog/understanding-rate-limiting-algorithms">https://www.quinbay.com/blog/understanding-rate-limiting-algorithms</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> rate limiting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Proxy chain</title>
      <link href="/2022/02/Java/ProxyChaine/"/>
      <url>/2022/02/Java/ProxyChaine/</url>
      
        <content type="html"><![CDATA[<h1 id="Proxy-Chain"><a href="#Proxy-Chain" class="headerlink" title="Proxy Chain"></a>Proxy Chain</h1><h2 id="Use-Case"><a href="#Use-Case" class="headerlink" title="Use Case"></a>Use Case</h2><ul><li>You have a proxy URL that requires authentication via username and password.</li><li>Your tool needs to configure that proxy.</li><li>Your tool would be easier to set up if it could use a proxy without authentication.</li></ul><h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><ul><li>We need a “middle proxy server” that will be responsible for forwarding the traffic.</li></ul><hr><h3 id="Detailed-Solution"><a href="#Detailed-Solution" class="headerlink" title="Detailed Solution"></a>Detailed Solution</h3><ul><li><strong>Middle Proxy Server</strong>: Set up a middle proxy server to forward all traffic from your tool, allowing it to use the proxy without the need for direct authentication.</li></ul><h2 id="Code-example"><a href="#Code-example" class="headerlink" title="Code example"></a>Code example</h2><ul><li>Maven:</li></ul><pre><code class="xml">        &lt;dependency&gt;            &lt;groupId&gt;net.lightbody.bmp&lt;/groupId&gt;            &lt;artifactId&gt;browsermob-core&lt;/artifactId&gt;            &lt;version&gt;2.1.5&lt;/version&gt;        &lt;/dependency&gt;</code></pre><ul><li>Sample code</li></ul><pre><code class="java">import java.net.InetSocketAddress;import java.util.concurrent.TimeUnit;import org.junit.jupiter.api.Test;import org.openqa.selenium.Proxy;import org.openqa.selenium.WebDriver;import org.openqa.selenium.chrome.ChromeDriver;import org.openqa.selenium.chrome.ChromeOptions;import io.github.bonigarcia.wdm.WebDriverManager;import me.tungexplorer.travian.service.crawl.browser.manager.ChromeBrowser;import net.lightbody.bmp.BrowserMobProxy;import net.lightbody.bmp.BrowserMobProxyServer;import net.lightbody.bmp.proxy.auth.AuthType;public class SeleniumChangeProxyAgentTest &#123;    @Test    void test() throws InterruptedException &#123;        // Setup WebDriverManager to use ChromeDriver with a specific version        WebDriverManager.chromedriver().driverVersion(&quot;96.0.4664.93&quot;).setup();        // Create ChromeOptions object and configure user agent and ignore certificate errors        ChromeOptions chromeOptions = new ChromeOptions();        chromeOptions.addArguments(&quot;user-agent=Tung Agent&quot;, &quot;--ignore-certificate-errors&quot;);        // Setup the proxy chain        BrowserMobProxy chainedProxy = new BrowserMobProxyServer();        // Configure the chained proxy with IP address and port        chainedProxy.setChainedProxy(new InetSocketAddress(&quot;182.54.239.111&quot;, 8160));        // Set proxy authentication        chainedProxy.chainedProxyAuthorization(&quot;user1&quot;, &quot;pass2&quot;, AuthType.BASIC);        // Allow trusting all servers        chainedProxy.setTrustAllServers(true);        // Start the proxy        chainedProxy.start(0);        // Create a Proxy object for Selenium and configure it as MANUAL        Proxy seleniumProxy = new Proxy();        seleniumProxy.setProxyType(Proxy.ProxyType.MANUAL);        // Get the port of the chained proxy and configure it for Selenium Proxy        String proxyStr = String.format(&quot;%s:%d&quot;, &quot;localhost&quot;, chainedProxy.getPort());        seleniumProxy.setHttpProxy(proxyStr);        seleniumProxy.setSslProxy(proxyStr);        // Add proxy to ChromeOptions        chromeOptions.setCapability(&quot;proxy&quot;, seleniumProxy);        // Create a ChromeBrowser object using ChromeDriver with the configured options        ChromeBrowser chromeBrowser = new ChromeBrowser(new ChromeDriver(chromeOptions), &quot;Chrome-Test1&quot;);        WebDriver webDriver = chromeBrowser.getWebDriver();        // Navigate to a website to check the IP address        webDriver.get(&quot;https://checkip.org/&quot;);        // Wait for a few seconds to observe the result, then close the browser        TimeUnit.SECONDS.sleep(10);        // Quit the WebDriver and stop the proxy        webDriver.quit();        chainedProxy.stop();    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> proxy </tag>
            
            <tag> selenium </tag>
            
            <tag> proxy chained </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lab - KrakenD + Keycloak + SSO, SLO</title>
      <link href="/2022/01/Other/Oauth2_KrakenD_Keycloak_SSO_SLO/"/>
      <url>/2022/01/Other/Oauth2_KrakenD_Keycloak_SSO_SLO/</url>
      
        <content type="html"><![CDATA[<h1 id="Lab-KrakenD-Keycloak-SSO-SLO"><a href="#Lab-KrakenD-Keycloak-SSO-SLO" class="headerlink" title="Lab - KrakenD + Keycloak: SSO, SLO"></a>Lab - KrakenD + Keycloak: SSO, SLO</h1><h2 id="API-backend-service"><a href="#API-backend-service" class="headerlink" title="API backend service"></a>API backend service</h2><ul><li><a href="https://end4tawjnxl4skw.m.pipedream.net/">https://end4tawjnxl4skw.m.pipedream.net</a> (This is a public endpoint, no authentication. Easy to register<br>at <a href="https://requestbin.com/">https://requestbin.com</a>)</li><li>Verify</li></ul><pre><code class="bash">curl https://end4tawjnxl4skw.m.pipedream.net/api/pingPONG</code></pre><h2 id="Krakend-Api-Gateway"><a href="#Krakend-Api-Gateway" class="headerlink" title="Krakend Api Gateway"></a>Krakend Api Gateway</h2><ul><li>Routing: <a href="http://localhost:8888/ping">http://localhost:8888/ping</a> -&gt; <a href="https://end4tawjnxl4skw.m.pipedream.net/api/ping">https://end4tawjnxl4skw.m.pipedream.net/api/ping</a></li><li>Run<br>command: <code>docker run -p 8888:8080 -p 9999:1234 -v &quot;$&#123;PWD&#125;:/etc/krakend/&quot; devopsfaith/krakend run -d -c krakend.json</code></li><li>Don’t forget:<ul><li>8888: api gateway port -&gt; <a href="http://localhost:8888/">http://localhost:8888</a></li><li>9999: bloomfilter rpc port (RPC Protocol)</li><li>This is container network, becareful when declare private host</li></ul></li></ul><pre><code class="json">&#123;  &quot;version&quot;: 2,  &quot;extra_config&quot;: &#123;    &quot;github_com/devopsfaith/krakend-gologging&quot;: &#123;      &quot;level&quot;: &quot;DEBUG&quot;,      &quot;prefix&quot;: &quot;[KRAKEND]&quot;,      &quot;syslog&quot;: false,      &quot;stdout&quot;: true,      &quot;format&quot;: &quot;default&quot;    &#125;  &#125;,  &quot;timeout&quot;: &quot;3000ms&quot;,  &quot;cache_ttl&quot;: &quot;300s&quot;,  &quot;output_encoding&quot;: &quot;json&quot;,  &quot;name&quot;: &quot;krakend_keycloak&quot;,  &quot;endpoints&quot;: [    &#123;      &quot;endpoint&quot;: &quot;/ping&quot;,      &quot;method&quot;: &quot;GET&quot;,      &quot;output_encoding&quot;: &quot;string&quot;,      &quot;extra_config&quot;: &#123;        &quot;github.com/devopsfaith/krakend-jose/validator&quot;: &#123;          &quot;alg&quot;: &quot;RS256&quot;,          &quot;jwk-url&quot;: &quot;http://172.17.0.1:8180/auth/realms/oauth2-demo-realm/protocol/openid-connect/certs&quot;,          &quot;disable_jwk_security&quot;: true,          &quot;propagate-claims&quot;: [            [              &quot;email&quot;,              &quot;X-APP-USER&quot;            ]          ]        &#125;,        &quot;github_com/devopsfaith/bloomfilter&quot;: &#123;          &quot;N&quot;: 10000000,          &quot;P&quot;: 0.0000001,          &quot;HashName&quot;: &quot;optimal&quot;,          &quot;TTL&quot;: 1500,          &quot;port&quot;: 1234,          &quot;TokenKeys&quot;: [            &quot;sid&quot;          ]        &#125;      &#125;,      &quot;backend&quot;: [        &#123;          &quot;url_pattern&quot;: &quot;/api/ping&quot;,          &quot;encoding&quot;: &quot;string&quot;,          &quot;method&quot;: &quot;GET&quot;,          &quot;extra_config&quot;: &#123;&#125;,          &quot;host&quot;: [            &quot;https://end4tawjnxl4skw.m.pipedream.net&quot;          ],          &quot;disable_host_sanitize&quot;: false        &#125;      ],      &quot;headers_to_pass&quot;: [        &quot;Accept&quot;,        &quot;Content-Type&quot;,        &quot;Authorization&quot;,        &quot;X-APP-USER&quot;      ]    &#125;  ]&#125;</code></pre><ul><li>For Authen by JWT<ul><li>extra_config <code>github.com/devopsfaith/krakend-jose/validator</code></li><li>propagate-claims: help you claim value from jwt token, and forward this value to backend api.</li><li>NOTE: Don’t forget declare <code>headers_to_pass</code>: X-APP-USER, Authorization</li></ul></li><li><h2 id="For-Single-Log-Out-SLO-extra-config-github-com-devopsfaith-bloomfilter-1234-port-for-adding-sid-to-bloomfilter"><a href="#For-Single-Log-Out-SLO-extra-config-github-com-devopsfaith-bloomfilter-1234-port-for-adding-sid-to-bloomfilter" class="headerlink" title="For Single Log Out (SLO)  - extra_config github_com/devopsfaith/bloomfilter  - 1234 port for adding sid to bloomfilter"></a>For Single Log Out (SLO)<br>  - extra_config <code>github_com/devopsfaith/bloomfilter</code><br>  - <code>1234</code> port for adding <code>sid</code> to bloomfilter</h2>ref <a href="https://www.krakend.io/docs/authorization/revoking-tokens/">https://www.krakend.io/docs/authorization/revoking-tokens/</a></li></ul><h2 id="Keycloak"><a href="#Keycloak" class="headerlink" title="Keycloak"></a>Keycloak</h2><ul><li>Run<br>command <code>docker run -p 8180:8080 -e KEYCLOAK_USER=admin -e KEYCLOAK_PASSWORD=admin quay.io/keycloak/keycloak:latest</code></li><li>Create realm: oauth2-demo-realm</li><li>Create client: oauth2-demo-pkce-client</li><li>Create user: user1-pass</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/oauth2/keycloak_oauth_client_declare.png" alt="keycloak_oauth_client_declare"></p><ul><li>Don’t forget:<ul><li><code>Access Type = public</code>: if you use public app, like as AngularJs</li><li><code>Valid Redirect URI</code>: this is rule, if not define exactly, you will got “Bad request”</li><li><code>Web Origins = * </code>: for CORS</li><li><code>BackChannel Logout URL</code> : for callback when has logout event</li></ul></li></ul><h2 id="How-to-get-JWT-Token"><a href="#How-to-get-JWT-Token" class="headerlink" title="How to get JWT Token?"></a>How to get JWT Token?</h2><ol><li>Using postman</li></ol><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/oauth2/oauth2_getTokenByPostman.png" alt="oauth2_getTokenByPostman"></p><ol start="2"><li>Using sample app - code by<br>AngularJs: <a href="https://github.com/tungtv202/oauth2-pkce-demo-frontend-only">https://github.com/tungtv202/oauth2-pkce-demo-frontend-only</a></li></ol><ul><li>Easy to login&#x2F;logout by UI</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/oauth2/oauth2-frontend-demo.png" alt="oauth2-frontend-demo"></p><h2 id="Authentication-Testing"><a href="#Authentication-Testing" class="headerlink" title="Authentication Testing"></a>Authentication Testing</h2><ol><li>Scenario 1: Call to API Gateway without token</li></ol><ul><li><code>GET http://localhost:8888/ping</code> - without <code>Authorization</code> header</li><li>KrakenD log: <code>Error #01: Token not found</code></li><li>HTTP response: <code>401</code></li></ul><ol start="2"><li>Scenario 2: Call to API Gateway with token</li></ol><pre><code class="bash">curl --location --request GET &#39;http://localhost:8888/ping&#39; --header &#39;Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia...&#39;</code></pre><ul><li>HTTP Respone: <code>200 - PONG</code></li></ul><h2 id="Single-Log-Out-Revoke-Token"><a href="#Single-Log-Out-Revoke-Token" class="headerlink" title="Single Log Out - Revoke Token"></a>Single Log Out - Revoke Token</h2><ol><li>Mechanism</li></ol><ul><li>Define callback endpoint at keycloak client setting page. (Backchannel Logout URL input form)</li><li>When someone user logout, Keycloak will call HTTP Request to callback endpoint</li><li>Request sample:<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/oauth2/oauth2_keycloak_logout_callback_request_sample.png" alt="oauth2_keycloak_logout_callback_request_sample"></li></ul><pre><code>logout_token=eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJCMHNsOGpwaUtMMmZyR1lMaWNHTEZURFptTVJWRVVlT01Dcmg0QTNhSG9FIn0.eyJpYXQiOjE2NDE2NzIyMjUsImp0aSI6IjBhYmFmNmZmLTljZmYtNGEzZS1hZTljLTA1MTQ4MGQwMTk2NCIsImlzcyI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODE4MC9hdXRoL3JlYWxtcy9vYXV0aDItZGVtby1yZWFsbSIsImF1ZCI6Im9hdXRoMi1kZW1vLXBrY2UtY2xpZW50Iiwic3ViIjoiZDg3NTYzZTYtODhkYS00YTZhLWIyYWMtMmFhZjhlYWY2ZTk1IiwidHlwIjoiTG9nb3V0Iiwic2lkIjoiMDgyMzU2ZGEtNWY3ZS00ZDJkLWE3OWItYTUzMTY1OTRhYTFmIiwiZXZlbnRzIjp7Imh0dHA6Ly9zY2hlbWFzLm9wZW5pZC5uZXQvZXZlbnQvYmFja2NoYW5uZWwtbG9nb3V0Ijp7fSwicmV2b2tlX29mZmxpbmVfYWNjZXNzIjp0cnVlfX0.FeykAYZZj4ehS_43Xjmge7t0mUPyUx8TCcvT8tA32n8eZWbpG5zRRcgR67Lm0CJiKOCwoug4rzHND-DOJ6K_cfocW7PkUoUObPsefAuz5Ljfd9ajIazkQiDCMguLTlEDl3M7pd3vY8W919_Vj9kZ0Or2-UZJlZ7mp5tsPzXi4WxxIpG5Z7f_rX6FblYjqXHV9gFfa1759ngLFFMwUkXoiyHXJ558Pze9RRIjqxbegm8tYWQmGcZxFpdneTWFLFgB2FRT6r2qtwJ76WAT_F8YgO2t76s62hf-Mb8rfg3ahXiEkOr6jMahdutkBI2IRDwkyiz1bIcOx1HEa9iILLzqYg</code></pre><p>Payload Decode:</p><pre><code class="json">&#123;  &quot;iat&quot;: 1641672225,  &quot;jti&quot;: &quot;0abaf6ff-9cff-4a3e-ae9c-051480d01964&quot;,  &quot;iss&quot;: &quot;http://localhost:8180/auth/realms/oauth2-demo-realm&quot;,  &quot;aud&quot;: &quot;oauth2-demo-pkce-client&quot;,  &quot;sub&quot;: &quot;d87563e6-88da-4a6a-b2ac-2aaf8eaf6e95&quot;,  &quot;typ&quot;: &quot;Logout&quot;,  &quot;sid&quot;: &quot;082356da-5f7e-4d2d-a79b-a5316594aa1f&quot;,  &quot;events&quot;: &#123;    &quot;http://schemas.openid.net/event/backchannel-logout&quot;: &#123;&#125;,    &quot;revoke_offline_access&quot;: true  &#125;&#125;</code></pre><ul><li>We need a <code>middleware</code>:<ul><li>Receive HTTP request from keycloak</li><li>Claim <code>sid</code> value</li><li>Add <code>sid</code> value to bloom filter of KrakenD via RPC Endpoint: <code>krakend:1234</code></li></ul></li><li>When the request to Apigateway has <code>sid</code> (that claim from jwt token), that existing in bloom filter, the request should be rejected</li></ul><ol start="2"><li>Demo 1</li></ol><ul><li>Sample tool for add <code>sid</code> to bloom<br>filter: <a href="https://github.com/devopsfaith/krakend-playground/tree/master/jwt-revoker">https://github.com/devopsfaith/krakend-playground/tree/master/jwt-revoker</a></li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/oauth2/oauth2_krakend_rpc_client_tool_1.png" alt="oauth2_krakend_rpc_client_tool_1"></p><ul><li>Fukkking: when I try to add new sid, I got error: <code>error on adding bloomfilter: connection is shut down</code><br>Ref: <a href="https://githubmate.com/repo/devopsfaith/bloomfilter/issues/11">https://githubmate.com/repo/devopsfaith/bloomfilter/issues/11</a></li></ul><ol start="3"><li>Demo 2</li></ol><ul><li>More modern tool <a href="https://github.com/tungtv202/go_jwt_revoker.git">https://github.com/tungtv202/go_jwt_revoker.git</a><ul><li>Receive JWTK Token</li><li>Claim <code>sid</code> value</li><li>Add <code>sid</code> to bloomfilter</li><li>NO UI</li></ul></li><li>Still fail: <code>error on adding bloomfilter: connection is shut down</code></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sso </tag>
            
            <tag> krakend </tag>
            
            <tag> oauth2 </tag>
            
            <tag> keycloak </tag>
            
            <tag> slo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keycloak</title>
      <link href="/2021/12/Other/Keycloak/"/>
      <url>/2021/12/Other/Keycloak/</url>
      
        <content type="html"><![CDATA[<h1 id="Keycloak"><a href="#Keycloak" class="headerlink" title="Keycloak"></a>Keycloak</h1><h2 id="Quick-start"><a href="#Quick-start" class="headerlink" title="Quick start"></a>Quick start</h2><pre><code class="bash">docker run -p 8080:8080 -e KEYCLOAK_USER=admin -e KEYCLOAK_PASSWORD=admin quay.io/keycloak/keycloak:latest</code></pre><p><a href="http://localhost:8080/auth/admin/">http://localhost:8080/auth/admin/</a></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><h3 id="Realm"><a href="#Realm" class="headerlink" title="Realm"></a>Realm</h3><ul><li>Master realm: this realm was created for you when you first started Keycloak, It contains the admin<br>account you created at the first login. You use this realm only to create other realms</li><li>Other realms: these realms are created by the admin in the master realm. In these realms, administrators<br>create users and applications. The applications are owned by the users.</li></ul><h3 id="Access-Type-oidc"><a href="#Access-Type-oidc" class="headerlink" title="Access Type (oidc)"></a>Access Type (oidc)</h3><ul><li>confidental &#x3D; basic authorization_code (Need <code>client_secret</code>)</li><li>public &#x3D; PKCE (when SPA like reactjs app)</li></ul><h3 id="Use-case"><a href="#Use-case" class="headerlink" title="Use case"></a>Use case</h3><ul><li>Keycloak vs Krakend<ul><li>SHOULD WE ASSIGN AUTHENTICATION DUTY TO API GATEWAY?</li><li>SHOULD WE USE KEYCLOAK LIKE AS AUTHORIZATION SEVER IN “INTERNAL” SYSTEM LIKE AS MICROSERVICE?</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sso </tag>
            
            <tag> oidc </tag>
            
            <tag> id provider </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KrakendD API Gateway</title>
      <link href="/2021/11/Other/KrakenD_API_Gateway/"/>
      <url>/2021/11/Other/KrakenD_API_Gateway/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h2><ul><li>Docker hub: <a href="https://hub.docker.com/r/devopsfaith/krakend">https://hub.docker.com/r/devopsfaith/krakend</a></li><li>Web page: <a href="https://www.krakend.io/">https://www.krakend.io</a></li><li>Github: <a href="https://github.com/luraproject/lura">https://github.com/luraproject/lura</a></li></ul><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul><li>When install, It doesn’t need a database for persisting config. (Don’t like Kong API Gateway, which needs an SQL database). All endpoint &amp; routing configs will storage in a SINGLE json file.</li><li>Sample config file: <a href="https://github.com/devopsfaith/krakend-ce/blob/master/krakend.json">https://github.com/devopsfaith/krakend-ce/blob/master/krakend.json</a></li><li>KrakenD don’t support admin web API for config. In order to generate JSON config, You need access here <a href="https://designer.krakend.io/">https://designer.krakend.io/</a>, Create it online &#x3D;&gt; download it &#x3D;&gt; moves it to the directory config of krakenD when startup. (Hope that in the future, We will have web admin for that, like as KongHQ)</li><li>We have a command for validate the json config file</li><li>If you want to customize something, like extract metric&#x2F;log for request&#x2F;response (more detailed compared to default), you need to know GoLang syntax.</li><li>It is very easy to config <code>rate limit</code> via IP or Header. This is the default feature, that don’t need to install any more plugins.</li><li>The config by <code>SINGLE</code> json files, special is <code>endpoint</code> is very “manual”, many boilerplate. EX:<ul><li>We need to declare each HTTP Method (GET&#x2F;POST&#x2F;DELETE…). We can’t group by it in one line.</li></ul></li><li>Metric is very poor: It only supports the metric for the number hit to each endpoint. (total fails, total passes…)</li><li>Log info is very poor. In <code>production</code> mode, the log info is only the access to the endpoint (Has IP, When I lab, It only a private IP, I don’t sure it support Public IP). We can’t extract request&#x2F;response body, even header info not. As I searched, the authors of KrakenD said have a plugin for extracting more info, but they also warning we should not use it in a live environment. Because performance is low. </li><li>KrakenD has a tending the restrict all by default. If you want to <code>whitelist</code> some config, you need to define it clearly. For example: If the API request will be authentication by <code>X-Token</code> Header, You need to declare the <code>X-Token</code> in while-list (headers_to_pass property), if not <code>X-Token</code> value will not forward to the backend. The same thing will happen with <code>Accept</code>,<code>Content-Type</code> headers.</li></ul><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><ul><li>According to advertisement, krakenD is better than Kong (lab: not yet)</li><li>Key is reactive (lab: not lab)</li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> krakend </tag>
            
            <tag> kong </tag>
            
            <tag> api gateway </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Web Game Supervisor</title>
      <link href="/2021/10/Stories/CrawlTravian_GameSupervisor/"/>
      <url>/2021/10/Stories/CrawlTravian_GameSupervisor/</url>
      
        <content type="html"><![CDATA[<h1 id="Web-Game-Supervisor"><a href="#Web-Game-Supervisor" class="headerlink" title="Web Game Supervisor"></a>Web Game Supervisor</h1><h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><ul><li>We have a web game (that already)</li><li>We want to follow some game accounts, we want to know which time of the day that account is active&#x2F;inactive. We want to monitor some info from that account every hour (ex: account’s score)</li><li>We want to visualize all activity about target accounts on a chart. (maybe this will support competitor for some tactic in-game)</li></ul><h2 id="Technicals-stack"><a href="#Technicals-stack" class="headerlink" title="Technicals stack"></a>Technicals stack</h2><ul><li><p>Selenium: for crawl data from the web game. </p><ul><li>login game with a setup account</li><li>access to the target account profile</li><li>detect HTML element - that has target info</li></ul></li><li><p>Spring: backend </p><ul><li>Spring Scheduler: cron trigger crawl task every hour</li></ul></li><li><p>Reactjs: frontend</p></li><li><p>Docker: deployment</p></li><li><p>Chartjs: library for the chart - that will visualize crawled data</p></li><li><p>Database: persistent data. </p><ul><li>H2 for dev, MySQL for prod.</li></ul></li><li><p>Jhipster: generate base code. </p></li><li><p>AWS Lightsail: hosting web app</p></li></ul><p>Note: this is a simple app for some users (CCU is very low). We don’t need cache or advanced technical</p><h2 id="Database-diagram"><a href="#Database-diagram" class="headerlink" title="Database diagram"></a>Database diagram</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/crawl_travian/db_diagram.png" alt="Database Diagram"></p><h2 id="Jhipster-JDL-Note"><a href="#Jhipster-JDL-Note" class="headerlink" title="Jhipster JDL Note"></a>Jhipster JDL Note</h2><ul><li>We can use <code>@readOnly</code> before the <code>entity</code> - that we want to is will only reducer&#x2F;api&#x2F;web-ui for reading data</li><li>We can use <code>filter</code> tag for want to generate advance search query support. <ul><li>Example: <code>filter UserFollowLog</code></li></ul></li></ul><h2 id="Backend-hightlight"><a href="#Backend-hightlight" class="headerlink" title="Backend hightlight"></a>Backend hightlight</h2><h3 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h3><ul><li>We can use <code>io.github.bonigarcia:webdrivermanager</code> for “binary chrome”, It will help we needn’t download <code>chromedriver</code> from somewhere on the internet, and import it to the source app, with manual config absolute-path.</li></ul><pre><code class="xml">        &lt;dependency&gt;            &lt;groupId&gt;io.github.bonigarcia&lt;/groupId&gt;            &lt;artifactId&gt;webdrivermanager&lt;/artifactId&gt;            &lt;version&gt;5.0.3&lt;/version&gt;        &lt;/dependency&gt;</code></pre><h3 id="ChromeBrowserManagement"><a href="#ChromeBrowserManagement" class="headerlink" title="ChromeBrowserManagement"></a>ChromeBrowserManagement</h3><ul><li><p>Config something related to selenium&#x2F; chrome driver</p></li><li><p>In order to crawling parallel in multiple chrome window. </p><ul><li>Example: Target User A -&gt; Chrome1, Target User B -&gt; Chrome2</li></ul></li><li><p><code>ChromeBrowserManagement.class</code> :</p></li></ul><pre><code class="java">import com.google.common.base.Preconditions;import io.github.bonigarcia.wdm.WebDriverManager;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.atomic.AtomicInteger;import lombok.extern.slf4j.Slf4j;import org.openqa.selenium.chrome.ChromeDriver;import org.openqa.selenium.chrome.ChromeOptions;import org.springframework.stereotype.Component;@Component@Slf4jpublic class ChromeBrowserManagement &#123;    public static final int MAX_BROWSER_NUMBER = 5;    private static boolean isInitConfig = false;    private static final ChromeOptions chromeOptions = new ChromeOptions();    private static final boolean isHeadLessMode = true;    private static final AtomicInteger TOTAL_BROWSER_OPENED = new AtomicInteger(0);    private final Map&lt;String, ChromeBrowser&gt; mapChromeBrowser = new ConcurrentHashMap&lt;&gt;();    public ChromeBrowser getFreeChromeBrowser(String loginUser) &#123;        ChromeBrowser chromeBrowser = null;        for (Map.Entry&lt;String, ChromeBrowser&gt; stringChromeBrowserEntry : mapChromeBrowser.entrySet()) &#123;            if (stringChromeBrowserEntry.getKey().startsWith(loginUser) &amp;&amp; !stringChromeBrowserEntry.getValue().isBusy()) &#123;                chromeBrowser = stringChromeBrowserEntry.getValue();                break;            &#125;        &#125;        if (chromeBrowser == null) &#123;            try &#123;                chromeBrowser = createNewBrowser(loginUser);            &#125; catch (IllegalArgumentException exception) &#123;                log.warn(exception.getMessage());            &#125;        &#125;        if (chromeBrowser == null) &#123;            for (Map.Entry&lt;String, ChromeBrowser&gt; stringChromeBrowserEntry : mapChromeBrowser.entrySet()) &#123;                if (!stringChromeBrowserEntry.getValue().isBusy()) &#123;                    chromeBrowser = stringChromeBrowserEntry.getValue();                    break;                &#125;            &#125;        &#125;        if (chromeBrowser == null) &#123;            throw new RuntimeException(&quot;Have not any free chrome right now&quot;);        &#125;        return chromeBrowser;    &#125;    private ChromeBrowser createNewBrowser(String loginUser) &#123;        Preconditions.checkArgument(TOTAL_BROWSER_OPENED.get() &lt; MAX_BROWSER_NUMBER, &quot;Can&#39;t create more chrome window&quot;);        ChromeBrowser chromeBrowser = new ChromeBrowser(new ChromeDriver(chromeOptions), &quot;Chrome-&quot; + (mapChromeBrowser.size() + 1));        mapChromeBrowser.put(loginUser + &quot;_&quot; + TOTAL_BROWSER_OPENED.get(), chromeBrowser);        TOTAL_BROWSER_OPENED.incrementAndGet();        return chromeBrowser;    &#125;    public static void configChromeDriver() &#123;        if (!isInitConfig) &#123;            WebDriverManager.chromedriver().driverVersion(&quot;94.0.4606.41&quot;).setup();            chromeOptions.setHeadless(isHeadLessMode);            chromeOptions.addArguments(&quot;--disable-dev-shm-usage&quot;, &quot;--disable-gpu&quot;);            isInitConfig = true;        &#125;    &#125;&#125;</code></pre><ul><li><code>chromeOptions.addArguments(&quot;--disable-dev-shm-usage&quot;, &quot;--disable-gpu&quot;)</code>: It will help us when running the app in docker with non-gui mode.</li><li><code>ChromeBrowser.class</code> is a class has <code>org.openqa.selenium.WebDriver</code> and other special info</li></ul><pre><code class="java">    public ChromeBrowser(WebDriver webDriver, String name) &#123;        this.createdTime = MyHelper.getNow();        this.webDriver = webDriver;        this.name = name;    &#125;</code></pre><h2 id="Frontend"><a href="#Frontend" class="headerlink" title="Frontend"></a>Frontend</h2><h3 id="Chartjs"><a href="#Chartjs" class="headerlink" title="Chartjs"></a>Chartjs</h3><ul><li><a href="https://www.chartjs.org/">https://www.chartjs.org</a></li><li>Expected looks like:</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/crawl_travian/expected_chart.png" alt="Chart expected"></p><ul><li><code>Chart.tsx</code></li></ul><pre><code class="js">import React from &#39;react&#39;;import &#123; Bubble &#125; from &#39;react-chartjs-2&#39;;import ChartDataLabels from &#39;chartjs-plugin-datalabels&#39;;export const Chart = prop1 =&gt; &#123;  const option: any = &#123;    pointStyle: &#39;circle&#39;,    title: &#123;      display: true,    &#125;,    plugins: &#123;      datalabels: &#123;        color: &#39;blue&#39;,        display: true,        formatter: function (value, context) &#123;          if (value.changepoc === 0) &#123;            return &#39;&#39;;          &#125;          return value.changepoc;        &#125;,      &#125;,    &#125;,    scales: &#123;      x: &#123;        type: &#39;category&#39;,        labels: [          &#39;.&#39;,          &#39;00AM&#39;,          &#39;01AM&#39;,          &#39;02AM&#39;,          &#39;03AM&#39;,          &#39;04AM&#39;,          &#39;05AM&#39;,          &#39;06AM&#39;,          &#39;07AM&#39;,          &#39;08AM&#39;,          &#39;09AM&#39;,          &#39;10AM&#39;,          &#39;11AM&#39;,          &#39;12PM&#39;,          &#39;13PM&#39;,          &#39;14PM&#39;,          &#39;15PM&#39;,          &#39;16PM&#39;,          &#39;17PM&#39;,          &#39;18PM&#39;,          &#39;19PM&#39;,          &#39;20PM&#39;,          &#39;21PM&#39;,          &#39;22PM&#39;,          &#39;23PM&#39;,          &#39;.&#39;,        ],      &#125;,      y: &#123;        type: &#39;category&#39;,        labels: prop1.yLabels,      &#125;,    &#125;,  &#125;;  const plugins: any = [ChartDataLabels];  return (    &lt;div&gt;      &lt;Bubble data=&#123;prop1.chartData&#125; options=&#123;option&#125; plugins=&#123;plugins&#125; height=&#123;150&#125; /&gt;    &lt;/div&gt;  );&#125;;</code></pre><ul><li><code>ChartDemo.tsx</code></li></ul><pre><code class="js">import React, &#123; useEffect &#125; from &#39;react&#39;;import &#123; Chart &#125; from &#39;./Chart&#39;;import &#123; useAppDispatch, useAppSelector &#125; from &#39;app/config/store&#39;;import &#123; getEntity &#125; from &#39;../user-chart-data/user-chart-data.reducer&#39;;import &#123; IChartPoint &#125; from &#39;app/shared/model/user-chart-data.model&#39;;export default function ChartDemo(props1) &#123;  const dispatch = useAppDispatch();  useEffect(() =&gt; &#123;    dispatch(getEntity(props1.userId));  &#125;, []);  const userChartDataEntity = useAppSelector(state =&gt; state.userChartData.entity);  const allChartPoints: IChartPoint[] = userChartDataEntity.chartPoints;  const bubbleIncrease = [];  const bubbleDecrease = [];  const bubbleConstant = [];  const bubbleUnknown = [];  const pocRate = userChartDataEntity.indexRate;  const getR = pocValue =&gt; &#123;    if (pocValue === null) &#123;      return 15;    &#125;    const r = pocRate === 0 ? pocValue : pocValue / pocRate;    return Math.max(r, 15);  &#125;;  allChartPoints.map((value, index) =&gt; &#123;    if (value.activity === &#39;INCREASE&#39;) &#123;      bubbleDecrease.push(&#123;        x: value.hour,        y: value.date,        r: getR(value.poc),        poc: value.poc,        changepoc: value.changepoc,      &#125;);    &#125; else if (value.activity === &#39;DECREASE&#39;) &#123;      bubbleIncrease.push(&#123;        x: value.hour,        y: value.date,        r: getR(value.poc),        poc: value.poc,        changepoc: value.changepoc,      &#125;);    &#125; else if (value.activity === &#39;CONSTANT&#39;) &#123;      bubbleConstant.push(&#123;        x: value.hour,        y: value.date,        r: getR(value.poc),        poc: value.poc,        changepoc: value.changepoc,      &#125;);    &#125; else &#123;      bubbleUnknown.push(&#123;        x: value.hour,        y: value.date,        r: getR(value.poc),        poc: value.poc,        changepoc: value.changepoc,      &#125;);    &#125;  &#125;);  const data1 = &#123;    datasets: [      &#123;        title: &#39;dataTitle1&#39;,        label: &#39;Increase&#39;,        data: bubbleDecrease,        backgroundColor: &#39;rgb(124,252,0)&#39;,        radius: 10000,      &#125;,      &#123;        label: &#39;Decrease&#39;,        title: &#39;dataTitle2&#39;,        data: bubbleIncrease,        backgroundColor: &#39;rgb(215,86,112)&#39;,      &#125;,      &#123;        label: &#39;Constant&#39;,        title: &#39;dataTitle3&#39;,        data: bubbleConstant,        backgroundColor: &#39;rgb(84,110,196)&#39;,      &#125;,      &#123;        label: &#39;Unknown&#39;,        title: &#39;dataTitle4&#39;,        data: bubbleUnknown,        backgroundColor: &#39;rgb(129,111,116)&#39;,      &#125;,    ],  &#125;;  return (    &lt;div className=&quot;App&quot;&gt;      &lt;Chart chartData=&#123;data1&#125; pocRate=&#123;userChartDataEntity.indexRate&#125; yLabels=&#123;userChartDataEntity.ylabels&#125; /&gt;    &lt;/div&gt;  );&#125;</code></pre><h3 id="Add-missing-Awesome-Icon"><a href="#Add-missing-Awesome-Icon" class="headerlink" title="Add missing Awesome Icon"></a>Add missing Awesome Icon</h3><pre><code class="js">import &#123; faSync &#125; from &#39;@fortawesome/free-solid-svg-icons&#39;;library.add(faSync);&lt;FontAwesomeIcon icon=&quot;sync&quot; spin=&#123;loading&#125; /&gt;&#123;&#39; &#39;&#125;</code></pre><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><h3 id="Package-the-source-code-Shell-script"><a href="#Package-the-source-code-Shell-script" class="headerlink" title="Package the source code - Shell script"></a>Package the source code - Shell script</h3><pre><code class="bash">echo &quot;BIG STEP 1: Build jar file&quot;mvn -Pprod package -DskipTestsecho &quot;BIG STEP 2: Build Dockerfile&quot;docker build -t crawl_travian .echo &quot;BIG STEP 3: Upload to AWS ECR&quot;docker tag crawl_travian:latest 168146697673.dkr.ecr.ap-southeast-1.amazonaws.com/crawl_travian:$1docker push 168146697673.dkr.ecr.ap-southeast-1.amazonaws.com/crawl_travian:$1</code></pre><h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><pre><code class="Dockerfile">FROM maven:3.8.2-jdk-11RUN apt-get updateRUN apt-get install -y curlRUN apt-get install -y wget \    zip \    unzipARG CHROME_VERSION=94.0.4606.81-1ARG CHROME_DRIVER_VERSION=94.0.4606.41#Step 2: Install ChromeRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \    &amp;&amp; echo &quot;deb http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt;&gt; /etc/apt/sources.list.d/google-chrome.list \    &amp;&amp; apt-get update \    &amp;&amp; apt-get update -qqy \    &amp;&amp; apt-get -qqy install google-chrome-stable=$CHROME_VERSION \    &amp;&amp; rm /etc/apt/sources.list.d/google-chrome.list \    &amp;&amp; rm -rf /var/lib/apt/lists/* /var/cache/apt/* \    &amp;&amp; sed -i &#39;s/&quot;$HERE\/chrome&quot;/&quot;$HERE\/chrome&quot; --no-sandbox/g&#39; /opt/google/chrome/google-chrome#Step 3: Install chromedriver for SeleniumRUN wget -q -O /tmp/chromedriver.zip https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip \    &amp;&amp; unzip /tmp/chromedriver.zip -d /opt \    &amp;&amp; rm /tmp/chromedriver.zip \    &amp;&amp; mv /opt/chromedriver /opt/chromedriver-$CHROME_DRIVER_VERSION \    &amp;&amp; chmod 755 /opt/chromedriver-$CHROME_DRIVER_VERSION \    &amp;&amp; ln -s /opt/chromedriver-$CHROME_DRIVER_VERSION /usr/bin/chromedriverCOPY target/crawl-travian-1.jar crawl-travian.jarRUN chmod 777 crawl-travian.jarENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/crawl-travian.jar&quot;]</code></pre><h3 id="Docker-compose"><a href="#Docker-compose" class="headerlink" title="Docker-compose"></a>Docker-compose</h3><pre><code class="yml">version: &#39;3.8&#39;services:  crawl-app:    image: 168146697673.dkr.ecr.ap-southeast-1.amazonaws.com/crawl_travian:latest    environment:      - _JAVA_OPTIONS=-Xmx1800m -Xms256m      - SPRING_PROFILES_ACTIVE=prod      - MANAGEMENT_METRICS_EXPORT_PROMETHEUS_ENABLED=true      - SPRING_DATASOURCE_URL=jdbc:mysql://crawl-mysql:3306/crawl_travian?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;createDatabaseIfNotExist=true&amp;allowPublicKeyRetrieval=true      - SPRING_LIQUIBASE_URL=jdbc:mysql://crawl-mysql:3306/crawl_travian?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;createDatabaseIfNotExist=true&amp;allowPublicKeyRetrieval=true    ports:      - 8080:8080  crawl-mysql:    image: mysql:8.0.26    volumes:      - /home/tungtv/workplace/volume/crawl_travian_local/:/var/lib/mysql/    environment:      - MYSQL_ROOT_PASSWORD=tung2021@      - MYSQL_DATABASE=crawl_travian    ports:      - 3306:3306    command: mysqld --lower_case_table_names=1 --skip-ssl --character_set_server=utf8mb4 --explicit_defaults_for_timestamp</code></pre><h2 id="DEMO"><a href="#DEMO" class="headerlink" title="DEMO"></a>DEMO</h2><ul><li><a href="https://github.com/tungtv202/crawl-travian">SourceCode</a><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/crawl_travian/home_page.png" alt="HomePage"><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/crawl_travian/user_follow_detail.png" alt="Chart page"><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/crawl_travian/crawl_log.png" alt="Crawl Log"></li></ul>]]></content>
      
      
      <categories>
          
          <category> stories </category>
          
      </categories>
      
      
        <tags>
            
            <tag> selenium </tag>
            
            <tag> crawl </tag>
            
            <tag> travian </tag>
            
            <tag> superviosr </tag>
            
            <tag> chromedriver </tag>
            
            <tag> chartjs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Framework Note</title>
      <link href="/2021/09/Java/Spring/"/>
      <url>/2021/09/Java/Spring/</url>
      
        <content type="html"><![CDATA[<h2 id="Spring-Framework-Note"><a href="#Spring-Framework-Note" class="headerlink" title="Spring Framework Note"></a>Spring Framework Note</h2><ul><li>Singleton pattern</li></ul><h2 id="Should-I-use-Service-Component-for-Repository-Dao-class"><a href="#Should-I-use-Service-Component-for-Repository-Dao-class" class="headerlink" title="Should I use @Service @Component for Repository&#x2F;Dao class?"></a>Should I use @Service @Component for Repository&#x2F;Dao class?</h2><ul><li>If the Repository&#x2F;Dao class using JDBC, it won’t make any difference</li><li>If the Repository&#x2F;Dao class using JPA&#x2F;hibernate, it will have a different class exception when throw. If using @Repository, Spring will wrap exception by own exception.</li></ul><h2 id="Spring-Boot-Actuator"><a href="#Spring-Boot-Actuator" class="headerlink" title="Spring Boot Actuator"></a>Spring Boot Actuator</h2><ul><li>Provides build-in production ready endpoints that can be used for monitoring and controlling your application such as &#x2F;info, heal&#x2F;…</li></ul><h2 id="Tiny-note"><a href="#Tiny-note" class="headerlink" title="Tiny note"></a>Tiny note</h2><ul><li><p><code>@SpringBootApplication</code> &#x3D; ( <code>@Configuration</code> + <code>@EnableAutoConfiguration</code> + <code>@ComponentScan</code> )</p></li><li><p>Disable particular auto-configuration in spring boot</p><ul><li>@EnableAutoConfiguration(exclude&#x3D;{DataSourceAutoConfiguration.class})</li><li>@SpringBootApplication(exclude&#x3D;DataSourceAutConfiguration.class)</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Build Dependency Injection</title>
      <link href="/2021/09/Java/BuildDependencyInjection/"/>
      <url>/2021/09/Java/BuildDependencyInjection/</url>
      
        <content type="html"><![CDATA[<h1 id="Build-Dependency-Injection"><a href="#Build-Dependency-Injection" class="headerlink" title="Build Dependency Injection"></a>Build Dependency Injection</h1><p>Using Java Reflection</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>Scanning Classes</li><li>Mapping Classes</li><li>Create Service Instances</li><li>Dependency Container</li></ul><h2 id="1-Scanning-Classes"><a href="#1-Scanning-Classes" class="headerlink" title="1. Scanning Classes"></a>1. Scanning Classes</h2><ul><li>Need separate two case: when run .jar file (compiled), and when run source code on IDE (classes)</li><li>Import:<ul><li>java.util.jar.JarFile</li><li>java.io.File</li></ul></li></ul><h2 id="2-Mapping-Classes"><a href="#2-Mapping-Classes" class="headerlink" title="2. Mapping Classes"></a>2. Mapping Classes</h2><ul><li>Using Java Annotation to mark “<code>Service/Bean</code>“</li></ul><pre><code class="java">@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Bean &#123;&#125;</code></pre><pre><code class="java">@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface Service &#123;&#125;</code></pre><pre><code class="java">@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.CONSTRUCTOR, ElementType.FIELD&#125;)public @interface Autowired &#123;&#125;</code></pre><ul><li>Create <code>ScanningConfiguration</code> to define <code>declared annotation</code>, that need process</li><li>Filter <code>declared annotation</code> from list classes (the result of step (1))</li></ul><h2 id="3-Create-Service-Instances"><a href="#3-Create-Service-Instances" class="headerlink" title="3. Create Service Instances"></a>3. Create Service Instances</h2><h3 id="ServicesInstantiation"><a href="#ServicesInstantiation" class="headerlink" title="ServicesInstantiation"></a>ServicesInstantiation</h3><ul><li>Initially, the list “dependency” does not need to be ordered</li></ul><pre class="mermaid">flowchart LRLD(QUEUE Classes - that need create instances)LD --loop--> PF[/Pool first class & removed from queue/]PF -->C(Class)C --> ADR{all dependency resolved?}ADR --yes--> CI[/Create instance/]ADR --No--> AC[/Add class to last queue/]AC --> LD</pre><ul><li>With this algorithm, the class that has zero dependencies will be created instance first. The class that has max dependency will<br>be created last</li></ul><h3 id="ObjectInstantiationService"><a href="#ObjectInstantiationService" class="headerlink" title="ObjectInstantiationService"></a>ObjectInstantiationService</h3><ul><li>List method<ul><li>createInstance:<ul><li>newInstance</li><li>setAutowiredFieldInstances</li><li>invokePostConstruct</li></ul></li><li>createBeanInstance</li><li>destroyInstance: before set instance is null, need check instance has <code>preDestroy</code> method? (check method<br>present <code>@PreDestoy</code> annotation), if yes, need invoke this method.</li></ul></li></ul><h2 id="4-Dependency-Container"><a href="#4-Dependency-Container" class="headerlink" title="4. Dependency Container"></a>4. Dependency Container</h2><ul><li><p>List method</p><ul><li>init</li><li>getService : for <code>runStartUpMethod</code></li><li>getServicesByAnnotation</li><li>getAllServices</li><li>getNewInstance(Class&lt;?&gt; serviceType, String instanceName)<ul><li><code>instanceName</code> like as <code>Qualifier</code> in Spring Bean</li><li>need separate case <code>Bean</code> vs <code>Service</code></li></ul></li><li>getServiceDetails<ul><li>if <code>Scope=PROTOTYPE</code></li></ul></li></ul></li><li><p>run StartUp Method  (method has annotation <code>@StartUp</code>)</p><ul><li>get serviceDetail from dependency container</li><li>find method present <code>@StartUp</code> annotation and returnType is <code>void</code></li><li>using <code>java.lang.reflect.Method#invoke</code> to invoke it</li></ul></li></ul><h2 id="Source-code"><a href="#Source-code" class="headerlink" title="Source code"></a>Source code</h2><ul><li><a href="https://github.com/tungtv202/DependencyInjectionExample.git">Source code</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> dependency </tag>
            
            <tag> dependency injection </tag>
            
            <tag> reflection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java chat app - use Cassandra, MySQL, Redis, RabbitMQ</title>
      <link href="/2021/09/ReviewBooks/ProJavaClusteringAndScalability/"/>
      <url>/2021/09/ReviewBooks/ProJavaClusteringAndScalability/</url>
      
        <content type="html"><![CDATA[<h1 id="Java-chat-app-use-Cassandra-MySQL-Redis-RabbitMQ"><a href="#Java-chat-app-use-Cassandra-MySQL-Redis-RabbitMQ" class="headerlink" title="Java chat app - use Cassandra, MySQL, Redis, RabbitMQ"></a>Java chat app - use Cassandra, MySQL, Redis, RabbitMQ</h1><ul><li><p>Book: <a href="https://www.apress.com/gp/book/9781484229842?utm_campaign=3_pier05_product_page&utm_content=11232017&utm_medium=referral&utm_source=safari&wt_mc=ThirdParty.Safari.3.EPR653.ProductPagePurchase#otherversion=9781484229859">Pro Java Clustering and Scalability - Building Real-Time Apps with Spring, Cassandra, Redis, WebSocket and RabbitMQ</a></p></li><li><p><a href="https://github.com/tungtv202/pro-java-clustering-scalability">Source code</a> (fork)</p></li></ul><h2 id="Single-node"><a href="#Single-node" class="headerlink" title="Single node"></a>Single node</h2><pre class="mermaid">graph TDU1(User 01 Browser) -- web socket --> CU2(User 02 Browser) -- web socket --> C C(Chat App)C --> R(Redis)C --> Ca(Cassandra)C --> M(MySQL)</pre><ul><li>MySQL: store user&#x2F; user role</li><li>Redis: store chatroom info (Redis Hash)</li><li>Cassandra: store chat message conversation</li></ul><h3 id="Flow"><a href="#Flow" class="headerlink" title="Flow"></a>Flow</h3><ul><li>after the WebSocket connected</li><li>client asks for the connected users and their old messages</li><li>client subscribes to start receiving<ul><li>updates when a user joins or leaves the chat room</li><li>when a public message is sent</li><li>when a user receives a private message</li></ul></li></ul><h2 id="Multinode"><a href="#Multinode" class="headerlink" title="Multinode"></a>Multinode</h2><ol><li>Problem</li></ol><ul><li>ClientA connect to server 1</li><li>ClientB connect to server 2</li><li>How clientA can connect to clientB?</li></ul><ol start="2"><li>Solution</li></ol><ul><li>Using RabbitMQ as a full external STOMP Broker</li><li>Using the Sticky Session Strategy<ul><li>implementation group: ‘org.springframework.session’, name: ‘spring-session’</li><li>spring.session.store-type: redis</li></ul></li></ul><pre class="mermaid">graph TDN(Nginx)N --> C1(Chat app 1)N --> C2(Chat app 2)N --> C3(Chat app 3)C1 --> R(RabbitMQ STOMP Cluster)C2 --> RC3 --> R</pre><pre class="mermaid">graph TDRe(Redis Cluster)Ca(Cassandra Cluster)My(MySQL with Replication)</pre><h2 id="Code-by-features"><a href="#Code-by-features" class="headerlink" title="Code by features"></a>Code by features</h2><h3 id="Private-Messages"><a href="#Private-Messages" class="headerlink" title="Private Messages"></a>Private Messages</h3><ul><li>Spring will auto transformed to destination</li><li>Example<ul><li>Private dest &#x3D; <code>/queue/AG1XX5.private.messages</code></li><li>When send message to user: user123, The destination will be transformed<br>into <code>/queue/AG1XX5.private.messages-user123</code><br><code>org.springframework.messaging.simp.SimpMessagingTemplate#convertAndSendToUser</code></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> summary_book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cassandra </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> redis </tag>
            
            <tag> mysql </tag>
            
            <tag> chat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Project Loom</title>
      <link href="/2021/08/Java/ProjectLoom/"/>
      <url>/2021/08/Java/ProjectLoom/</url>
      
        <content type="html"><![CDATA[<h1 id="Project-Loom"><a href="#Project-Loom" class="headerlink" title="Project Loom"></a>Project Loom</h1><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><ul><li>Can’t have a million Thread with traditional way. (Normally, number of threads in 2_000-10_000). If lager, we will get<br>the OutOfMemory, and JVM stack is expensive</li><li>Reactive is hard to study, reading, and debug</li></ul><h2 id="Virtual-Threads"><a href="#Virtual-Threads" class="headerlink" title="Virtual Threads"></a>Virtual Threads</h2><ul><li>Make the blocking is very cheap. Over 23 million virtual threads in 16 GB of memory</li><li>Way1</li></ul><pre><code>Thread thread = Thread.startVirtualThread(runnable);</code></pre><ul><li>Way2</li></ul><pre><code class="java">Thread thread = Thread.builder()   .virtual()   .name(taskname)   .task(runnable)   .build();</code></pre><ul><li>Example: <a href="https://horstmann.com/unblog/2020-12-05/dailyImages/ImageProcessor.java">https://horstmann.com/unblog/2020-12-05/dailyImages/ImageProcessor.java</a></li></ul><h3 id="VirtualThread-getState"><a href="#VirtualThread-getState" class="headerlink" title="VirtualThread.getState()"></a>VirtualThread.getState()</h3><table><thead><tr><th>VirtualThread State</th><th>Thread State</th></tr></thead><tbody><tr><td>NEW</td><td>NEW</td></tr><tr><td>STARTED, RUNNABLE</td><td>RUNNABLE</td></tr><tr><td>RUNNING</td><td>if mounted, carrier thread state else RUNNABLE</td></tr><tr><td>PARKING, YIELDING</td><td>RUNNABLE</td></tr><tr><td>PINNED, PARKED, PARKED_SUSPENDED</td><td>WAITING</td></tr><tr><td>TERMINATED</td><td>TERMINATED</td></tr></tbody></table><h2 id="Structured-concurrency"><a href="#Structured-concurrency" class="headerlink" title="Structured concurrency"></a>Structured concurrency</h2><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul><li>Don’t use <code>pool</code> in virtualThread. If using ExecutorService, Loom will create new thread for each task. In Loom, you<br>are encouraged to use a separate executor service for each task set.</li><li>We can create virtual thread pool with deadline</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> concurrency </tag>
            
            <tag> project loom </tag>
            
            <tag> fiber </tag>
            
            <tag> virtual thread </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bloom Filter</title>
      <link href="/2021/08/Other/BloomFilter/"/>
      <url>/2021/08/Other/BloomFilter/</url>
      
        <content type="html"><![CDATA[<h1 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h1><h2 id="False-positive"><a href="#False-positive" class="headerlink" title="False positive"></a>False positive</h2><ul><li><p>If it returns <code>false</code> &#x3D;&gt; 100% believe</p></li><li><p>If it returns <code>true</code> &#x3D;&gt; Maybe it <code>really true</code> or <code>false</code> -&gt; Then need using other way to check exactly the result.</p></li><li><p>Get benefit when:</p><ul><li>The entries are very big.</li><li>Want to decrease the request to “exactly checking”</li><li>The percent of probabilistic <code>false</code> is very bigger than <code>true</code></li></ul></li><li><p>Use case in the world</p><ul><li>Google using it to checking account_name is it exists or not when the user registers a new account.</li></ul></li></ul><h2 id="Understand-via-code-sample"><a href="#Understand-via-code-sample" class="headerlink" title="Understand via code sample"></a>Understand via code sample</h2><pre><code class="java">import java.util.BitSet;public interface BloomFilter &#123;  void add(Object object);  boolean mightContain(Object object);&#125;class MyBloomFilter implements BloomFilter &#123;  private static final int DEFAULT_SIZE = 1000;  // Default bit array size  private static final int[] HASH_SEEDS = &#123;7, 11, 13, 31, 37, 61&#125;;  // Seed values for multiple hash functions  private final BitSet bitSet;  // Bit array to store hashed values  private final int size;  // Size of the bit array  MyBloomFilter() &#123;    this(DEFAULT_SIZE);  // Initialize with default size if no size is provided  &#125;  MyBloomFilter(int size) &#123;    this.size = size;  // Set the bit array size    this.bitSet = new BitSet(size);  // Initialize the bit array  &#125;  @Override  public void add(Object object) &#123;    for (int seed : HASH_SEEDS) &#123;  // Apply each hash function      int hashPosition = hashObject(object, seed);  // Get the bit position from the hash      bitSet.set(hashPosition);  // Set the bit at the calculated position    &#125;  &#125;  @Override  public boolean mightContain(Object object) &#123;    for (int seed : HASH_SEEDS) &#123;  // Check each hash function      int hashPosition = hashObject(object, seed);  // Get the bit position from the hash      if (!bitSet.get(hashPosition)) &#123;  // If any bit is not set, the object is definitely not in the set        return false;      &#125;    &#125;    return true;  // If all bits are set, the object might be in the set  &#125;  private int hashObject(Object object, int seed) &#123;    int result = object.hashCode() ^ seed;  // Combine the object&#39;s hash code with the seed    result = (result &amp; 0x7fffffff) % size;  // Ensure the hash is within bounds of the bit array    return result;  &#125;  public static void main(String[] args) &#123;    BloomFilter bloomFilter = new MyBloomFilter();    bloomFilter.add(&quot;Hello&quot;);    bloomFilter.add(&quot;World&quot;);    System.out.println(bloomFilter.mightContain(&quot;Hello&quot;)); // true    System.out.println(bloomFilter.mightContain(&quot;Java&quot;)); // false  &#125;&#125;</code></pre><p>Summary:</p><ul><li><code>DEFAULT_SIZE</code>: Controls the size of the bit array, affecting memory usage and false positive rate.</li><li><code>HASH_SEEDS</code>: Provides multiple seeds for hashing, simulating multiple hash functions.</li><li><code>bitSet</code>: Efficiently stores the set bits after hashing.</li><li><code>size</code>: Defines the length of the bit array, ensuring hash values are within bounds.</li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bloom filter </tag>
            
            <tag> algorithm </tag>
            
            <tag> false positive </tag>
            
            <tag> bf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Design Pattern</title>
      <link href="/2021/08/Java/Java_Pattern/"/>
      <url>/2021/08/Java/Java_Pattern/</url>
      
        <content type="html"><![CDATA[<h2 id="Proxy-vs-Decorator-pattern"><a href="#Proxy-vs-Decorator-pattern" class="headerlink" title="Proxy vs Decorator pattern"></a>Proxy vs Decorator pattern</h2><h3 id="Similarities"><a href="#Similarities" class="headerlink" title="Similarities"></a>Similarities</h3><ul><li>Structure - both patterns forms a wrapper over real object</li><li>Syntax - in both patterns, a wrappers’ entity class is created that implements the same interface as that of the real<br>entity class</li></ul><h3 id="Differences"><a href="#Differences" class="headerlink" title="Differences"></a>Differences</h3><ol><li>Intention</li></ol><ul><li>Decorator Pattern - wraps entity and adds new functionality to it</li><li>Proxy Pattern - wrap entity and restricts direct access to that entity, for security or performance or remote access</li></ul><ol start="2"><li>Usability</li></ol><ul><li>Decorator pattern allows to consume both decorator and original entity whereas Proxy pattern allows consuming only<br>proxy class and must completely restrict the direct access to original entity.</li></ul><ol start="3"><li>Instantiation Technique</li></ol><ul><li>Decorator Pattern - can instantiate the original object or can accept the instance to decorate from consumer (via<br>constructor)</li><li>Proxy Pattern - can not accept original instance from consumer since original object is abstraction for a consumer via<br>Proxy. Hence, proxy internally instantiates the original object.</li></ul><ol start="4"><li>The delegate’s lifecycle</li></ol><ul><li>Some <code>keyword context</code>:<ul><li><code>Aggregation</code>: when the child class <code>CAN</code> exist independently of the parent class. Example: (Car vs Wheel, When<br>there is no car object, the wheels can still exist (maybe for truck))</li><li><code>Composition</code>: when the child class <code>CANNOT</code> exist independently of the parent class. Example: A Library class has a<br>set of Accounts. When remove A Library, the Accounts cannot stand on their own.</li></ul></li><li>Decorator: The delegate is not owned by the decorator, and thus it is an <code>aggregate</code>.</li><li>Proxy: The delegate does not exist without the proxy, it is a <code>composite</code> of the proxy.</li></ul><h2 id="Adapter-Pattern"><a href="#Adapter-Pattern" class="headerlink" title="Adapter Pattern"></a>Adapter Pattern</h2><ul><li>Decrease the dependency between abstraction vs implementation</li><li>Decrease the child class</li></ul><h3 id="Code-Example"><a href="#Code-Example" class="headerlink" title="Code Example"></a>Code Example</h3><pre><code class="java">interface WebDriver &#123;    void getElement();    void selectElement();&#125;class ChromeDriver implements WebDriver &#123;    @Override    public void getElement() &#123;        System.out.println(&quot;Get element from ChromeDriver&quot;);    &#125;    @Override    public void selectElement() &#123;        System.out.println(&quot;Select element from ChromeDriver&quot;);    &#125;&#125;class IEDriver &#123;    public void findElement() &#123;        System.out.println(&quot;Find element from IEDriver&quot;);    &#125;    public void clickElement() &#123;        System.out.println(&quot;Click element from IEDriver&quot;);    &#125;&#125;class IEWebDriverAdapter implements WebDriver &#123;    final IEDriver ieDriver;    IEWebDriverAdapter(IEDriver ieDriver) &#123;        this.ieDriver = ieDriver;    &#125;    @Override    public void getElement() &#123;        ieDriver.findElement();    &#125;    @Override    public void selectElement() &#123;        ieDriver.clickElement();    &#125;&#125;public class AdapterPatternExample &#123;    public static void main(String[] args) &#123;        IEDriver ieDriver = new IEDriver();        WebDriver webDriverViaAdapter = new IEWebDriverAdapter(ieDriver);        WebDriver webDriverViaChromeDriver = new ChromeDriver();    &#125;&#125;</code></pre><h2 id="Bridge-Pattern"><a href="#Bridge-Pattern" class="headerlink" title="Bridge Pattern"></a>Bridge Pattern</h2><h3 id="Code-Example-1"><a href="#Code-Example-1" class="headerlink" title="Code Example"></a>Code Example</h3><p><img src="https://refactoring.guru/images/patterns/diagrams/bridge/example-en.png" alt="BridgePattern"></p><ul><li>Device</li></ul><pre><code class="java">abstract class EntertainmentDevice &#123;    public int deviceState;    public int maxSetting;    public int volumeLevel = 0;    public abstract void buttonFivePressed();    public abstract void buttonSixPressed();    public void deviceFeedback() &#123;        if (deviceState &gt; maxSetting || deviceState &lt; 0) &#123;            deviceState = 0;        &#125;        System.out.println(&quot;On &quot; + deviceState);    &#125;    public void buttonSevenPressed() &#123;        volumeLevel++;        System.out.println(&quot;Volume at: &quot; + volumeLevel);    &#125;    public void buttonEightPressed() &#123;        volumeLevel--;        System.out.println(&quot;Volume at: &quot; + volumeLevel);    &#125;&#125;class TVDevice extends EntertainmentDevice &#123;    public TVDevice(int newDeviceState, int newMaxSetting) &#123;        deviceState = newDeviceState;        maxSetting = newMaxSetting;    &#125;    @Override    public void buttonFivePressed() &#123;        System.out.println(&quot;Channel Down&quot;);        deviceState--;    &#125;    @Override    public void buttonSixPressed() &#123;        System.out.println(&quot;Channel Up&quot;);        deviceState++;    &#125;&#125;class DVDDevice extends EntertainmentDevice &#123;    @Override    public void buttonFivePressed() &#123;        //todo    &#125;    @Override    public void buttonSixPressed() &#123;        //todo    &#125;&#125;</code></pre><ul><li>Button</li></ul><pre><code class="java">abstract class RemoteButton &#123;    private final EntertainmentDevice theDevice;    public RemoteButton(EntertainmentDevice device) &#123;        theDevice = device;    &#125;    public void buttonFivePressed() &#123;        theDevice.buttonFivePressed();    &#125;    public void buttonSixPressed() &#123;        theDevice.buttonSixPressed();    &#125;    public void deviceFeedback() &#123;        theDevice.deviceFeedback();    &#125;    public abstract void buttonNinePressed();&#125;class TVRemoteMute extends RemoteButton &#123;    public TVRemoteMute(EntertainmentDevice device) &#123;        super(device);    &#125;    @Override    public void buttonNinePressed() &#123;        System.out.println(&quot;TV was Muted&quot;);    &#125;&#125;class TVRemotePause extends RemoteButton &#123;    public TVRemotePause(EntertainmentDevice device) &#123;        super(device);    &#125;    @Override    public void buttonNinePressed() &#123;        System.out.println(&quot;TV was Paused&quot;);    &#125;&#125;</code></pre><ul><li>Test</li></ul><pre><code class="java">public class Test &#123;    public static void main(String[] args) &#123;        RemoteButton theTV = new TVRemoteMute(new TVDevice(1, 200));        RemoteButton theTV2 = new TVRemotePause(new TVDevice(1, 200));//        RemoteButton theDVD = new DEVRemote(new DVDDevice(1,14));        System.out.println(&quot;Test TV with Mute&quot;);        theTV.buttonFivePressed();        theTV.buttonSixPressed();        theTV.buttonNinePressed();        System.out.println(&quot;\nTest TV with Pause&quot;);        theTV2.buttonFivePressed();        theTV2.buttonSixPressed();        theTV2.buttonSixPressed();        theTV2.buttonSixPressed();        theTV2.buttonSixPressed();        theTV2.buttonNinePressed();        theTV2.deviceFeedback();    &#125;&#125;</code></pre><p>Output</p><pre><code>Test TV with MuteChannel DownChannel UpTV was MutedTest TV with PauseChannel DownChannel UpChannel UpChannel UpChannel UpTV was PausedOn 4</code></pre><h2 id="Template-method"><a href="#Template-method" class="headerlink" title="Template method"></a>Template method</h2><h3 id="Code-example"><a href="#Code-example" class="headerlink" title="Code example"></a>Code example</h3><pre><code class="java">abstract public class AbstractQuery &#123;    static int id = 0;    public Result getResult() &#123;        // todo some thing        return new Result(id++, generateName());    &#125;    abstract protected String generateName();&#125;@AllArgsConstructorclass Result &#123;    long id;    String name;&#125;class EmployeeQuery extends AbstractQuery &#123;    @Override    protected String generateName() &#123;        return &quot;employee 1&quot;;    &#125;&#125;class ManagerQuery extends AbstractQuery &#123;    @Override    protected String generateName() &#123;        return &quot;manager 2&quot;;    &#125;&#125;</code></pre><h2 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy"></a>Strategy</h2><h2 id="Command"><a href="#Command" class="headerlink" title="Command"></a>Command</h2><p><img src="https://gpcoder.com/wp-content/uploads/2018/12/design-patterns-command-diagram.png" alt="Command pattern"></p><p>Code example: Undo Redo Document</p><pre><code class="java">import java.util.Stack;// Receiver class - is an object that performs a set of cohesive actions.class Document &#123;    private final Stack&lt;String&gt; lines = new Stack&lt;&gt;();    public void write(String text) &#123;        lines.push(text);    &#125;    public void eraseLast() &#123;        if (!lines.isEmpty()) &#123;            lines.pop();        &#125;    &#125;    public void readDocument() &#123;        System.out.println(&quot;---Start reading document&quot;);        lines.forEach(System.out::println);        System.out.println(&quot;---Finish reading document&quot;);    &#125;&#125;// Command class - o store all the information required for executing an actioninterface Command &#123;    void undo();    void redo();&#125;class DocumentEditorCommand implements Command &#123;    private final Document document;    private final String text;    public DocumentEditorCommand(Document document, String text) &#123;        this.document = document;        this.text = text;        this.document.write(text);    &#125;    @Override    public void undo() &#123;        document.eraseLast();    &#125;    @Override    public void redo() &#123;        document.write(text);    &#125;&#125;// Invoker class - knows how to execute a given command but doesn&#39;t know how the command has been implementedclass DocumentInvoker &#123;    private final Stack&lt;Command&gt; undoCommands = new Stack&lt;&gt;();    private final Stack&lt;Command&gt; redoCommands = new Stack&lt;&gt;();    private final Document document = new Document();    public void undo() &#123;        if (!undoCommands.isEmpty()) &#123;            Command cmd = undoCommands.pop();            cmd.undo();            redoCommands.push(cmd);        &#125; else &#123;            System.out.println(&quot;Nothing to undo&quot;);        &#125;    &#125;    public void redo() &#123;        if (!redoCommands.isEmpty()) &#123;            Command cmd = redoCommands.pop();            cmd.redo();            undoCommands.push(cmd);        &#125; else &#123;            System.out.println(&quot;Nothing to redo&quot;);        &#125;    &#125;    public void write(String text) &#123;        Command cmd = new DocumentEditorCommand(document, text);        undoCommands.push(cmd);        redoCommands.clear();    &#125;    public void read() &#123;        document.readDocument();    &#125;&#125;// Client class -  controls the command execution processpublic class UndoRedoExample &#123;    public static void main(String[] args) &#123;        DocumentInvoker instance = new DocumentInvoker();        instance.write(&quot;The 1st text. &quot;);        instance.undo();        instance.read(); // EMPTY        instance.redo();        instance.read(); // The 1st text.        instance.write(&quot;The 2nd text. &quot;);        instance.write(&quot;The 3rd text. &quot;);        instance.read(); // The 1st text. The 2nd text. The 3rd text.        instance.undo(); // The 1st text. The 2nd text.        instance.undo(); // The 1st text.        instance.undo(); // EMPTY        instance.undo(); // Nothing to undo    &#125;&#125;</code></pre><p>Output </p><pre><code>---Start reading document---Finish reading document---Start reading documentThe 1st text. ---Finish reading document---Start reading documentThe 1st text. The 2nd text. The 3rd text. ---Finish reading documentNothing to undo</code></pre><h2 id="Observer"><a href="#Observer" class="headerlink" title="Observer"></a>Observer</h2><p><img src="https://dz2cdn1.dzone.com/storage/temp/14009476-observerdesignpattern.png" alt="Observer Pattern"></p><p>Code example: (keyword: observer weather data)</p><pre><code class="java">import java.util.ArrayList;import java.util.List;import java.util.Random;import java.util.concurrent.TimeUnit;import lombok.SneakyThrows;public interface Subject &#123;    void register(Observer o);    void remove(Observer o);    void notifyObservers();&#125;interface Observer &#123;    void update(int temp, int humidity);&#125;class WeatherStation implements Subject &#123;    private final List&lt;Observer&gt; observers;    private int temp;    private int humidity;    public WeatherStation() &#123;        this.observers = new ArrayList&lt;&gt;();    &#125;    @Override    public void register(Observer o) &#123;        observers.add(o);    &#125;    @Override    public void remove(Observer o) &#123;        int observerIndex = observers.indexOf(o);        if (observerIndex &gt;= 0) &#123;            observers.remove(o);        &#125;    &#125;    @Override    public void notifyObservers() &#123;        observers.forEach(o -&gt; o.update(temp, humidity));    &#125;    public void measurementsChanged(int temp, int humidity) &#123;        this.temp = temp;        this.humidity = humidity;        notifyObservers();    &#125;&#125;class CurrentConditionsDisplay implements Observer &#123;    private int temp;    private int humidity;    public CurrentConditionsDisplay(Subject weatherStation) &#123;        weatherStation.register(this);    &#125;    @Override    public void update(int temp, int humidity) &#123;        this.temp = temp;        this.humidity = humidity;        displayCurrent();    &#125;    private void displayCurrent() &#123;        System.out.println(&quot;Current temperature: &quot; + temp);        System.out.println(&quot;Current humidity: &quot; + humidity);    &#125;&#125;class ForecastDisplay implements Observer &#123;    private final List&lt;Integer&gt; tempHistory;    private final List&lt;Integer&gt; humidityHistory;    public ForecastDisplay(Subject weatherStation) &#123;        tempHistory = new ArrayList&lt;&gt;();        humidityHistory = new ArrayList&lt;&gt;();        weatherStation.register(this);    &#125;    @Override    public void update(int temp, int humidity) &#123;        this.tempHistory.add(temp);        this.humidityHistory.add(humidity);        display7DayHistory();    &#125;    private void display7DayHistory() &#123;        System.out.println(&quot;Temperature History: &quot; + tempHistory.subList(Math.max(tempHistory.size() - 7, 0), tempHistory.size()));        System.out.println(&quot;Humidity History: &quot; + humidityHistory.subList(Math.max(humidityHistory.size() - 7, 0), humidityHistory.size()));    &#125;&#125;class ObserverDemoMain &#123;    @SneakyThrows    public static void main(String[] args) &#123;        WeatherStation weatherStation = new WeatherStation();        CurrentConditionsDisplay currentConditionsDisplay = new CurrentConditionsDisplay(weatherStation);        ForecastDisplay forecastDisplay = new ForecastDisplay(weatherStation);        // simulate update        for (int i = 0; i &lt; 5; i++) &#123;            System.out.println(&quot;\n --- Update &quot; + i + &quot; ---&quot;);            int randomTemp = getRandomInt(-50, 40);            int randomHumidity = getRandomInt(0, 100);            weatherStation.measurementsChanged(randomTemp, randomHumidity);            TimeUnit.SECONDS.sleep(1);        &#125;    &#125;    private static int getRandomInt(int min, int max) &#123;        Random rand = new Random();        return rand.nextInt(max + 1 - min) + min;    &#125;&#125;</code></pre><p>Output</p><pre><code> --- Update 0 ---Current temperature: 28Current humidity: 60Temperature History: [28]Humidity History: [60] --- Update 1 ---Current temperature: -33Current humidity: 82Temperature History: [28, -33]Humidity History: [60, 82] --- Update 2 ---Current temperature: -39Current humidity: 9Temperature History: [28, -33, -39]Humidity History: [60, 82, 9] --- Update 3 ---Current temperature: 22Current humidity: 82Temperature History: [28, -33, -39, 22]Humidity History: [60, 82, 9, 82] --- Update 4 ---Current temperature: 19Current humidity: 46Temperature History: [28, -33, -39, 22, 19]Humidity History: [60, 82, 9, 82, 46]Process finished with exit code 0</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> command </tag>
            
            <tag> java </tag>
            
            <tag> pattern </tag>
            
            <tag> design pattern </tag>
            
            <tag> proxy </tag>
            
            <tag> decorator </tag>
            
            <tag> bridge </tag>
            
            <tag> adapter </tag>
            
            <tag> observer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Partition Database</title>
      <link href="/2021/07/Database/Partition_Database/"/>
      <url>/2021/07/Database/Partition_Database/</url>
      
        <content type="html"><![CDATA[<h1 id="Partition-Database"><a href="#Partition-Database" class="headerlink" title="Partition Database"></a>Partition Database</h1><h2 id="Table-inheritance-vs-partitioning"><a href="#Table-inheritance-vs-partitioning" class="headerlink" title="Table inheritance vs partitioning"></a>Table inheritance vs partitioning</h2><ul><li>Partitioning &lt; inheritance</li></ul><h2 id="Horizontal-Vertical-partitioning"><a href="#Horizontal-Vertical-partitioning" class="headerlink" title="Horizontal &amp; Vertical partitioning"></a>Horizontal &amp; Vertical partitioning</h2><p>Example:</p><ul><li>Table: Student</li><li>Column: ID, Name, Code, ClassCode, Email, PhoneNumber</li></ul><p>So:</p><ul><li>Horizon: partition1(Name: A-&gt;N), partition2(Name: M-&gt;Z)</li><li>Vertical: partition1(ID, Name, ClassCode), partition2(Code, Email, PhoneNumber)</li></ul><h2 id="Horizontal-partitioning"><a href="#Horizontal-partitioning" class="headerlink" title="Horizontal partitioning"></a>Horizontal partitioning</h2><ul><li>By Range: via date, numeric, alphabet</li><li>By List: ENUM value</li><li>By Hash (best loadbalancer)</li></ul><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><ul><li>Can not declare PrimaryKey or Unique on Partition table (But CAN with child table). Reason is ensure to isolate<br>among child partition table.</li><li>Can not delecare two child table has conflict range.(partition key conflict)</li><li>What happened if CRUD on column not in RANGE?<ul><li>Error if not <code>TABLE DEFAULT PARTITION</code></li></ul></li></ul><h2 id="Partition-pruning"><a href="#Partition-pruning" class="headerlink" title="Partition pruning"></a>Partition pruning</h2><ul><li>Can understand it is a “coordinator”, that will detect exactly child partition via <code>key column partition</code></li><li>If <code>pruning</code> has been disabled, so the partition don’t have sense. Because it will scan all child partition</li></ul><h2 id="Multi-level-partition"><a href="#Multi-level-partition" class="headerlink" title="Multi-level partition"></a>Multi-level partition</h2><h2 id="Sharding"><a href="#Sharding" class="headerlink" title="Sharding"></a>Sharding</h2><ul><li>The specific variant of Horizon Partition, but each partition will host on different (separate) database nodes.</li></ul>]]></content>
      
      
      <categories>
          
          <category> database </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql </tag>
            
            <tag> partition </tag>
            
            <tag> sharding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Equal method</title>
      <link href="/2021/06/Java/EqualMethod/"/>
      <url>/2021/06/Java/EqualMethod/</url>
      
        <content type="html"><![CDATA[<h2 id="S02"><a href="#S02" class="headerlink" title="S02"></a>S02</h2><pre><code class="java">    public class Point &#123;        private int x;        private int y;        public Point(int x, int y) &#123;            this.x = x;            this.y = y;        &#125;        public boolean equals(Point obj) &#123;            if (!(obj instanceof Point)) &#123;                return false;            &#125;            return x == obj.x &amp;&amp; y == obj.y;        &#125;        public int hashCode() &#123;            int result = x;            result = 31 * result + y;            return result;        &#125;    &#125;    @Test    public void whats_the_problem() &#123;        Point point = new Point(1, 1);        Point clone = new Point(1, 1);        List&lt;Point&gt; pts  = List.of(point);        Assert.assertTrue(point.equals(clone));  // TRUE        Assert.assertTrue(pts.contains(clone));     // FALSE    &#125;</code></pre><ul><li>Solution: <code>public boolean equals(Point obj)</code> -&gt; <code>public boolean equals(Object obj)</code></li></ul><h2 id="S03"><a href="#S03" class="headerlink" title="S03"></a>S03</h2><pre><code class="java">    public boolean equals(Object obj) &#123;            if (!(obj instanceof Point)) &#123;                return false;            &#125;            Point other = (Point) obj;            return x == other.x &amp;&amp; y == other.y;        &#125;</code></pre><pre><code class="java">    @Test    public void whats_the_problem() &#123;        Point point = new Point(1, 1);        Set&lt;Point&gt; pts = new HashSet&lt;&gt;();        pts.add(point);        point.x = 3;        Assert.assertTrue(pts.contains(point));  // FALSE    &#125;</code></pre><ul><li>How to pass <code>EqualVerifier</code>?</li></ul><pre><code class="java"> EqualsVerifier.forClass(Point.class)            .suppress(Warning.NONFINAL_FIELDS)            .verify();</code></pre><h2 id="S04"><a href="#S04" class="headerlink" title="S04"></a>S04</h2><ul><li><code>equals</code> and <code>hashCode</code> method has be generated by IDE, without<br>tick <code>Accept subclasses as parameter to equal() method</code></li></ul><pre><code class="java">    public class Point &#123;        private final int x;        private final int y;        public Point(int x, int y) &#123;            this.x = x;            this.y = y;        &#125;        @Override        public boolean equals(Object o) &#123;            if (this == o) return true;            if (o == null || getClass() != o.getClass()) return false;            Point point = (Point) o;            return x == point.x &amp;&amp; y == point.y;        &#125;        @Override        public int hashCode() &#123;            return Objects.hash(x, y);        &#125;    &#125;    @Test    public void whats_the_problem() &#123;        Point point = new Point(1, 1);        Point sub = new Point(1, 1) &#123;   &#125;;        Assert.assertEquals(point, sub);  // FALSE    &#125;</code></pre><ul><li>How to pass <code>EqualVerifier</code>?</li></ul><pre><code class="java"> EqualsVerifier.forClass(Point.class)            .usingGetClass()            .verify();</code></pre><h3 id="S05"><a href="#S05" class="headerlink" title="S05"></a>S05</h3><ul><li><code>equals</code> and <code>hashCode</code> method has be generated by IDE, with tick <code>Accept subclasses as parameter to equal() method</code></li></ul><pre><code class="java">    public class Point &#123;        private final int x;        private final int y;        public Point(int x, int y) &#123;            this.x = x;            this.y = y;        &#125;        @Override        public boolean equals(Object o) &#123;            if (this == o) return true;            if (!(o instanceof Point)) return false;            Point point = (Point) o;            return x == point.x &amp;&amp; y == point.y;        &#125;        @Override        public int hashCode() &#123;            return Objects.hash(x, y);        &#125;    &#125;        @Test    public void solves_liskov_substitution_principle() &#123;        Point point = new Point(1, 1);        Point sub = new Point(1, 1) &#123;   &#125;;        Assert.assertEquals(point, sub);  // TRUE    &#125;</code></pre><pre><code class="java">    public class ColorPoint extends Point &#123;        private final Color color;        public ColorPoint(int x, int y, Color color) &#123;            super(x, y);            this.color = color;        &#125;        @Override        public boolean equals(Object o) &#123;            if (this == o) return true;            if (!(o instanceof ColorPoint)) return false;            if (!super.equals(o)) return false;            ColorPoint that = (ColorPoint) o;            return color.equals(that.color);        &#125;        @Override        public int hashCode() &#123;            return Objects.hash(super.hashCode(), color);        &#125;    &#125;    @Test    public void whats_the_problem()&#123;        Point p = new Point(1,1);        Point q = new ColorPoint(1,1, Color.BLACK);        Assert.assertEquals(p,q); // TRUE        Assert.assertEquals(q,p); // FALSE    &#125;    </code></pre><ul><li>Solution? -&gt; S06</li></ul><h3 id="S06"><a href="#S06" class="headerlink" title="S06"></a>S06</h3><ul><li>Modified <code>equal</code> method of ColorPoint</li></ul><pre><code class="java">    public class ColorPoint extends Point &#123;        private final Color color;        public ColorPoint(int x, int y, Color color) &#123;            super(x, y);            this.color = color;        &#125;        @Override        public boolean equals(Object o) &#123;            if (this == o) return true;            if (o instanceof ColorPoint) &#123;                ColorPoint that = (ColorPoint) o;                return super.equals(that) &amp;&amp; color == that.color;            &#125;            if (o instanceof Point) &#123;                return o.equals(this);            &#125;            return false;        &#125;        @Override        public int hashCode() &#123;            return Objects.hash(super.hashCode(), color);        &#125;    &#125;    @Test    public void whats_the_problem() &#123;        Point p = new Point(1, 1);        Point q = new ColorPoint(1, 1, Color.BLACK);        Assert.assertEquals(p, q); // TRUE        Assert.assertEquals(q, p); // TRUE    &#125;</code></pre><h3 id="S08"><a href="#S08" class="headerlink" title="S08"></a>S08</h3><ul><li><code>final</code> before <code>equal</code> &amp; <code>hashCode</code> method</li></ul><pre><code class="java">    public class Point &#123;        private final int x;        private final int y;        public Point(int x, int y) &#123;            this.x = x;            this.y = y;        &#125;        @Override        public final boolean equals(Object o) &#123;            if (this == o) return true;            if (!(o instanceof Point)) return false;            Point point = (Point) o;            return x == point.x &amp;&amp; y == point.y;        &#125;        @Override        public final int hashCode() &#123;            return Objects.hash(x, y);        &#125;    &#125;    @Test    public void whats_the_problem() &#123;        EqualsVerifier.forClass(Point.class).verify(); // TRUE    &#125;</code></pre><p>Ref: <a href="https://youtu.be/pNJ_O10XaoM">Youtube</a></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> equal </tag>
            
            <tag> hashcode </tag>
            
            <tag> EqualsVerifier </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Functional Pattern</title>
      <link href="/2021/04/Other/Functional_Pattern/"/>
      <url>/2021/04/Other/Functional_Pattern/</url>
      
        <content type="html"><![CDATA[<h1 id="Functional-Pattern"><a href="#Functional-Pattern" class="headerlink" title="Functional Pattern"></a>Functional Pattern</h1><p>Groups:</p><ul><li>Behavioral patterns: strategy, visitor, chain of responsibility, the template method, observer, iterator… </li><li>Creation patterns:  factory, builder, prototype, the factory method… </li><li>Structural patterns: adapter, bridge, proxy, decorator…</li></ul><h2 id="The-Factory-Method-Pattern"><a href="#The-Factory-Method-Pattern" class="headerlink" title="The Factory Method Pattern"></a>The Factory Method Pattern</h2><p>-&gt;  to create an instance of the object</p><ol><li>We have:</li></ol><pre><code class="java">    enum VehicleColor &#123;            RED,            BLUE,            BLACK,            WHITE        &#125;    enum VehicleType &#123;            CAR,            BUS,            TRUCK;    &#125;    interface Vehicle &#123;    &#125;    @Getter    @RequiredArgsConstructor    static class Car implements Vehicle &#123;        private final VehicleColor color;    &#125;    @Getter    @RequiredArgsConstructor    static class Bus implements Vehicle &#123;        private final VehicleColor color;    &#125;    @Getter    @RequiredArgsConstructor    static class Truck implements Vehicle &#123;        private final VehicleColor color;    &#125;</code></pre><ol start="2"><li>Legacy</li></ol><pre><code class="java">    public static Vehicle instanceOfType(VehicleType type, VehicleColor color) &#123;        if (type.equals(VehicleType.CAR)) &#123;            return new Car(color);        &#125; else if (type.equals(VehicleType.BUS)) &#123;            return new Bus(color);        &#125; else if (type.equals(VehicleType.TRUCK)) &#123;            return new Truck(color);        &#125;        throw new IllegalArgumentException(&quot;No support for type &quot; + type);    &#125;</code></pre><p>-&gt;</p><pre><code class="java">Vehicle vehicle = instanceOfType(VehicleType.BUS, VehicleColor.BLUE);</code></pre><ol start="3"><li>Functional</li></ol><pre><code class="java">    enum VehicleType &#123;        CAR(Car::new),        BUS(Bus::new),        TRUCK(Truck::new);        public final Function&lt;VehicleColor, Vehicle&gt; factory;        VehicleType(Function&lt;VehicleColor, Vehicle&gt; factory) &#123;            this.factory = factory;        &#125;    &#125;</code></pre><p>-&gt;</p><pre><code class="java"> Vehicle vehicle = VehicleType.BUS.factory.apply(VehicleColor.RED);</code></pre><p>&#x2F;&#x2F; The purpose of creating <code>Function</code> in enum to avoid forgetting to declare a new case. That maybe<br>get <code>IllegalArgumentException</code> if you use <code>legacy</code> way.</p><h2 id="The-Template-Method-Pattern"><a href="#The-Template-Method-Pattern" class="headerlink" title="The Template Method Pattern"></a>The Template Method Pattern</h2><p>-&gt;  allows us to define some common steps for an algorithm. Then, the subclasses override some of these steps with their specific behaviors for a particular step</p><ol><li>We have:</li></ol><pre><code class="java">    interface Vehicle &#123;    &#125;    static class Bus implements Vehicle &#123;    &#125;</code></pre><ol start="2"><li>Legacy</li></ol><pre><code class="java">    abstract class AbstractVehicleLegacy implements Vehicle &#123;        public void start() &#123;            preStartCheck();            System.out.printf(&quot;Start AbstractVehicleLegacy %s \n&quot;, this.getClass().getSimpleName());        &#125;        abstract void preStartCheck();    &#125;    public class BusImpl extends AbstractVehicleLegacy &#123;        @Override        void preStartCheck() &#123;            System.out.printf(&quot;Start %s \n&quot;, this.getClass().getSimpleName());        &#125;    &#125;</code></pre><p>-&gt;</p><pre><code class="java">        var busImpl = new BusImpl();        busImpl.start();</code></pre><ol start="3"><li>Functional</li></ol><pre><code class="java">    interface Vehicle &#123;        default void start(Consumer&lt;Void&gt; preStartCheck) &#123;            preStartCheck.accept(null);            System.out.printf(&quot;Start %s \n&quot;, this.getClass().getSimpleName());        &#125;    &#125;</code></pre><p>-&gt;</p><pre><code class="java">        Bus bus = new Bus();        bus.start(t -&gt; System.out.printf(&quot;Start Bus modern \n&quot;));</code></pre><h2 id="The-Builder-Pattern"><a href="#The-Builder-Pattern" class="headerlink" title="The Builder Pattern"></a>The Builder Pattern</h2><p>-&gt; to provide a way of constructing an object in steps, separating the construction logic from its representation. &#x2F;&#x2F;<br>Can use <code>Lombok</code> to create <code>Builder</code></p><h2 id="The-Strategy-Pattern"><a href="#The-Strategy-Pattern" class="headerlink" title="The Strategy Pattern"></a>The Strategy Pattern</h2><p>-&gt; is probably one of the most widely used design patterns; it’s normally used in every situation where we have to<br>choose a different behavior based on some property or input</p><ol><li>We have</li></ol><pre><code class="java">    interface DeliveryCalculator &#123;        BigDecimal priceFor(Item item);    &#125;    @RequiredArgsConstructor    @Getter    static class Item &#123;        private final Integer id;        private final BigDecimal price;    &#125;</code></pre><ol start="2"><li>Legacy</li></ol><pre><code class="java">    class BasicDeliveryCalculator implements DeliveryCalculator &#123;        @Override        public BigDecimal priceFor(Item item) &#123;            return new BigDecimal(1);        &#125;    &#125;    class PremiumDeliveryCalculator implements DeliveryCalculator &#123;        @Override        public BigDecimal priceFor(Item item) &#123;            return new BigDecimal(0.9);        &#125;    &#125;</code></pre><p>-&gt;</p><pre><code class="java">        DeliveryCalculator factory = new PremiumDeliveryCalculator();        var price = factory.priceFor(new Item(1, new BigDecimal(&quot;9.9&quot;)));</code></pre><ol start="3"><li>Functional</li></ol><pre><code class="java">    enum PLAN_MODERN &#123;        BASIC(deliveryPriceWithPercentageSurplus(&quot;0.025&quot;)),        PREMIUM(deliveryPriceWithPercentageSurplus(&quot;0.015&quot;)),        BUSINESS(deliveryPriceWithPercentageSurplus(&quot;0.0&quot;));        public final Function&lt;Item, BigDecimal&gt; deliveryPrice;        PLAN_MODERN(Function&lt;Item, BigDecimal&gt; deliveryPrice) &#123;            this.deliveryPrice = deliveryPrice;        &#125;        private static Function&lt;Item, BigDecimal&gt; deliveryPriceWithPercentageSurplus(String percentageSurplus) &#123;            return item -&gt; item.getPrice().multiply(new BigDecimal(percentageSurplus)).add(new BigDecimal(&quot;1.0&quot;));        &#125;    &#125;</code></pre><p>-&gt;</p><pre><code class="java">        BigDecimal price = PLAN_MODERN.BASIC.deliveryPrice.apply( new Item(1, new BigDecimal(&quot;12.99&quot;)));</code></pre><p>&#x2F;&#x2F; Is this very similar to <code>factory method pattern</code>?</p><h2 id="The-Chain-of-Responsibility-Pattern"><a href="#The-Chain-of-Responsibility-Pattern" class="headerlink" title="The Chain-of-Responsibility Pattern"></a>The Chain-of-Responsibility Pattern</h2><ol><li>We have</li></ol><pre><code class="java">    enum WashState &#123;        INITIAL,        INITIAL_WASH,        SOAP,        POLISHED,        DRIED    &#125;    static class Car &#123;        private WashState washState;        public Car() &#123;            this.washState = WashState.INITIAL;            System.out.println(&quot;Car state transitioned to &quot; + washState);        &#125;        public Car updateState(WashState state) &#123;            System.out.println(&quot;Car state transitioned to &quot; + state);            this.washState = state;            return this;        &#125;        public WashState washState() &#123;            return washState;        &#125;    &#125;    static abstract class CarWashStep &#123;        protected CarWashStep nextStep;        public CarWashStep andThen(CarWashStep nextStep) &#123;            this.nextStep = nextStep;            return nextStep;        &#125;        abstract Car applyTo(Car car);    &#125;</code></pre><ol start="2"><li>Legacy</li></ol><pre><code class="java">    @Test    public void legacyTest() &#123;        final Car car = new Car();        final CarWashStep initialStep = new CarWashStep() &#123;            @Override            Car applyTo(Car car) &#123;                car.updateState(WashState.INITIAL_WASH);                if (nextStep != null) &#123;                    return nextStep.applyTo(car);                &#125;                return car;            &#125;        &#125;;        final CarWashStep dryStep = new CarWashStep() &#123;            @Override            Car applyTo(Car car) &#123;                car.updateState(WashState.DRIED);                if (nextStep != null) &#123;                    return nextStep.applyTo(car);                &#125;                return car;            &#125;        &#125;;        final CarWashStep polishStep = new CarWashStep() &#123;            @Override            Car applyTo(Car car) &#123;                car.updateState(WashState.POLISHED);                if (nextStep != null) &#123;                    return nextStep.applyTo(car);                &#125;                return car;            &#125;        &#125;;        initialStep.andThen(polishStep)            .andThen(dryStep);        final Car finalCar = initialStep.applyTo(car);        System.out.println(&quot;Final car state is &quot; + finalCar.washState());    &#125;</code></pre><ol start="3"><li>Functional</li></ol><pre><code class="java">    @Test    public void modernTest() &#123;        final Car car = new Car();        Function&lt;Car, Car&gt; initial = c -&gt; new Car();        final Function&lt;Car, Car&gt; chain = initial            .andThen(c -&gt; c.updateState(WashState.INITIAL_WASH))            .andThen(c -&gt; c.updateState(WashState.POLISHED))            .andThen(c -&gt; c.updateState(WashState.DRIED));        chain.apply(car);    &#125;</code></pre><h2 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h2><h3 id="Practice-01"><a href="#Practice-01" class="headerlink" title="Practice 01"></a>Practice 01</h3><pre><code class="java">public static class ClearEmailContent &#123;        interface Builder &#123;            @FunctionalInterface            interface RequireHtml &#123;                RequirePreview html(String html);            &#125;            @FunctionalInterface            interface RequirePreview &#123;                RequireAttachments preview(Preview preview);            &#125;            @FunctionalInterface            interface RequireAttachments &#123;                ClearEmailContent attachments(List&lt;ParsedAttachment&gt; attachments);            &#125;        &#125;        public static Builder.RequireHtml builder() &#123;            return html -&gt; preview -&gt; attachments                -&gt; new ClearEmailContent(html, preview, !attachments.isEmpty(), attachments);        &#125;        private Preview preview;        private boolean hasAttachment;        private String html;        private List&lt;ParsedAttachment&gt; attachments;        ClearEmailContent(String html, Preview preview,                          boolean hasAttachment,                          List&lt;ParsedAttachment&gt; attachments) &#123;            Preconditions.checkNotNull(preview);            Preconditions.checkNotNull(html);            Preconditions.checkNotNull(attachments);            this.preview = preview;            this.hasAttachment = hasAttachment;            this.html = html;            this.attachments = attachments;        &#125;    &#125;</code></pre><ul><li>usage</li></ul><pre><code class="java"> public ClearEmailContent extract(Message message) throws IOException &#123;        return ClearEmailContent.builder()            .html(messageContentExtractor.extract(message)                .getHtmlBody()                .orElse(&quot;&quot;))            .preview(previewFactory.fromMime4JMessage(message))            .attachments(messageParser.retrieveAttachments(message));    &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> pattern </tag>
            
            <tag> functional </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cassandra Note</title>
      <link href="/2021/04/Database/Cassandra_Note/"/>
      <url>/2021/04/Database/Cassandra_Note/</url>
      
        <content type="html"><![CDATA[<h2 id="CQL"><a href="#CQL" class="headerlink" title="CQL"></a>CQL</h2><h3 id="1-Keyspaces"><a href="#1-Keyspaces" class="headerlink" title="1. Keyspaces"></a>1. Keyspaces</h3><ul><li>Create</li></ul><pre><code class="SQL">CREATE KEYSPACE keyspace101 WITH REPLICATION  = &#123; &#39;class&#39; : &#39;NetworkTopologyStrategy&#39;, &#39;DC1&#39;: 3&#125; AND DURABLE_WRITES = false;</code></pre><h3 id="2-Tables"><a href="#2-Tables" class="headerlink" title="2. Tables"></a>2. Tables</h3><ul><li>Set the TTL for an entire row</li></ul><pre><code class="sql">INSERT INTO table101 (id, token)VALUES (&#39;id1&#39;, &#39;token2&#39;) USING TTL 10800;</code></pre><ul><li>Set a table-wide, default TTL</li></ul><pre><code class="sql">CREATE TABLE reset_tokens (    id varchar PRIMARY KEY,    token varchar) WITH default_time_to_live = 10800;</code></pre><h2 id="Table-properties"><a href="#Table-properties" class="headerlink" title="Table properties"></a>Table properties</h2><pre><code>- comment- caching (keys, row_per_partition)- read_repair_chance- dclocal_read_repair_chance- default_time_to_live  (ttls to delete data)- gc_grace_seconds  (circle time to gc delete forever data, that make tombstones)- bloom_filter_fp_chance- compaction- compression- min/max_index_interval- memtable_flush_period_in_ms- populate_io_cache_on_flush- speculative_retry- ...</code></pre><h2 id="Data-Type"><a href="#Data-Type" class="headerlink" title="Data Type"></a>Data Type</h2><h3 id="1-Basic-Data-Types"><a href="#1-Basic-Data-Types" class="headerlink" title="1. Basic Data Types"></a>1. Basic Data Types</h3><ul><li>Numeric: bigint, decimal, double, float, int, variant</li><li>String: ascii, text, varchar</li><li>Date: timestamp, timeuuid</li><li>Other: boolean. uuid, inet, blob</li></ul><h3 id="2-Complex-Data-Types"><a href="#2-Complex-Data-Types" class="headerlink" title="2. Complex Data Types"></a>2. Complex Data Types</h3><ul><li><code>2.1 Set</code><ul><li>Create</li></ul><pre><code class="sql">    CREATE TABLE courses (     id varchar,     name static,     features set&lt;varchar&gt; static,     module_id int,     PRIMARY KEY (id, module_id)     )</code></pre><ul><li>Insert</li></ul><pre><code class="sql">    INSERT INTO courses (id, features) VALUES (&#39;nodejs-big-picture&#39;, &#123;&#39;cc&#39;&#125;); </code></pre><ul><li>Adding to a set</li></ul><pre><code class="sql">UPDATE courses SET features = features + &#123;&#39;cc&#39;&#125; WHERE course_id = ‘nodejs-big-picture’;</code></pre><ul><li>Removing from a set</li></ul><pre><code class="sql">UPDATE courses SET features = features - &#123;&#39;cc&#39;&#125; WHERE course_id = ‘nodejs-big-picture&#39;;</code></pre><ul><li>Empty</li></ul><pre><code class="sql">UPDATE courses SET features = &#123;&#125; WHERE course_id = &#39;nodejs-big-picture&#39;; </code></pre></li><li><code>2.2 List</code><ul><li>Create</li></ul><pre><code class="sql">CREATE TABLE courses ( id varchar, name static, module_id int, clips list&lt;varchar&gt;, PRIMARY KEY (id, module_id) )</code></pre><ul><li>Insert</li></ul><pre><code class="sql">INSERT INTO courses (id, module_id, clips) VALUES (&#39;nodejs-big-picture&#39;,1,[&#39;Course Overview&#39;]); </code></pre><ul><li>Adding to a list</li></ul><pre><code class="sql">UPDATE courses SET clips = [&#39;Course Introduction&#39;] + clips WHERE course_id = &#39;nodejs-big-picture&#39; AND module_id = 2; </code></pre><pre><code class="sql">UPDATE courses SET clips = clips + [&#39;Considering Node.js&#39;] WHERE course_id = &#39;nodejs-big-picture&#39; AND module_id = 2;</code></pre><ul><li>Removing from a list</li></ul><pre><code class="sql">UPDATE courses SET clips = clips - [‘Course Overview&#39;] WHERE course_id = ‘nodejs-big-picture‘ and module_id = 1;</code></pre><ul><li>Manipulating a list by element id</li></ul><pre><code class="sql">UPDATE courses SET clips[2] = ‘What Makes up Node.js?’ WHERE course_id = ‘nodejs-big-picture‘ AND module_id = 2; </code></pre><pre><code class="sql">UPDATE courses SET clips[2] = ‘What Makes up Node.js?’ WHERE course_id = ‘nodejs-big-picture‘ AND module_id = 2; </code></pre></li><li><code>2.3 Map</code><ul><li>Create</li></ul><pre><code class="sql">CREATE TABLE users ( id varchar, first_name varchar, last_name varchar, password varchar, reset_token varchar, last_login map&lt;varchar,timestamp&gt;, PRIMARY KEY (id) )</code></pre><ul><li>Inserting with a map</li></ul><pre><code class="sql">INSERT INTO users (id, first_name, last_name, last_login) VALUES (&#39;john-doe&#39;, &#39;John&#39;, &#39;Doe&#39;, &#123;&#39;383cc0867cd2&#39;: &#39;2015-06-30 09:02:24&#39;&#125;); </code></pre><ul><li>Updating &#x2F; adding to a map</li></ul><pre><code class="sql">UPDATE users SET last_login[&#39;383cc0867cd2&#39;] = &#39;2015-07-01 11:17:42&#39; WHERE user_id = &#39;john-doe&#39;;</code></pre><pre><code class="sql">UPDATE users SET last_login = last_login + &#123;&#39;7eb0a8997f39&#39;: &#39;2015-07-02 07:32:17&#39;&#125; WHERE user_id = &#39;john-doe&#39;;</code></pre><ul><li>Removing from a map</li></ul><pre><code class="sql">DELETE last_login[&#39;383cc0867cd2&#39;] FROM users WHERE id = &#39;john-doe&#39;;</code></pre><pre><code class="sql">UPDATE users SET last_login = last_login - &#123;&#39;7eb0a8997f39&#39;&#125; WHERE id = &#39;john-doe&#39;;</code></pre><ul><li>Emptying the entire map</li></ul><pre><code class="sql">UPDATE users SET last_login = &#123;&#125;WHERE id = &#39;john-doe&#39;;</code></pre><ul><li>Collections and TTL</li></ul><pre><code class="sql">UPDATE users USING TTL 31536000 SET last_login[&#39;383cc0867cd2&#39;] = &#39;2015-07-01 11:17:42&#39; WHERE user_id = &#39;john-doe&#39;;</code></pre></li><li><code>2.4 Tuples</code><ul><li>Create</li></ul><pre><code class="sql">CREATE TABLE users ( id varchar, first_name varchar, last_name varchar, password varchar, reset_token varchar, last_login map&lt;varchar, frozen&lt;tuple&lt;timestamp,inet&gt;&gt;&gt;, PRIMARY KEY (id) )</code></pre></li></ul><h3 id="3-User-Defined-Types"><a href="#3-User-Defined-Types" class="headerlink" title="3. User Defined Types"></a>3. User Defined Types</h3><pre><code class="sql">CREATE TYPE person (name varchar, id varchar); </code></pre><pre><code class="sql">CREATE TABLE courses (  id varchar,  author frozen&lt;person&gt; static,  // ...  clips list&lt;frozen&lt;clip&gt;&gt;,  module_id int,  // ...  PRIMARY KEY (id, module_id) )</code></pre><h3 id="4-Data-with-JSON"><a href="#4-Data-with-JSON" class="headerlink" title="4. Data with JSON"></a>4. Data with JSON</h3><ul><li>Insert<ul><li>normal:</li></ul><pre><code class="sql">INSERT INTO courses (id, module_id, author, clips) VALUES (‘nodejs-big-picture’, 1, &#123; name: ‘Paul O&#39;&#39;Fallon&#39;, id: ‘paul-ofallon&#39; &#125;, [&#123; name: ’Course Overview’, duration: 70 &#125;] );</code></pre><ul><li>Use json</li></ul><pre><code class="sql">INSERT INTO courses JSON &#39;&#123; &quot;id&quot;: &quot;nodejs-big-picture&quot;, &quot;module_id&quot;: 1, &quot;author&quot;: &#123; &quot;name&quot;: &quot;Paul O’Fallon&quot;, &quot;id&quot;: &quot;paul-ofallon&quot; &#125;, &quot;clips&quot;: [&#123; &quot;name&quot;: &quot;Course Overview&quot;, &quot;duration&quot;: 70 &#125;] &#125;&#39; </code></pre></li><li>Selecting</li></ul><pre><code class="sql">SELECT JSON * FROM courses;</code></pre><pre><code class="sql">SELECT DISTINCT id, name, toJson(released) FROM courses;</code></pre><h2 id="Materialized-Views"><a href="#Materialized-Views" class="headerlink" title="Materialized Views"></a>Materialized Views</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/cassandra/MaterializedViews.JPG" alt="MaterializedView"></p><h2 id="Secondary-Indexes"><a href="#Secondary-Indexes" class="headerlink" title="Secondary Indexes"></a>Secondary Indexes</h2><ul><li>If <code>dont secondary index</code>, you cant query on <code>collection</code> column</li></ul><pre><code class="sql">CREATE TABLE users (  id varchar,  first_name varchar,  last_name varchar,  company varchar,  tags set&lt;varchar&gt;,  // ...  PRIMARY KEY (id) ); </code></pre><pre><code class="sql">CREATE INDEX ON users(tags)</code></pre><pre><code class="sql">SELECT * FROM users WHERE tags CONTAINS &#39;java&#39;;</code></pre><h2 id="Batches"><a href="#Batches" class="headerlink" title="Batches"></a>Batches</h2><pre><code>BEGIN BATCH INSERT INTO courses (id, tags) VALUES (‘nodejs-big-picture‘,  &#123;‘developer&#39;, &#39;javascript&#39;, &#39;node.js&#39;, &#39;open-source&#39;&#125;);  INSERT INTO course_tags (tag, course_id) VALUES (‘developer’,&#39;nodejs-big-picture&#39;); // ... etc. APPLY BATCH;</code></pre><h2 id="Lightweight-Transactions"><a href="#Lightweight-Transactions" class="headerlink" title="Lightweight Transactions"></a>Lightweight Transactions</h2><pre><code>1. Prepare ! Promise 2. Read ! Results 3. Propose ! Accept 4. Commit ! Ack</code></pre><h2 id="User-Defined-Functions"><a href="#User-Defined-Functions" class="headerlink" title="User Defined Functions"></a>User Defined Functions</h2><ul><li>Use <code>hourly</code></li></ul><pre><code class="sql">SELECT asTimeStr(duration, false) FROM courses WHERE id = ‘advanced-javascript’</code></pre><pre><code>- Input: 24936 =&gt; Output: 6h 55 m</code></pre><ul><li>Defined<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/cassandra/DefinedFunction.JPG" alt="DefinedFunction"></li></ul><h2 id="Primary-Key-and-Composite-Partition-Keys"><a href="#Primary-Key-and-Composite-Partition-Keys" class="headerlink" title="Primary Key and Composite Partition Keys"></a>Primary Key and Composite Partition Keys</h2><pre><code class="sql">CREATE TABLE keyspace101.table2 (    id varchar PRIMARY KEY,    name varchar,    author varchar)</code></pre><pre><code class="sql">CREATE TABLE keyspace101.table2 (    id varchar,    name varchar,    author varchar,    PRIMARY KEY ((id, author)))</code></pre><h2 id="Tombstones"><a href="#Tombstones" class="headerlink" title="Tombstones"></a>Tombstones</h2><p>When data is deleted in Cassandra, it isn’t just removed immediately from the cluster. A tombstone is written to the<br>database, marking the data in question as deleted. This tombstone is partitioned data just like any other data written<br>to the cluster, and this managed accordingly, with hinted handoffs, read repairs everything.<br>For example, we have a key space with a replication factor of three, and we delete some data at a consistency level of<br>quorum. Now let’s suppose this right doesn’t make it to the third node in a cluster responsible for that token range.<br>Having the tombstone allows the reed repair process to propagate this tombstone to the outdated node</p><ul><li>property: <code>gc_grace_seconds</code> (default is 10 days)</li><li>Note: CAN see a tombstone data by some tools</li></ul><h3 id="Why-tombstone"><a href="#Why-tombstone" class="headerlink" title="Why tombstone?"></a>Why tombstone?</h3><ul><li>it comes from a use case: when we try to delete some row while having one node in cluster DOWN.<br>When that node online comeback, it thinks another node is “missing” some data. It doesn’t think that row has been removed. So &#x3D;&gt; maybe data has been re-updated&#x2F;re-insert again to another node.  &#x3D;&gt; In order to avoid that, Cassandra doesn’t remove permanently data, it just marks “flag” to rows - that has been removed.</li></ul><h3 id="When-do-tombstones-cause-problems"><a href="#When-do-tombstones-cause-problems" class="headerlink" title="When do tombstones cause problems?"></a>When do tombstones cause problems?</h3><ul><li>Disk usage</li><li>Read performance - (When we query with a special partition key but has too many rows with same partition key (contains all row has been marked removed flag))</li></ul><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><p>cql:</p><pre><code class="sql">expand on; // select result is row formatexpand off; // select result is column formattracing on; tracing off;</code></pre><h2 id="Counters"><a href="#Counters" class="headerlink" title="Counters"></a>Counters</h2><p>Counters are a special data type in Cassandra since dealing with them in a distributed environment requires special<br>care. Counters must live in tables by themselves other than the primary keys they also carry with them. Some added<br>overhead as they rely on read before, right.</p><h2 id="Multi-row-Partitions"><a href="#Multi-row-Partitions" class="headerlink" title="Multi-row Partitions"></a>Multi-row Partitions</h2><ol><li>Composite partition keys and Clustering keys<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/cassandra/Key.JPG" alt="Key"></li><li>Static columns</li></ol><pre><code class="sql">CREATE TABLE courses (  id varchar,  name varchar STATIC,  module_id int,  module_name varchar,  PRIMARY KEY (id, module_id));</code></pre><ol start="3"><li>Time Series Data</li></ol><ul><li>TimeUUID Data Type<ul><li>Example: <code>45b94a50-12e5-11e5-9114-091830ac5256</code></li><li>The number of 100 ns intervals since UUID epoch</li><li>MAC address</li><li>Clock sequence number to prevent duplicates</li></ul></li><li>cql ex:</li></ul><pre><code class="sql">CREATE TABLE course_page_views (  course_id text,  view_id timeuuid,  PRIMARY KEY (course_id, view_id) ) WITH CLUSTERING ORDER BY (view_id DESC);</code></pre><ul><li>now</li></ul><pre><code class="sql">CREATE TABLE course_page_views (  course_id text,  view_id timeuuid,  PRIMARY KEY (course_id, view_id) ) WITH CLUSTERING ORDER BY (view_id DESC);</code></pre><ul><li>dateOf &#x2F; unixTimestampOf</li></ul><pre><code class="sql">SELECT course_id, dateOf(view_id) FROM course_page_views WHERE course_id = ‘advanced-python‘;</code></pre><ul><li>minTimeuuid &#x2F; maxTimeuuid</li></ul><pre><code class="sql">SELECT dateOf(view_id) FROM course_page_views WHERE course_id = ‘advanced-python&#39; AND view_id &gt;= maxTimeuuid(&#39;2019-11-01 00:00+0000&#39;) AND view_id &lt; minTimeuuid(‘2019-12-01 00:00+0000&#39;)</code></pre><ol start="4"><li>Bucketing Time series data</li></ol><pre><code class="sql">CREATE TABLE course_page_views (  bucket_id text,  course_id text,  view_id timeuuid,  PRIMARY KEY ((bucket_id, course_id), view_id) ) WITH CLUSTERING ORDER BY (view_id DESC);</code></pre><h2 id="Storing-Data-in-Cassandra"><a href="#Storing-Data-in-Cassandra" class="headerlink" title="Storing Data in Cassandra"></a>Storing Data in Cassandra</h2><p>All data stored in Cassandra is associated with a token</p><h2 id="Snitches"><a href="#Snitches" class="headerlink" title="Snitches"></a>Snitches</h2><p>???</p><h2 id="Replication-Strategies"><a href="#Replication-Strategies" class="headerlink" title="Replication Strategies"></a>Replication Strategies</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/cassandra/Cassandra_terminology.JPG" alt="Cassandra_Terminology"></p><ol><li>SimpleStrategy</li></ol><ul><li>Used in dev env or single data center cluster</li><li>CQL ex</li></ul><pre><code class="sql">CREATE KEYSPACE keyspace101 with replication =&#123;&#39;class&#39;: &#39;SimpleStrategy&#39;, &#39;replication_factor&#39; : 3&#125;</code></pre><p><code>replication_factor : 3</code> &#x3D; asking Cassandra to store three copies of all the partitions in all the tables written to<br>the cluster, in this keyspace</p><ol start="2"><li>NetworkTopologyStrategy</li></ol><ul><li>CQL ex</li></ul><pre><code class="sql">CREATE KEYSPACE keyspace102 with replication =&#123;&#39;class&#39;: &#39;NetworkTopologyStrategy&#39;, &#39;DC1&#39; : 3, &#39;DC2&#39;: 1&#125;</code></pre><p>storing 4 copies of the data for each partition in each table&#x2F;in the keyspace. <code>BUT</code> 3 in DC1, and 1 in DC2</p><h2 id="Tunable-Consistency"><a href="#Tunable-Consistency" class="headerlink" title="Tunable Consistency"></a>Tunable Consistency</h2><ul><li><code>Hinted Handoff</code> is used to handle transient failures</li><li>(Write Consistency + Read Consistency) &gt; Replication Factor</li><li>Command set consistency level<ul><li>multi-dc: <code> consistency local_one;</code></li><li>single-dc: <code>consistency quorum;</code></li></ul></li></ul><ol><li>Read<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/cassandra/TunableConsistency_Reads.JPG" alt="Tunable Consistency Reads"></li></ol><ul><li>Read Repair<ul><li>What happens when 1 node with old data, and 2 node with good data?</li><li>Then, the third node will return a digest that does not match the other two nodes.</li><li>The coordinator node will then read the full data from all three nodes, determine the correct data</li><li>Write correct data back to the node that’s in error</li></ul></li></ul><ol start="2"><li>Write</li></ol><ul><li><p>Coordinator (node receive write statement) will wait to receive an ack of the other node before returning a positive<br>response to the collar<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/cassandra/TunableConsistency_Writes.JPG" alt="Tunable Consistency - Writes"></p></li><li><p>Hinted Handoff</p><ul><li>What happens if one of those writes fails?</li><li>Cassandra uses a strategy called a hinted handoff to help in these situations</li><li>The data is written down on the coordinator node</li><li>Then the coordinator know tries repeatedly to deliver the data</li></ul></li></ul><ol start="3"><li>Consistency with multiple data centers</li></ol><ul><li>EACH_QUORUM</li><li>LOCAL_QUORUM</li><li>LOCAL_ONE</li></ul><h2 id="Difference-between-partition-key-composite-key-and-clustering-key"><a href="#Difference-between-partition-key-composite-key-and-clustering-key" class="headerlink" title="Difference between partition key, composite key and clustering key?"></a>Difference between partition key, composite key and clustering key?</h2><ul><li>Primary Key: Cassandra uses a special type of primary key called a composite key to represent group of related rows, alsoe called “Partitions”</li><li>Primary Key &#x3D; Partition Key + OPTIONAL clustering columns</li><li>Partition key &#x3D; it determines the nodes on which rows are stored and itself contains multiple columns </li><li>Composite column &#x3D; It controls how data is stored inside the partition</li></ul><h2 id="Static-column"><a href="#Static-column" class="headerlink" title="Static column"></a>Static column</h2><p>it is a special column that is shared by all the rows of a partition. the static column is very useful when we want to share a column with a single value.</p><ul><li>A table that does not define any clustering columns cannot have a static column. </li><li>You can batch conditional updates to a static column.</li><li>Use the DISTINCT keyword to select static columns. </li><li>If COMPACT STORAGE is specified when the table is built, static columns are not allowed at this time</li><li>If a column is part of partition key&#x2F;Clustering columns, it cannot be described as a static column</li></ul><h2 id="How-to-quick-warm-up-cassandra-after-start-for-development"><a href="#How-to-quick-warm-up-cassandra-after-start-for-development" class="headerlink" title="How to quick warm up cassandra after start for development?"></a>How to quick warm up cassandra after start for development?</h2><ul><li>Run with argument <code>JVM_OPTS=-Dcassandra.skip_wait_for_gossip_to_settle=0 -Dcassandra.initial_token=1</code><br>Example in docker-compose file</li></ul><pre><code class="yaml">version: &#39;3&#39;services:  cassandra:    image: cassandra:4.1.3    ports:      - &quot;9042:9042&quot;    healthcheck:      test: [&quot;CMD&quot;, &quot;cqlsh&quot;, &quot;-e&quot;, &quot;describe keyspaces&quot;]      interval: 3s      timeout: 20s      retries: 5    environment:      - JVM_OPTS=-Dcassandra.skip_wait_for_gossip_to_settle=0 -Dcassandra.initial_token=1</code></pre><h2 id="TWCS-vs-STCS"><a href="#TWCS-vs-STCS" class="headerlink" title="TWCS vs STCS"></a>TWCS vs STCS</h2><h3 id="TimeWindowCompactionStrategy-TWCS"><a href="#TimeWindowCompactionStrategy-TWCS" class="headerlink" title="TimeWindowCompactionStrategy (TWCS)"></a>TimeWindowCompactionStrategy (TWCS)</h3><p>Use Case: </p><pre><code>TWCS is specifically designed for time-series data, where data is constantly appended and older data becomes less frequently accessed.</code></pre><p>How It Works: </p><pre><code>TWCS divides data into windows based on time intervals (e.g., hours, days, or weeks). Within each window, data is organized using Size-Tiered Compaction (STCS). When a window becomes older than a specified time threshold, it is compacted into a single SSTable (sorted string table). This helps in efficiently managing time-series data with minimal disk I/O.</code></pre><p>Advantages:</p><pre><code>Efficient for time-series data.Reduces read amplification for recent data.Older data is less frequently compacted, reducing compaction overhead.</code></pre><h3 id="SizeTieredCompactionStrategy-STCS"><a href="#SizeTieredCompactionStrategy-STCS" class="headerlink" title="SizeTieredCompactionStrategy (STCS)"></a>SizeTieredCompactionStrategy (STCS)</h3><p>Use Case: </p><pre><code>STCS is a general-purpose compaction strategy that can be used for a wide range of workloads.</code></pre><p>How It Works: </p><pre><code>STCS organizes data into SSTables based on their size. When the number of SSTables in a given level exceeds a threshold, compaction is triggered. SSTables are merged into larger SSTables, and data is compacted based on size.</code></pre><p>Advantages:</p><pre><code>Simplicity and good performance for various workloads.Effective for write-intensive workloads where data is rapidly changing.</code></pre><p>Disadvantages:</p><pre><code>Less efficient for time-series data since it doesn&#39;t consider time-based access patterns.</code></pre><h3 id="Choosing-Between-TWCS-and-STCS"><a href="#Choosing-Between-TWCS-and-STCS" class="headerlink" title="Choosing Between TWCS and STCS"></a>Choosing Between TWCS and STCS</h3><p>The choice between TWCS and STCS depends on your data and access patterns:</p><ul><li><p>If you are dealing with time-series data, such as log data, sensor readings, or financial data, TWCS is typically a better choice. It optimizes storage and access patterns for time-based data.</p></li><li><p>For general-purpose workloads or write-intensive applications, STCS can work well. It’s simple to configure and is suitable for scenarios where data isn’t primarily organized by time.</p></li></ul><p>In some cases, a mixed strategy approach is used, where you might use TWCS for recent data and STCS for historical or less frequently accessed data to strike a balance between efficiency and simplicity.</p><p>Ultimately, the choice of compaction strategy should align with your specific use case and performance requirements. It’s important to benchmark and monitor your Cassandra cluster to ensure that the chosen strategy meets your needs.</p>]]></content>
      
      
      <categories>
          
          <category> database </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cql </tag>
            
            <tag> cassandra </tag>
            
            <tag> database </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Which database should i use</title>
      <link href="/2021/03/Database/Which_database_should_i_used/"/>
      <url>/2021/03/Database/Which_database_should_i_used/</url>
      
        <content type="html"><![CDATA[<h2 id="CAP-Theorem"><a href="#CAP-Theorem" class="headerlink" title="CAP Theorem"></a>CAP Theorem</h2><p><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/other_file/CAP_Considerations.png" alt="CAP Theorem"></p><h2 id="7-Database-Paradigms"><a href="#7-Database-Paradigms" class="headerlink" title="7 Database Paradigms"></a>7 Database Paradigms</h2><h3 id="1-Key-Value-Database"><a href="#1-Key-Value-Database" class="headerlink" title="1. Key-Value Database"></a>1. Key-Value Database</h3><ul><li><strong>Examples:</strong> Redis, Memcached</li><li><strong>Pros:</strong> Fast</li><li><strong>Cons:</strong><ul><li>Limited space</li><li>No queries</li></ul></li><li><strong>Best for:</strong><ul><li>Caching</li><li>Pub&#x2F;Sub</li><li>Leaderboards</li></ul></li><li><strong>Real world example:</strong> <a href="https://github.blog/2009-10-20-how-we-made-github-fast/">How We Made GitHub Fast</a></li></ul><h3 id="2-Wide-Column"><a href="#2-Wide-Column" class="headerlink" title="2. Wide Column"></a>2. Wide Column</h3><ul><li><strong>Examples:</strong> HBase, Cassandra</li><li><strong>Pros:</strong><ul><li>Schema-less</li><li>Easy to scale</li></ul></li><li><strong>Cons:</strong> No joins</li><li><strong>Best for:</strong><ul><li>Time-series data</li><li>Historical records</li><li>High write, low read workloads</li></ul></li><li><strong>Real world example:</strong> <a href="https://netflixtechblog.com/scaling-time-series-data-storage-part-i-ec2b6d44ba39">Scaling Time Series Data Storage at Netflix</a></li></ul><h3 id="3-Document"><a href="#3-Document" class="headerlink" title="3. Document"></a>3. Document</h3><ul><li><strong>Examples:</strong> MongoDB, Firestore</li><li><strong>Pros:</strong><ul><li>Schema-less</li><li>Relational-ish queries</li></ul></li><li><strong>Cons:</strong> No joins</li><li><strong>Best for:</strong><ul><li>Most applications</li><li>Games</li><li>IoT</li></ul></li><li><strong>Not ideal for:</strong><ul><li>Graphs</li></ul></li></ul><h3 id="4-Relational"><a href="#4-Relational" class="headerlink" title="4. Relational"></a>4. Relational</h3><ul><li><strong>Examples:</strong> MySQL, PostgreSQL</li><li><strong>Best for:</strong> Most applications</li><li><strong>Not ideal for:</strong><ul><li>Unstructured data</li></ul></li></ul><h3 id="5-Graph"><a href="#5-Graph" class="headerlink" title="5. Graph"></a>5. Graph</h3><ul><li><strong>Example:</strong> Neo4j</li><li><strong>Best for:</strong><ul><li>Graphs</li><li>Knowledge graphs</li><li>Recommendation engines</li></ul></li><li><strong>Real world example:</strong> Airbnb</li></ul><h3 id="6-Search"><a href="#6-Search" class="headerlink" title="6. Search"></a>6. Search</h3><ul><li><strong>Example:</strong> Elasticsearch</li><li><strong>Best for:</strong><ul><li>Search engines</li><li>Typeahead</li></ul></li></ul><h3 id="7-Multi-Model"><a href="#7-Multi-Model" class="headerlink" title="7. Multi-Model"></a>7. Multi-Model</h3><ul><li><strong>Example:</strong> Fauna</li></ul>]]></content>
      
      
      <categories>
          
          <category> database </category>
          
      </categories>
      
      
        <tags>
            
            <tag> database </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maven Note</title>
      <link href="/2021/02/Java/Maven_Note/"/>
      <url>/2021/02/Java/Maven_Note/</url>
      
        <content type="html"><![CDATA[<h1 id="Maven-Note"><a href="#Maven-Note" class="headerlink" title="Maven Note"></a>Maven Note</h1><h2 id="Maven-build-lib-common-for-another-project-Tutorial"><a href="#Maven-build-lib-common-for-another-project-Tutorial" class="headerlink" title="Maven build lib-common for another project - Tutorial"></a>Maven build lib-common for another project - Tutorial</h2><p>This is a tutorial for build a common library that can import for another project.</p><h3 id="Repo-lib-common-side"><a href="#Repo-lib-common-side" class="headerlink" title="Repo: lib-common side"></a>Repo: lib-common side</h3><h4 id="1-pom-xmlfile"><a href="#1-pom-xmlfile" class="headerlink" title="1. pom.xmlfile"></a>1. <code>pom.xml</code>file</h4><ul><li>Should attention to three parameters</li></ul><pre><code class="xml">&lt;groupId&gt;me.tungexplorer&lt;/groupId&gt;&lt;artifactId&gt;app-common&lt;/artifactId&gt;&lt;version&gt;0.0.5&lt;/version&gt;</code></pre><ul><li><code>version</code>: We can use the format <code>&lt;version&gt;$&#123;buildNumber.version&#125;&lt;/version&gt;</code>, that will help we set version as<br>dynamic when run command <code>mvn clean source:jar deploy --settings $SETTINGS -DbuildNumber.version=$version</code></li><li>NOTICE: <code>version</code> has suffixed is “SNAPSHOT” will push to snapshot maven repo (if define). Otherwise, it will push to<br>release-repo.</li><li>Declare some properties. If not, it may do not happen anything, but when another project import it, it will throw not<br>found exception</li></ul><pre><code class="xml">&lt;properties&gt;    &lt;java.version&gt;11&lt;/java.version&gt;    &lt;maven.javadoc.skip&gt;false&lt;/maven.javadoc.skip&gt;    &lt;maven.compiler.source&gt;11&lt;/maven.compiler.source&gt;    &lt;maven.compiler.target&gt;11&lt;/maven.compiler.target&gt;    &lt;start-class&gt;tung.explorer.appcommon.AppCommonApplication&lt;/start-class&gt;&lt;/properties&gt;</code></pre><ul><li>Declare <code>maven repo</code> (where to store this source code - that has been compiled to <code>jar</code> file)</li></ul><pre><code class="xml">&lt;distributionManagement&gt;    &lt;repository&gt;        &lt;id&gt;packagecloud-tungtv202&lt;/id&gt;        &lt;url&gt;packagecloud+https://packagecloud.io/tungtv202/release&lt;/url&gt;    &lt;/repository&gt;    &lt;snapshotRepository&gt;        &lt;id&gt;packagecloud-tungtv202-snapshot&lt;/id&gt;        &lt;url&gt;packagecloud+https://packagecloud.io/tungtv202/snaphot&lt;/url&gt;    &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt;</code></pre><p>-<code>snapshotRepository</code> is optional</p><ul><li><p><code>id</code> MUST BE define in <code>settings.xml</code> file (.m2 directory)</p></li><li><p><a href="https://packagecloud.io/">https://packagecloud.io</a> : cloud repo service</p></li><li><p><code>extensions</code> require or not with a different repository. Example: <code>packagecloud</code> is required, <code>nexus</code> is not</p></li></ul><pre><code class="xml">&lt;build&gt;    &lt;extensions&gt;        &lt;extension&gt;            &lt;groupId&gt;io.packagecloud.maven.wagon&lt;/groupId&gt;            &lt;artifactId&gt;maven-packagecloud-wagon&lt;/artifactId&gt;            &lt;version&gt;0.0.4&lt;/version&gt;        &lt;/extension&gt;    &lt;/extensions&gt;&lt;/build&gt;</code></pre><ul><li>Example <code>build</code></li></ul><pre><code class="xml">&lt;build&gt;    &lt;extensions&gt;        &lt;extension&gt;            &lt;groupId&gt;io.packagecloud.maven.wagon&lt;/groupId&gt;            &lt;artifactId&gt;maven-packagecloud-wagon&lt;/artifactId&gt;            &lt;version&gt;0.0.4&lt;/version&gt;        &lt;/extension&gt;    &lt;/extensions&gt;    &lt;plugins&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;            &lt;executions&gt;                &lt;execution&gt;                    &lt;id&gt;repackage&lt;/id&gt;                    &lt;goals&gt;                        &lt;goal&gt;repackage&lt;/goal&gt;                    &lt;/goals&gt;                    &lt;configuration&gt;                        &lt;attach&gt;false&lt;/attach&gt;                        &lt;classifier&gt;exec&lt;/classifier&gt;                    &lt;/configuration&gt;                &lt;/execution&gt;            &lt;/executions&gt;        &lt;/plugin&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;            &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;            &lt;configuration&gt;                &lt;archive&gt;                    &lt;manifest&gt;                        &lt;addClasspath&gt;true&lt;/addClasspath&gt;                        &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;                        &lt;mainClass&gt;$&#123;start-class&#125;&lt;/mainClass&gt;                    &lt;/manifest&gt;                &lt;/archive&gt;            &lt;/configuration&gt;        &lt;/plugin&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;            &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;            &lt;executions&gt;                &lt;execution&gt;                    &lt;id&gt;copy-dependencies&lt;/id&gt;                    &lt;phase&gt;package&lt;/phase&gt;                    &lt;goals&gt;                        &lt;goal&gt;copy-dependencies&lt;/goal&gt;                    &lt;/goals&gt;                    &lt;configuration&gt;                        &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt;                    &lt;/configuration&gt;                &lt;/execution&gt;            &lt;/executions&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;</code></pre><h4 id="2-file-settings-xml"><a href="#2-file-settings-xml" class="headerlink" title="2. file settings.xml"></a>2. file <code>settings.xml</code></h4><ul><li>Default storage in <code>./m2</code> directory, We can use absolute path with <code>--settings</code> parameter</li><li>Example:</li></ul><pre><code class="xml">&lt;servers&gt;    &lt;server&gt;        &lt;id&gt;tungexplorer-maven&lt;/id&gt;        &lt;username&gt;AKIASOJSS7XE5EHSQY6C&lt;/username&gt;        &lt;password&gt;nh7kQ84k5cyNvcEcapRo+e4gzo5OwYaYVaoS7vMQZ&lt;/password&gt;        &lt;configuration&gt;            &lt;wagonProvider&gt;s3&lt;/wagonProvider&gt;        &lt;/configuration&gt;    &lt;/server&gt;    &lt;server&gt;        &lt;id&gt;packagecloud-tungtv202&lt;/id&gt;        &lt;password&gt;a5ebcbe4709a89e5ae59a98d0052acd4f0035c3b908cecd7e&lt;/password&gt;    &lt;/server&gt;&lt;/servers&gt;</code></pre><ul><li><code>id</code> will be using in <code>pom.xml</code></li></ul><h4 id="3-Command"><a href="#3-Command" class="headerlink" title="3. Command"></a>3. Command</h4><pre><code class="bash">mvn clean source:jar deploy --settings $SETTINGS -DbuildNumber.version=$version -Dmaven.install.skip=true</code></pre><h3 id="Repo-lib-common-dependencies"><a href="#Repo-lib-common-dependencies" class="headerlink" title="Repo lib-common-dependencies"></a>Repo lib-common-dependencies</h3><ul><li>import owns dependency like as any other dependencies</li></ul><pre><code class="xml">&lt;dependency&gt;    &lt;groupId&gt;me.tungexplorer&lt;/groupId&gt;    &lt;artifactId&gt;app-common&lt;/artifactId&gt;    &lt;version&gt;0.0.5&lt;/version&gt;&lt;/dependency&gt;</code></pre><ul><li>Cons: we need import all <code>dependency</code> - that used in <code>lib-common</code>. Solution: when combine to jar file, we should<br>combine all dependencies (more size). Other is created a <code>lib-common-dependencies</code></li></ul><pre><code class="xml">&lt;parent&gt;    &lt;groupId&gt;me.tungexplorer&lt;/groupId&gt;    &lt;artifactId&gt;app-common-dependencies&lt;/artifactId&gt;    &lt;version&gt;0.1.3&lt;/version&gt;&lt;/parent&gt;</code></pre><ul><li><p><code>lib-common-dependencies</code> will be upload to maven repo (like as <code>lib-common</code>), But it will only file <code>pom.xml</code> (<br>without source code). In <code>pom.xml</code> file has defined all dependencies</p></li><li><p>NOTICE:</p></li></ul><pre><code class="xml">&lt;packaging&gt;pom&lt;/packaging&gt;</code></pre><h3 id="How-to-import"><a href="#How-to-import" class="headerlink" title="How to import?"></a>How to import?</h3><ul><li><code>pom.xml</code></li></ul><pre><code class="xml">&lt;repositories&gt;    &lt;repository&gt;        &lt;id&gt;tungtv202-release&lt;/id&gt;        &lt;url&gt;https://packagecloud.io/priv/819e36c995b147d6355673c5c65fdcb70412f13fga78f96ab/tungtv202/release/maven2        &lt;/url&gt;        &lt;releases&gt;            &lt;enabled&gt;true&lt;/enabled&gt;        &lt;/releases&gt;        &lt;snapshots&gt;            &lt;enabled&gt;true&lt;/enabled&gt;        &lt;/snapshots&gt;    &lt;/repository&gt;&lt;/repositories&gt;</code></pre><h2 id="What-is-different-between-dependencyManagement-and-dependencies"><a href="#What-is-different-between-dependencyManagement-and-dependencies" class="headerlink" title="What is different between dependencyManagement and dependencies"></a>What is different between <code>dependencyManagement</code> and <code>dependencies</code></h2><ul><li>Example: pom parent</li></ul><pre><code class="xml">&lt;dependencyManagement&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;3.8&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/dependencyManagement&gt;</code></pre><ul><li>pom child</li></ul><pre><code class="xml">&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;junit&lt;/groupId&gt;        &lt;artifactId&gt;junit&lt;/artifactId&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><ul><li>Artifacts specified in the <dependencies> section will ALWAYS be included as a dependency of the child module(s).</li><li>Artifacts specified in the <dependencyManagement> section will only be included in the child module if they were also<br>specified in the <dependencies> section of the child module itself. Why is it good, you ask? Because you specify the<br>version and&#x2F;or scope in the parent, and you can leave them out when specifying the dependencies in the child POM. This<br>can help you use unified versions for dependencies for child modules without specifying the version in each child<br>module.</li></ul><h2 id="Goal-vs-phase"><a href="#Goal-vs-phase" class="headerlink" title="Goal vs. phase"></a>Goal vs. phase</h2><ul><li>Example: We want to plugin will run at <code>test-compile</code> phase</li></ul><pre><code class="xml">&lt;plugin&gt;    &lt;groupId&gt;io.github.evis&lt;/groupId&gt;    &lt;artifactId&gt;scalafix-maven-plugin&lt;/artifactId&gt;    &lt;version&gt;0.1.6_0.9.29&lt;/version&gt;    &lt;executions&gt;        &lt;execution&gt;            &lt;id&gt;scala-check-style&lt;/id&gt;            &lt;goals&gt;                &lt;goal&gt;scalafix&lt;/goal&gt;            &lt;/goals&gt;            &lt;phase&gt;process-test-classes&lt;/phase&gt;        &lt;/execution&gt;    &lt;/executions&gt;&lt;/plugin&gt;</code></pre><ul><li><code>id</code> is unique. If we don’t define it, maven auto-create id has the format: <code>default-abcxyz</code></li><li>If we don’t define exactly <code>goal</code>, maven will auto using <code>goal</code><br>default (<a href="https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#built-in-lifecycle-bindings">https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#built-in-lifecycle-bindings</a>)</li><li>If a plugin doesn’t have <code>goal</code> default, this plugin will not run</li><li>Previous example: plugin scalafix has <code>scalafix</code> default goal. When maven is running to <code>process-test-classes</code> phase,<br>it will execute scalafix plugin.</li><li><a href="https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#lifecycle-reference"><code>phase list</code></a></li></ul><h2 id="Force-a-dependency-cleanup-on-the-build-phase"><a href="#Force-a-dependency-cleanup-on-the-build-phase" class="headerlink" title="Force a dependency cleanup on the build phase"></a>Force a dependency cleanup on the build phase</h2><p><code>mvn dependency:purge-local-repository ... </code> </p><h2 id="Check-new-version-of-dependencies"><a href="#Check-new-version-of-dependencies" class="headerlink" title="Check new version of dependencies"></a>Check new version of dependencies</h2><pre><code class="bash">mvn org.codehaus.mojo:versions-maven-plugin:display-dependency-updates &quot;-Dmaven.version.ignore=.*-M.*,.*-alpha.*,.*-RC.*,.*beta.*,.*Alpha.*,.*SNAP.*&quot;</code></pre><h2 id="Bash-install-maven"><a href="#Bash-install-maven" class="headerlink" title="Bash install maven"></a>Bash install maven</h2><pre><code>MAVEN_VERSION=3.9.4curl -Lf https://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz | tar -C /opt -xzvexport M2_HOME=/opt/apache-maven-$MAVEN_VERSIONln -s $M2_HOME/bin/mvn /usr/bin/mvn</code></pre><ul><li>Dockerfile</li></ul><pre><code class="Dockerfile">FROM eclipse-temurin:11-jdk-focalARG MAVEN_VERSION=3.9.4# Install gitRUN apt-get updateRUN apt-get install -y git wget unzip# Install MavenWORKDIR /rootRUN curl -Lf https://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz | tar -C /opt -xzvENV M2_HOME /opt/apache-maven-$MAVEN_VERSIONRUN ln -s $M2_HOME/bin/mvn /usr/bin/mvn</code></pre><h3 id="Re-run-the-test-failed-at-least-once-time"><a href="#Re-run-the-test-failed-at-least-once-time" class="headerlink" title="Re-run the test failed at least once time"></a>Re-run the test failed at least once time</h3><p>For case the unstable test, we can re-run the test failed at least once time. </p><pre><code class="xml">&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;    &lt;configuration&gt;        &lt;rerunFailingTestsCount&gt;2&lt;/rerunFailingTestsCount&gt;    &lt;/configuration&gt;&lt;/plugin&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> maven </tag>
            
            <tag> lib-common </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reactive - Reactor Note</title>
      <link href="/2021/02/Java/reactive/Reactive_Reactor_Note/"/>
      <url>/2021/02/Java/reactive/Reactive_Reactor_Note/</url>
      
        <content type="html"><![CDATA[<h1 id="Reactive-Reactor-Note"><a href="#Reactive-Reactor-Note" class="headerlink" title="Reactive - Reactor Note"></a>Reactive - Reactor Note</h1><h2 id="Hot-vs-Cold"><a href="#Hot-vs-Cold" class="headerlink" title="Hot vs. Cold"></a>Hot vs. Cold</h2><ul><li>Can using <code>.share()</code> for switching cold publisher to hot publisher. Then pipeline-evaluates will not re-trigger. But<br>only the elements that are emitted after these new subscriptions&#96;.</li></ul><pre><code class="java">Flux&lt;Long&gt; coldTicks = Flux.interval(Duration.ofSeconds(1));Flux&lt;Long&gt; clockTicks = coldTicks.share();clockTicks.subscribe(tick -&gt; System.out.println(&quot;clock1 &quot; + tick + &quot;s&quot;);Thread.sleep(2000);clockTicks.subscribe(tick -&gt; System.out.println(&quot;\tclock2 &quot; + tick + &quot;s&quot;);</code></pre><pre><code>Output:clock1 1sclock1 2sclock1 3s    clock2 3sclock1 4s    clock2 4sclock1 5s    clock2 5sclock1 6s    clock2 6s</code></pre><h2 id="How-to-debug-better"><a href="#How-to-debug-better" class="headerlink" title="How to debug better"></a>How to debug better</h2><ul><li><code>Hooks.onOperatorDebug();</code> - Should be using in <code>dev env</code>? (spent more resource?)</li><li><code>checkpoint</code></li></ul><pre><code class="java">private static void checkpoint() &#123;        int seconds = LocalTime.now().getSecond();        Mono&lt;Integer&gt; source;        if (seconds % 2 == 0) &#123;            source = Flux.range(1, 10)                         .elementAt(5)                         .checkpoint(&quot;source range(1,10)&quot;);        &#125;        else if (seconds % 3 == 0) &#123;            source = Flux.range(0, 4)                         .elementAt(5)                         .checkpoint(&quot;source range(0,4)&quot;);        &#125;        else &#123;            source = Flux.just(1, 2, 3, 4)                         .elementAt(5)                         .checkpoint(&quot;source just(1,2,3,4)&quot;);        &#125;        source.block(); //line 186</code></pre><ul><li><code>ReactorDebugAgent.init();</code></li></ul><h2 id="Reactor-Context"><a href="#Reactor-Context" class="headerlink" title="Reactor Context"></a>Reactor Context</h2><ol><li>Simple</li></ol><pre><code class="java">@Log4j2@RestController@SpringBootApplicationpublic class SimpleApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(SimpleApplication.class, args);    &#125;    private static Scheduler SCHEDULER = Schedulers.fromExecutor(Executors.newFixedThreadPool(10));    private static &lt;T&gt; Flux&lt;T&gt; prepare(Flux&lt;T&gt; in) &#123;        return in                .doOnNext(log::info)                .subscribeOn(SCHEDULER);    &#125;    Flux&lt;String&gt; read() &#123;        Flux&lt;String&gt; letters = prepare(Flux.just(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;));        Flux&lt;Integer&gt; numbers = prepare(Flux.just(1, 2, 3));        return prepare(Flux.zip(letters, numbers).map(tuple -&gt; tuple.getT1() + &#39;:&#39; + tuple.getT2()))                .doOnEach(signal -&gt; &#123;                    if (!signal.isOnNext()) &#123;                        return;                    &#125;                    ContextView context = signal.getContextView();                    Object userId = context.get(&quot;userId&quot;);                    log.info(&quot;user id for this pipeline stage for data &#39;&quot; + signal.get() + &quot;&#39;  is &#39;&quot; + userId + &quot;&#39;&quot;);                &#125;)                .contextWrite(Context.of(&quot;userId&quot;, UUID.randomUUID().toString()));    &#125;    @GetMapping(&quot;/data&quot;)    Flux&lt;String&gt; get() &#123;        return read();    &#125;&#125;</code></pre><ul><li>Output</li></ul><pre><code class="log">2021-02-02 21:33:36.088  INFO 22868 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port(s): 80802021-02-02 21:33:36.100  INFO 22868 --- [           main] e.r.learning.context.SimpleApplication   : Started SimpleApplication in 1.908 seconds (JVM running for 3.124)2021-02-02 21:33:40.467  INFO 22868 --- [pool-1-thread-3] e.r.learning.context.SimpleApplication   : 12021-02-02 21:33:40.467  INFO 22868 --- [pool-1-thread-2] e.r.learning.context.SimpleApplication   : A2021-02-02 21:33:40.469  INFO 22868 --- [pool-1-thread-2] e.r.learning.context.SimpleApplication   : B2021-02-02 21:33:40.469  INFO 22868 --- [pool-1-thread-2] e.r.learning.context.SimpleApplication   : C2021-02-02 21:33:40.470  INFO 22868 --- [pool-1-thread-3] e.r.learning.context.SimpleApplication   : A:12021-02-02 21:33:40.476  INFO 22868 --- [pool-1-thread-3] e.r.learning.context.SimpleApplication   : user id for this pipeline stage for data &#39;A:1&#39;  is &#39;9faa8b25-e09e-4028-9e0f-38f98f4264b1&#39;2021-02-02 21:33:40.493  INFO 22868 --- [pool-1-thread-3] e.r.learning.context.SimpleApplication   : 22021-02-02 21:33:40.493  INFO 22868 --- [pool-1-thread-3] e.r.learning.context.SimpleApplication   : 32021-02-02 21:33:40.505  INFO 22868 --- [pool-1-thread-4] e.r.learning.context.SimpleApplication   : B:22021-02-02 21:33:40.506  INFO 22868 --- [pool-1-thread-4] e.r.learning.context.SimpleApplication   : user id for this pipeline stage for data &#39;B:2&#39;  is &#39;9faa8b25-e09e-4028-9e0f-38f98f4264b1&#39;2021-02-02 21:33:40.511  INFO 22868 --- [pool-1-thread-4] e.r.learning.context.SimpleApplication   : C:32021-02-02 21:33:40.511  INFO 22868 --- [pool-1-thread-4] e.r.learning.context.SimpleApplication   : user id for this pipeline stage for data &#39;C:3&#39;  is &#39;9faa8b25-e09e-4028-9e0f-38f98f4264b1&#39;</code></pre><ol start="2"><li>Mdc</li></ol><ul><li><a href="https://github.com/spring-tips/reactor-context/blob/master/src/main/java/com/example/reactorcontext/mdc/MdcApplication.java">https://github.com/spring-tips/reactor-context/blob/master/src/main/java/com/example/reactorcontext/mdc/MdcApplication.java</a></li><li><a href="https://youtu.be/5tlZddM5Jo0">https://youtu.be/5tlZddM5Jo0</a></li></ul><h2 id="Reactive-Dos-and-Don’t"><a href="#Reactive-Dos-and-Don’t" class="headerlink" title="Reactive Dos and Don’t"></a>Reactive Dos and Don’t</h2><ul><li><a href="https://youtu.be/0rnMIueRKNU">https://youtu.be/0rnMIueRKNU</a></li></ul><ol><li>Dont</li></ol><ul><li><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/dont1.JPG" alt="Dont1"></li><li>-&gt; alway <code>return</code></li></ul><pre><code class="java">    static &lt;T&gt; Flux&lt;T&gt; addLogging(Flux&lt;T&gt; flux) &#123;        return flux                .doOnNext(it -&gt; System.out.println(&quot;Received&quot; + it))                .doOnError(e -&gt; e.printStackTrace())    &#125;    //    getFlux()        .transform(flux -&gt; addLogging(flux))        .subscribe();</code></pre><ol start="2"><li>Dont</li></ol><ul><li><p>side effect are not welcomed</p></li><li><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/dont2.JPG" alt="Dont2"></p></li><li><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/dont3.JPG" alt="Dont3"></p></li><li><p>use it for heavy computations</p></li><li><p>block non-blocking threads</p></li><li><p>don’t use ThreadLocals -&gt; dùng <code>Context</code></p></li></ul><pre><code class="java">      Mono.deferContextual(ctx -&gt; &#123;            return ctx.get(&quot;userId&quot;);        &#125;)                .contextWrite(Context.of(&quot;userId&quot;, &quot;1111&quot;))                .subscribe();</code></pre><ul><li>should not care about the thread (reactor care)</li></ul><ol start="4"><li>Do - check various operators</li></ol><ul><li><code>flatMap</code> - transform every item <code>concurrently</code> into a sub-stream, and join the current and the sub-stream</li><li><code>concatMap</code> - same as flatMap, but one-by-one</li><li><code>switchMap</code> - same as concatMap, but will cancel the previous sub-stream when a new item arrives</li><li><code>flatMapSequential</code> - same as flatMap, but preserves the order of sub-stream items according to the original stream’s<br>order</li></ul><ol start="5"><li>Do - think about the resiliency</li></ol><ul><li><code>.timeout(Duration)</code> - cancel the subscription and fail if no items emitted</li><li><code>.retry()/retryWithBackoff()</code> - retry the subscription on failure</li><li><code>.repeatWhenEmpty()</code> - repeat the subscription when it completes without values</li><li><code>.defaultEmpty()</code> - fallback when empty</li><li><code>.onErrorResume()</code> - fallback on error</li></ul><ol start="6"><li>Do</li></ol><ul><li>use <code>.checkpoint(&quot;something&quot;)</code> to “mark” reactive “milestones”</li><li>read about Hooks.onOperatorDebug()</li><li>use reactor-tools <code>ReactorDebugAgent</code></li></ul><h2 id="Solution-for-blocking"><a href="#Solution-for-blocking" class="headerlink" title="Solution for blocking"></a>Solution for blocking</h2><ul><li><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/solutionForBlocking1.JPG" alt="solutionForBlocking1"></li><li><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/solutionForBlocking2.JPG" alt="solutionForBlocking2"></li><li><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/solutionForBlocking3.JPG" alt="solutionForBlocking3"></li></ul><h3 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h3><ul><li>Imposter Reactive Method<ul><li>a method returning reactive types, but is implemented synchronously</li><li>e.g., performing significant work before subscribing</li></ul></li><li>Blocking Encapsulation<ul><li>hiding knowledge of blocking from the caller</li><li>e.g., blocking on the proper Scheduler without requiring the caller to do so</li></ul></li></ul><p><a href="https://youtu.be/xCu73WVg8Ps">https://youtu.be/xCu73WVg8Ps</a></p><ul><li><code>.subscribeOn(Schedulers.boundedElastic())</code></li><li>Bad examples</li><li><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/badExample1.JPG" alt="BadExample1"></li></ul><h3 id="Takeaways"><a href="#Takeaways" class="headerlink" title="Takeaways"></a>Takeaways</h3><ul><li>pick the best threading model for each app</li><li>isolate blocking to separate service if&#x2F;when possible</li><li>otherwise, isolate blocking to separate thread pool</li><li>avoid doing significant work before subscribing</li><li>encapsulate blocking at the lowest level possible</li><li>use <code>BlockHound</code> during testing</li></ul><h2 id="Reactive-for-Spring-MVC"><a href="#Reactive-for-Spring-MVC" class="headerlink" title="Reactive for Spring MVC"></a><code>Reactive</code> for Spring MVC</h2><ul><li><a href="https://youtu.be/IZ2SoXUiS7M">https://youtu.be/IZ2SoXUiS7M</a></li><li><a href="https://github.com/rstoyanchev/reactive-for-webmvc">https://github.com/rstoyanchev/reactive-for-webmvc</a></li></ul><h3 id="Best-Practices"><a href="#Best-Practices" class="headerlink" title="Best Practices"></a>Best Practices</h3><ul><li>don’t mix blocking and non-blocking APIs</li><li>vertical non-blocking slices</li><li>don’t put non-blocking code behind synchronous APIs</li><li>compose single, deferred, request handling chain</li><li>don’t use <code>block()</code>, <code>subscribe()</code> and the like</li><li>let spring MVC handle it</li></ul><h2 id="Servlet-or-Reactive-Stacks"><a href="#Servlet-or-Reactive-Stacks" class="headerlink" title="Servlet or Reactive Stacks"></a>Servlet or Reactive Stacks</h2><ul><li><p><a href="https://youtu.be/Dp_aJh-akkU">https://youtu.be/Dp_aJh-akkU</a></p></li><li><p><a href="https://github.com/rstoyanchev/demo-reactive-spring">https://github.com/rstoyanchev/demo-reactive-spring</a></p></li><li><p>We can have a hundred threads, a thousand threads, but finally, it is limited.</p></li><li><p>With the reactive module, we fixed the number of threads.</p></li><li><p>reactive paradigm is not necessarily for every application</p></li><li><p>Reason for WebFlux</p><ul><li>Gateways, edge services, high traffic</li><li>latency, streaming scenarios</li><li>high concurrency with less hardware resources</li><li>functional programming model</li><li>lightweight and transparent (less magic, more control)</li><li>Immutability</li></ul></li><li><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/webFluxFlow.JPG" alt="Spring WebFlux flow"></p></li><li><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/note/mvcFlow.JPG" alt="Spring MVC flow"></p></li></ul><h2 id="Observable"><a href="#Observable" class="headerlink" title="Observable"></a>Observable</h2><p>Has 3 channel</p><ul><li>data channel</li><li>error channel</li><li>complete channel</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> reactive </tag>
            
            <tag> reactor </tag>
            
            <tag> spring webflux </tag>
            
            <tag> observable </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reactive Programming - Reactor - DEMO 001</title>
      <link href="/2021/01/Java/reactive/ProjectReactorExercise/"/>
      <url>/2021/01/Java/reactive/ProjectReactorExercise/</url>
      
        <content type="html"><![CDATA[<h1 id="DEMO-CODE-PROJECT-REACTOR"><a href="#DEMO-CODE-PROJECT-REACTOR" class="headerlink" title="DEMO CODE - PROJECT REACTOR"></a>DEMO CODE - PROJECT REACTOR</h1><h2 id="Technical"><a href="#Technical" class="headerlink" title="Technical"></a>Technical</h2><ul><li>Java 11, Maven, Lombok</li><li>Spring boot, Spring WebFlux: for reactive web<ul><li>Webclient: reactive client to perform HTTP requests, non-blocking</li></ul></li><li>Project Reactor: for reactive programming<ul><li>BlockHound: Java agent to detect blocking calls from non-blocking threads.</li></ul></li><li>Jackson: JSON serialization&#x2F;deserialization library</li></ul><h2 id="De-bai"><a href="#De-bai" class="headerlink" title="Đề bài"></a>Đề bài</h2><p>Xây dựng api backend cho 1 hệ thống e-commerce, api có nhiệm vụ lấy ra danh sách <code>Popular purchases</code> theo <code>username</code>.<br>Cơ chế như sau:</p><ul><li>Lấy ra 5 giao dịch gần nhất, bằng cách call api tới 3rd <code>GET /api/purchases/by_user/:username?limit=5</code>.<br>Example: <code>GET /api/purchases/by_user/Jasen64</code><br>Response:</li></ul><pre><code class="json">&#123;  &quot;purchases&quot;: [    &#123;      &quot;id&quot;: 598185,      &quot;username&quot;: &quot;Jasen64&quot;,      &quot;productId&quot;: 996330,      &quot;date&quot;: &quot;2020-11-11T12:16:15.635Z&quot;    &#125;,    &#123;      &quot;id&quot;: 719912,      &quot;username&quot;: &quot;Jasen64&quot;,      &quot;productId&quot;: 17423,      &quot;date&quot;: &quot;2020-11-16T19:31:38.636Z&quot;    &#125;,    &#123;      &quot;id&quot;: 179878,      &quot;username&quot;: &quot;Jasen64&quot;,      &quot;productId&quot;: 653043,      &quot;date&quot;: &quot;2020-11-16T23:26:20.636Z&quot;    &#125;  ]&#125;</code></pre><ul><li>Với mỗi sản phẩm, lấy ra danh sách tất cả những người đã mua sản phẩm đấy, bằng cách call api<br>tới <code>GET /api/purchases/by_product/:product_id</code>.<br>Example: <code>GET /api/purchases/by_product/996330</code><br>Response:</li></ul><pre><code class="json">  &#123;  &quot;purchases&quot;: [    &#123;      &quot;id&quot;: 598185,      &quot;username&quot;: &quot;Jasen64&quot;,      &quot;productId&quot;: 996330,      &quot;date&quot;: &quot;2020-11-11T12:16:15.635Z&quot;    &#125;,    &#123;      &quot;id&quot;: 128524,      &quot;username&quot;: &quot;Gregg_Rowe34&quot;,      &quot;productId&quot;: 996330,      &quot;date&quot;: &quot;2020-11-21T01:48:24.637Z&quot;    &#125;,    &#123;      &quot;id&quot;: 494415,      &quot;username&quot;: &quot;Gregg_Rowe34&quot;,      &quot;productId&quot;: 996330,      &quot;date&quot;: &quot;2020-11-16T23:14:56.637Z&quot;    &#125;  ]&#125;  </code></pre><ul><li>Cùng thời điểm, lấy ra danh sách thông tin về Sản phẩm, bằng cách call api tới <code>GET /api/products/:product_id</code>.<br>Example: <code>GET /api/products/996330</code><br>Response:</li></ul><pre><code class="json">&#123;  &quot;product&quot;: &#123;    &quot;id&quot;: 996330,    &quot;face&quot;: &quot;(ᵒ̤̑ ₀̑ ᵒ̤̑)&quot;,    &quot;price&quot;: 1165,    &quot;size&quot;: 23  &#125;&#125;</code></pre><ul><li>Kết quả của api lấy thông tin <code>Popular Purchase</code> format như sau:</li></ul><pre><code class="json">[  &#123;    &quot;id&quot;: 555622,    &quot;face&quot;: &quot; 。◕‿◕。 &quot;,    &quot;price&quot;: 1100,    &quot;size&quot;: 27,    &quot;recent&quot;: [      &quot;Frannie79&quot;,      &quot;Barney_Bins18&quot;,      &quot;Hortense6&quot;,      &quot;Melvina84&quot;    ]  &#125;,  ...]</code></pre><ul><li>Dữ liệu của <code>Popular Purchase</code> phải được cache</li><li>Dữ liệu của <code>Popular Purchase</code> phải được sắp xếp theo product có số lượng người mua nhiều nhất lên đầu.</li></ul><h2 id="Step-by-step"><a href="#Step-by-step" class="headerlink" title="Step by step"></a>Step by step</h2><h3 id="First-round"><a href="#First-round" class="headerlink" title="First round"></a>First round</h3><ol><li>Webclient</li></ol><ul><li>Để call tới hệ thống 3rd qua api, cần 1 http client. Trong Spring 5, có hỗ trợ reactive web client là <code>Webclient</code>. Nó<br>hỗ trợ function programming, syntax khá tiện, tiện như <code>openFeign client</code> trong spring cloud.</li></ul><pre><code class="java">@Slf4j@Configurationpublic class ClientConfig &#123;    // base url của hệ thống 3rd    @Value(&quot;$&#123;client.base_url&#125;&quot;)    private String baseUrl;    @Bean    @Primary    public WebClient webClient() &#123;        var strategies = ExchangeStrategies.builder()            .codecs(clientCodecConfigurer -&gt; &#123;                clientCodecConfigurer.defaultCodecs().jackson2JsonDecoder(new CustomizeJsonDecoder(objectMapper(), MediaType.APPLICATION_JSON));            &#125;)            .build();        return WebClient.builder()            .exchangeStrategies(strategies)            .baseUrl(baseUrl)            .defaultHeader(HttpHeaders.CONTENT_TYPE, MediaType.APPLICATION_JSON_VALUE)            .build();    &#125;    // Sử dụng jackson để deserialize     private ObjectMapper objectMapper() &#123;        ObjectMapper objectMapper = new ObjectMapper();        objectMapper.configure(DeserializationFeature.UNWRAP_ROOT_VALUE, true);        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);        return objectMapper;    &#125;&#125;</code></pre><pre><code class="java">@Slf4j@Componentpublic class UserClient &#123;    @Autowired    private WebClient webClient;    public Mono&lt;User&gt; get(String username) &#123;        return webClient.get()            .uri(&quot;users/&#123;username&#125;&quot;, username)            .retrieve()            .bodyToMono(User.class)            .doOnError(e -&gt; log.error(e.getMessage()));    &#125;&#125;@Slf4j@Componentpublic class ProductClient &#123;    @Autowired    private WebClient webClient;    public Mono&lt;Product&gt; get(int id) &#123;        log.info(&quot;Start get productId=&#123;&#125;&quot;, id);        return webClient.get()            .uri(&quot;products/&#123;id&#125;&quot;, id)            .retrieve()            .bodyToMono(Product.class)            .doOnNext(e -&gt; log.info(&quot;Product id=&#123;&#125;, data=&#123;&#125;&quot;, id, e))            .delayElement(Duration.ofMillis(500))            .doOnError(e -&gt; log.error(e.getMessage()));    &#125;&#125;@Component@Slf4jpublic class PurchaseClient &#123;    @Autowired    private WebClient webClient;    public Mono&lt;PurchaseList&gt; listByUsername(String username, int limit) &#123;        log.info(&quot;Start listByUsername, username=&#123;&#125;&quot;, username);        return webClient.get()            .uri(uriBuilder -&gt; uriBuilder.path(&quot;purchases/by_user/&quot; + username)                .queryParam(&quot;limit&quot;, limit)                .build())            .retrieve()            .bodyToMono(PurchaseList.class)            .doOnNext(e -&gt; log.info(&quot;PurchaseList username=&#123;&#125;, data=&#123;&#125;&quot;, username, e))            .delayElement(Duration.ofMillis(1000))            .doOnError(e -&gt; log.error(e.getMessage()));    &#125;    public Mono&lt;PurchaseList&gt; listByProductId(int productId, int limit) &#123;        log.info(&quot;Start listByProductId, id=&#123;&#125;&quot;, productId);        return webClient.get()            .uri(uriBuilder -&gt; uriBuilder.path(&quot;purchases/by_product/&quot; + productId)                .queryParam(&quot;limit&quot;, limit).build())            .retrieve()            .bodyToMono(PurchaseList.class)            .doOnNext(e -&gt; log.info(&quot;listByProductId productId=&#123;&#125;, data=&#123;&#125;&quot;, productId, e))            .delayElement(Duration.ofMillis(600))            .doOnError(e -&gt; log.error(e.getMessage()));    &#125;&#125;</code></pre><ul><li>Sử dụng <code>doOnNext</code> để xem log</li><li>Sử dụng <code>delayElement</code> để giả lập thời gian mà api 3rd trả về.</li></ul><ol start="2"><li>LocalCacheServiceImpl</li></ol><ul><li>Cache trực tiếp trên localmem</li></ul><pre><code class="java">public static final int MAX_AGE=30000;public static Map&lt;String, CacheValue&gt; INMEMORY_DATABASE=new ConcurrentHashMap&lt;&gt;();@Overridepublic List&lt;PopularPurchasesDto&gt; getPopularPurchases(String username)&#123;    log.info(&quot;getPopularPurchases , username=&#123;&#125;&quot;,username);    if(ObjectUtils.isEmpty(username))return null;    var cacheValue=INMEMORY_DATABASE.get(username);    return cacheValue==null?null:(System.currentTimeMillis()-cacheValue.getCommittedTime()&gt;MAX_AGE    ?null:cacheValue.getPopularPurchasesDto());    &#125;@Overridepublic void setPopularPurchases(String username,Flux&lt;PopularPurchasesDto&gt; value)&#123;    List&lt;PopularPurchasesDto&gt; cacheValue=new ArrayList&lt;&gt;();    value.collectList().map(cacheValue::addAll).subscribe();    INMEMORY_DATABASE.put(username,new CacheValue(cacheValue));    &#125;@Overridepublic void setPopularPurchases(String username,List&lt;PopularPurchasesDto&gt; value)&#123;    log.info(&quot;setPopularPurchases, username=&#123;&#125;&quot;,username);    INMEMORY_DATABASE.put(username,new CacheValue(value));    &#125;</code></pre><ul><li>evict manual (có thể sử dụng Caffein Cache hoặc Redis, để cache auto evict, nếu muốn làm phức tạp hơn)</li></ul><ol start="3"><li>Route Controller</li></ol><ul><li>Spring web Flux hỗ trợ việc khai báo các controller kiểu <code>HandlerFunction and RouterFunctions</code></li></ul><pre><code class="java"> @Bean    RouterFunction&lt;ServerResponse&gt; routes(PurchaseHandler handler)&#123;    return route(GET(&quot;/api/recent_purchases/&#123;username&#125;&quot;),handler::purchasesRecent);    &#125;</code></pre><ol start="4"><li>Core logic</li></ol><pre><code class="java">    public Mono&lt;ServerResponse&gt; purchasesRecent(ServerRequest request)&#123;final var username=request.pathVariable(&quot;username&quot;);    return userClient.get(username)    .flatMap(c-&gt;ok().contentType(MediaType.APPLICATION_JSON)    .body(getPopularPurchasesDto(username),PopularPurchasesDto.class))    .switchIfEmpty(ServerResponse.status(HttpStatus.NOT_FOUND)    .bodyValue(String.format(&quot;User with username of %s  was not found&quot;,username)));    &#125;private Flux&lt;PopularPurchasesDto&gt; getPopularPurchasesDto(String username)&#123;    var cacheValue=cacheService.getPopularPurchases(username);    if(cacheValue!=null)return Flux.fromIterable(cacheValue);    var productFlux=purchaseClient.listByUsername(username,PURCHASES_RECENT_LIMIT)    .flatMapMany(Flux::fromIterable)    .flatMap(e-&gt;productClient.get(e.getProductId()));    var popularPurchasesDtoFlux=productFlux.flatMap(e-&gt;    purchaseClient.listByProductId(e.getId(),Integer.MAX_VALUE)    .map(r-&gt;new PopularPurchasesDto(e,r)));    // sort    var result=popularPurchasesDtoFlux.collectSortedList((a,b)-&gt;    a.getRecentUsername().size()&gt;b.getRecentUsername().size()?-1:0)    .flatMapMany(Flux::fromIterable);    cacheService.setPopularPurchases(username,result);    return result;    &#125;</code></pre><ul><li>Ý đồ code: sau khi lấy ra được danh sách 5 giao dịch gần nhất. Thì các api lấy danh sách user đã mua theo sản phẩm, và<br>api lấy thông tin sản phẩm, có thể được call 1 cách song song, độc lập, đồng thời, để giảm tổng thời gian truy vấn.</li><li>Flow: get value từ cache -&gt; có thì trả về, không có thì call từ api tới 3rd, tính toán ra DTO -&gt; lưu vào cache -&gt; trả<br>về.</li></ul><h3 id="Phat-hien-cac-van-de-va-cai-tien"><a href="#Phat-hien-cac-van-de-va-cai-tien" class="headerlink" title="Phát hiện các vấn đề và cải tiến"></a>Phát hiện các vấn đề và cải tiến</h3><ol><li>Use <code>zip</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/shouldUseZip.JPG" alt="Should Use Zip"></li></ol><ul><li>Các api lấy thông tin sản phẩm, và api lấy danh sách người mua theo sản phẩm, được chạy ở các thread khác<br>nhau (<code>ctor-http-nio-4</code>, <code>ctor-http-nio-5</code>, <code>parallel-4</code>, <code>parallel-1</code> ) &#x3D;&gt; có vẻ đã chạy đúng ý đồ, none-blocking</li><li>Nhưng các <code>PurchaseClient</code> chỉ được run, khi các <code>ProductClient</code> đã done. Thực tế thì có thể triển khai 2 client<br>này <code>run</code> cùng lúc.</li><li>Chuyền <code>double flatMap</code> &#x3D;&gt; <code>zip</code></li></ul><pre><code class="java">    private Mono&lt;List&lt;PopularPurchasesDto&gt;&gt;getPopularPurchasesWithoutCache(String username)&#123;    log.info(&quot;getPopularPurchasesWithoutCache - username: &#123;&#125;&quot;,username);    return purchaseClient.listByUsername(username,PURCHASES_RECENT_LIMIT)    .flatMapMany(Flux::fromIterable)    .flatMap(e-&gt;Flux.zip(productClient.get(e.getProductId()),    purchaseClient.listByProductId(e.getProductId(),Integer.MAX_VALUE)))    .map(e-&gt;new PopularPurchasesDto(e.getT1(),e.getT2()))    .collectSortedList((a,b)-&gt;    a.getRecentUsername().size()&gt;b.getRecentUsername().size()?-1:0);    &#125;</code></pre><ul><li>Kết quả <code>PurchaseClient</code> và  <code>ProductClient</code> run parallel<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/shoudUseZipResult.JPG" alt="Should Use ZIp Result"></li></ul><ol start="2"><li>Cold publisher problem</li></ol><ul><li>Old code:</li></ul><pre><code class="java">   public void setPopularPurchases(String username,Flux&lt;PopularPurchasesDto&gt; value)&#123;    List&lt;PopularPurchasesDto&gt; cacheValue=new ArrayList&lt;&gt;();    value.collectList().map(cacheValue::addAll).subscribe();    INMEMORY_DATABASE.put(username,new CacheValue(cacheValue));    &#125;</code></pre><ul><li>Việc truyền vào 1 <code>Flux&lt;PopularPurchasesDto&gt;</code>, và Flux đang mặc định là <code>cold publisher</code>, sau đó thực<br>hiện <code>.subscribe()</code> khiến cho toàn bộ các api được call lại 1 lần nữa. &#x3D;&gt; BAD</li><li>New Cold: nhét method <code>setPopularPurchases</code> vào trong <code>doOnNext</code> của <code>publisher</code></li></ul><pre><code class="java">    private Flux&lt;PopularPurchasesDto&gt; getPopularPurchasesDto3(String username)&#123;    var cacheValue=cacheService.getPopularPurchasesAsync(username)    .switchIfEmpty(getPopularPurchasesWithoutCache(username)    .doOnSuccess(e-&gt;cacheService.setPopularPurchases(username,e)));    return cacheValue.flatMapMany(Flux::fromIterable);    &#125;</code></pre><ol start="3"><li>Something keep <code>blocking</code></li></ol><ul><li>Theo 1 số blog chia sẻ trên mạng, thì method <code>getPopularPurchases(String username)</code> hiện tại có thể dẫn tới blocking (<br>mình chưa tái hiện được). &#x3D;&gt; giải pháp thay thế:</li></ul><pre><code class="java">    @Overridepublic Mono&lt;List&lt;PopularPurchasesDto&gt;&gt;getPopularPurchasesAsync(String username)&#123;    return Mono.fromCompletionStage(CompletableFuture.supplyAsync(()-&gt;getPopularPurchases(username)));    &#125;</code></pre><ul><li>Sử dụng <code>blockhood</code> để phát hiện blocking</li></ul><pre><code class="xml">&lt;dependency&gt;    &lt;groupId&gt;io.projectreactor.tools&lt;/groupId&gt;    &lt;artifactId&gt;blockhound&lt;/artifactId&gt;    &lt;version&gt;1.0.4.RELEASE&lt;/version&gt;&lt;/dependency&gt;</code></pre><ul><li>Install blockhound</li></ul><pre><code class="java">    public static void main(String[]args)&#123;    BlockHound.install();    SpringApplication.run(ReactorApplication.class,args);    &#125;</code></pre><ul><li>Test khi call api 3rd, và thấy bị <code>blocking</code> &#x3D;&gt; Có vẻ như server 3rd, không hỗ trợ việc reactive<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/reactor/BlockingBy3rd.JPG" alt="Blocking by 3rd"></li></ul><h2 id="How-to-run-source-code"><a href="#How-to-run-source-code" class="headerlink" title="How to run source code?"></a>How to run source code?</h2><ol><li>Maven clean &amp; package</li></ol><p><code>./mvnw clean package</code></p><ol start="2"><li><p>Change config file</p><ul><li><code>src/main/resources/application.properties</code><ul><li>Change api base_url (nodejs server, that run at <code>daw-purchases-master</code>)</li><li>Change port web server (default is 8080)</li></ul></li></ul></li><li><p>Run spring-boot<br><code>./mvnw spring-boot:run </code></p></li><li><p>Test api - get recent purchases</p></li></ol><pre><code class="bash">curl --request GET &#39;http://127.0.0.1:8080/api/recent_purchases/Jasen64&#39; </code></pre><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li><p>Bài lap focus vào <code>reactive</code> nên sẽ <code>do it</code> mọi nơi có thể. Nhưng thực tế nhiều chỗ đang sử dụng nó 1 cách không hiệu<br>quả.</p><ul><li>Các api của 3rd không hỗ trợ reactive</li><li>Danh sách <code>Popular purchase</code> mà hệ thống query và trả về cho client thông qua api <code>/api/recent_purchases/&#123;user&#125;</code>,<br>không cần stream, bởi kết quả là 1 danh sách, yêu cầu phải được <code>sort</code>. Việc sort này được thực hiện khi toàn bộ<br>List DTO đã DONE.</li></ul></li><li><p>Sourcecode: <a href="https://github.com/tungtv202/reactor_flux_001">https://github.com/tungtv202/reactor_flux_001</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> reactive </tag>
            
            <tag> reactor </tag>
            
            <tag> spring webflux </tag>
            
            <tag> webclient </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nhật ký cải tiến / xây mới ứng dụng Sync Data (via Rest API)</title>
      <link href="/2021/01/Stories/App_Sync/"/>
      <url>/2021/01/Stories/App_Sync/</url>
      
        <content type="html"><![CDATA[<h1 id="Bai-toan"><a href="#Bai-toan" class="headerlink" title="Bài toán"></a>Bài toán</h1><ul><li>Xây dựng ứng dụng phục vụ việc đồng bộ dữ liệu giữa 2 hệ thống (thông qua call API)</li><li>Đối tượng cần đồng đồng bộ: dữ liệu của khách hàng (ví dụ thông tin sản phẩm, đơn hàng…)</li><li>Đồng bộ tự động khi 2 bên có thay đổi dữ liệu</li><li>Đồng bộ thủ công khi có request đồng bộ từ user</li></ul><h1 id="He-thong-cu-dang-co"><a href="#He-thong-cu-dang-co" class="headerlink" title="Hệ thống cũ đang có"></a>Hệ thống cũ đang có</h1><h2 id="Co-che"><a href="#Co-che" class="headerlink" title="Cơ chế"></a>Cơ chế</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/app_sync/OldArt.JPG" alt="OldArt"></p><ul><li><code>Sync App</code> đăng ký webhook với 2 hệ thống <code>A</code> và <code>B</code>. Khi thông tin sản phẩm trên 2 hệ thống A và B có thay đổi, 2 hệ<br>thống A và B sẽ call webhook tới địa chỉ <code>Sync App</code> đăng ký</li><li><code>I</code> là 1 application gateway, nhận webhook từ A và B. <code>I</code> cần đảm bảo tính Available cực cao.</li><li><code>I</code> nhận webhook, gửi message vào Kafka. Với mỗi <code>type</code> đồng bộ, sẽ có 1 topic khác nhau.</li><li><code>Sync App</code> consumer message từ Kafka và thực hiện nghiệp vụ đồng bộ bằng cách lấy thông tin mới nhất từ <code>source</code>, kết<br>hợp với thông tin được lưu trong database, sau đó tạo ra model (body) để call api sang <code>dest</code></li></ul><h2 id="Cac-van-de"><a href="#Cac-van-de" class="headerlink" title="Các vấn đề"></a>Các vấn đề</h2><h3 id="Van-de-chu-quan"><a href="#Van-de-chu-quan" class="headerlink" title="Vấn đề chủ quan"></a>Vấn đề chủ quan</h3><ul><li>code cũ rất khó maintaince. Không handler được các error, không phân tách được nghiệp vụ đúng của đồng bộ tự động, và<br>thủ công.</li></ul><h3 id="Van-de-khach-quan"><a href="#Van-de-khach-quan" class="headerlink" title="Vấn để khách quan"></a>Vấn để khách quan</h3><ul><li>Khi hệ thống A hoặc B rơi vào cao điểm, số lượng webhook được call tới I rất nhiều. Vì call qua api, nên có thể khi<br>request được nhận tại I không còn đảm bảo được đúng thứ tự &#x3D;&gt; sai lệch đồng bộ.</li><li><code>Sync App</code> lấy message từ Kafka để trigger đồng bộ. Nhưng không thể scale được, vì phụ thuộc vào số partition của<br>Kafka, dẫn tới msg có thể bị lag rất nhiều nếu peak.</li><li>Cơ chế retry khi call api từ <code>Sync App</code> sang A,B bị lỗi hoạt động không hiệu quả. (bị ignore, hoặc retry không đúng<br>thời điểm mình muốn retry)</li></ul><h2 id="Van-de-can-suy-nghi-khi-quyet-dinh-code-lai-app"><a href="#Van-de-can-suy-nghi-khi-quyet-dinh-code-lai-app" class="headerlink" title="Vấn đề cần suy nghĩ khi quyết định code lại app"></a>Vấn đề cần suy nghĩ khi quyết định code lại app</h2><ul><li>Làm thế nào để Sync App nhận được message báo có sự thay đổi dữ liệu từ 2 hệ thống, nhưng phải đảm bảo đúng thứ tự?</li><li>Làm thế nào để lọc các message bị trùng lặp. (khi lỗi hệ thống, hoặc khi hệ thống <code>Sync App</code> thực hiện call api đồng<br>bộ sang A, nhưng ngay sau đó A lại gửi lại message thay đổi data ngược lại <code>Sync App</code> ?) điều này tiềm ẩn nguy cơ<br>loop.</li><li>Làm thế nào để scale được worker tùy ý?</li><li>Làm thế nào để chủ động retry việc đồng bộ khi có lỗi hệ thống A, B?</li></ul><h1 id="He-thong-moi"><a href="#He-thong-moi" class="headerlink" title="Hệ thống mới"></a>Hệ thống mới</h1><h2 id="Co-che-1"><a href="#Co-che-1" class="headerlink" title="Cơ chế"></a>Cơ chế</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/app_sync/NewArt.JPG" alt="Hệ thống mới"></p><ul><li>Không sử dụng cơ chế nghe webhook khi có data thay đổi. (bỏ hệ thống <code>I</code>). &#x3D;&gt; Consum trực tiếp từ Kafka nội bộ của A<br>và B.</li><li>Việc sync data từ <code>SyncApp</code> tới A và B vẫn sử dụng call Rest API như cũ.</li><li>Khi nhận được msg kafka báo có dữ liệu thay đổi. Sync App sẽ filter và lưu vào database. Sau đó 1 task vụ khác sẽ<br>query từ database ra để xử lý. (Database được sử dụng nhiều hơn. Sử dụng database Sql Server như 1 queue.)</li></ul><h2 id="Giai-quyet-cac-van-de"><a href="#Giai-quyet-cac-van-de" class="headerlink" title="Giải quyết các vấn đề"></a>Giải quyết các vấn đề</h2><ul><li>Đảm bảo thứ tự dữ liệu thay đổi?<blockquote><p>Đảm bảo tuyệt đối tính thứ tự của mỗi topic A hoặc B. Tức là khi thay đổi dữ liệu bên A thì msg gửi đi luôn đảm bảo thứ tự đúng khi nhận được <code>Sync App</code>.</p></blockquote><blockquote><p>Đảm bảo tương đối tính thứ tự với 2 hệ thống A và B. Tức là với 1 sản phẩm X có trên cả A và B. Và X có sự thay đổi ở cả A và B. Thực tế X được thay đổi trên A trước. Nhưng có thể Kafka A bị lag, dẫn tới thay đổi trên B diễn ra sau, nhưng  <code>Sync App</code> lại nhận được trước. Case này % rất thấp????</p></blockquote></li><li>Khi consum msg Kafka A và Kafka B, sẽ có rất nhiều msg không thuộc phạm vi của <code>Sync App</code>, làm thể nào để filter nó 1<br>cách nhanh nhất có thể?<blockquote><p>Toàn bộ danh sách storeId thuộc phạm vi quản lý của SynApp sẽ được chứa trong 1 HashSet. Sử dụng HashSet này để filter cho performance nhanh nhất. HashSet này sẽ được load khi startUp app, và cập nhật định kỳ + cập nhật ngay lập tức khi có handler.</p></blockquote></li><li>Làm thế nào để scale worker tùy ý?<blockquote><p>Sử dụng table trong database như 1 queue.</p></blockquote></li><li>Table <code>SynRequests</code> (table core)</li></ul><pre><code class="sql">create table SyncRequests(    Id             int identity        constraint SyncRequests_pk            primary key,    StoreId        int                        not null,    SyncType       nvarchar(100)              not null,    SyncAction     nvarchar(100),    Target         nvarchar(100),    ObjectId       int                        not null,    SubObjectId    int,    Origin         nvarchar(50),    ProgressStatus nvarchar(100)              not null,    FireCounter    int,    Payload        nvarchar(max),    CreatedOn      datetime default getdate() not null,    Error          nvarchar(max),    PrevFireTime   datetime,    ForceSync      bit)</code></pre><pre><code>- SyncType: enum, quy định type sync, ví dụ: product_a_to_b, product_b_to_a ...- SyncAction: enum, quy định action sync, ví dụ: deleted, update- Target: enum, quy định đối tượng sync, ví dụ: product, order, inventory- ObjectId, SubObjectId: lưu trữ thông tin Id của product, order...- Origin: enum, quy định nguồn gốc của lệnh sync: auto, manual, system- ProgressStatus: enum. Trạng thái của lệnh sync. Waiting, processing, fail, success. Khi đang ở trạng thái `processing` thì worker sẽ filter, không xử lý. Khi success thì sẽ bị xóa bản ghi này đi. Khi fail, thì sẽ bị giữ lại. Sẽ có 1 job schedule scan định kỳ set trạng thái từ fail -&gt; waiting- FireCounter: số lần lệnh sync được chạy, bình thường nếu sync thành công fireCounter = 0, tuy nhiên nếu fail thì fireCounter sẽ được tăng dần lên. Và khi tới ngưỡng, mà vẫn fail, thì sẽ bị loại bỏ khỏi table.- Payload: chứa dữ liệu trong msg được gửi từ A , B- Error: lưu lại message lỗi khi đồng bộ - PrevFireTime: thời gian của lần sync gần nhất. - ForceSync: flag, hỗ trợ nghiệp vụ sync, trong trường hợp maintaince.</code></pre><h2 id="Start"><a href="#Start" class="headerlink" title="Start"></a>Start</h2><ul><li>Như mọi khi, chuyển đổi toàn bộ Spring RestTemplate sang dùng OpenFeign Client. Rất thuận tiện cho việc call api từ<br>SyncApp tới A, B.<ul><li>Sửa lại config Feign, handler ở <code>Decode</code> để log lại toàn bộ api đã được call từ <code>SyncApp</code> sang A và B. (method !&#x3D;<br>GET)</li><li>Sử dụng thêm header <code>accept gzip</code> để tăng performance call api. Nhưng không đạt hiệu quả, vì các api của A, B<br>không đồng nhất header.</li></ul></li><li>Chuyển sang dùng <code>enum</code> ở bất kỳ chỗ nào có thể.</li><li>Sử dụng JPA thay cho JDBC thuần</li><li>Tất cả các model, khuyến khích sử dụng @Builder tối đa. Nhằm tránh việc sử dụng style code Setter, khó tracking code<br>khi bị mutable.</li><li><code>StoreContext</code> model core dự án. Ngoài chứa thông tin store, sẽ chứa toàn bộ thông tin cấu hình đồng bộ. Vì được sử<br>dụng cực kỳ nhiều, nên sẽ được local cache. Và sẽ refresh mỗi khi có 1 sync request mới</li><li>Lưu ý: code cần handler việc khi app startup xong 1 time, load xong toàn bộ danh sách storeId từ db vào HashSet xong,<br>thì mới trigger bật kafka consumer.</li></ul><pre><code class="java">public class InitInMemoryDbTask &#123;    @Autowired    private CacheService cacheService;    @Autowired    private KafkaListenerEndpointRegistry kafkaListenerContainerFactory;    @Scheduled(fixedRate = 10000)    void initAllPosTenantIdAndWebStoreId() &#123;        cacheService.initAllPosTenantId();        cacheService.initAllWebStoreId();        if (!Constants.KAFKA_IS_RUNNING) &#123;            kafkaListenerContainerFactory.getAllListenerContainers().forEach(e -&gt; &#123;                if (!e.isRunning()) &#123;                    e.start();                &#125;            &#125;);            Constants.KAFKA_IS_RUNNING = true;        &#125;    &#125;&#125;</code></pre><ul><li>Việc sử dụng <code>jib</code> để build docker, cho tốc độ deploy ci thật nhanh</li></ul><pre><code class="xml">&lt;plugins&gt;    &lt;plugin&gt;        &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt;        &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt;        &lt;version&gt;2.6.0&lt;/version&gt;    &lt;/plugin&gt;    &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;        &lt;configuration&gt;            &lt;annotationProcessorPaths&gt;                &lt;path&gt;                    &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;                    &lt;artifactId&gt;lombok&lt;/artifactId&gt;                    &lt;version&gt;$&#123;lombok.version&#125;&lt;/version&gt;                &lt;/path&gt;                &lt;path&gt;                    &lt;groupId&gt;org.mapstruct&lt;/groupId&gt;                    &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt;                    &lt;version&gt;1.4.1.Final&lt;/version&gt;                &lt;/path&gt;            &lt;/annotationProcessorPaths&gt;        &lt;/configuration&gt;    &lt;/plugin&gt;&lt;/plugins&gt;</code></pre><ul><li>Interceptor <code>FeignDecode</code> để hứng body từ FeignClient cho mục đích log. (hơi lằng nhằng vì response có thể<br>được <code>gzip</code>)</li></ul><pre><code class="java">     Response finalResponse = null;        var headerResponseEncoding = response.headers().get(&quot;content-encoding&quot;);        if (!CollectionUtils.isEmpty(headerResponseEncoding) &amp;&amp; headerResponseEncoding.contains(&quot;gzip&quot;)) &#123;            String decompressedBody = decompress(response);            assert decompressedBody != null;            finalResponse = response.toBuilder().body(decompressedBody.getBytes()).build();        &#125; else &#123;            finalResponse = response;        &#125;        ByteArrayOutputStream baos = new ByteArrayOutputStream();        finalResponse.body().asInputStream().transferTo(baos);        Reader reader = new InputStreamReader(new ByteArrayInputStream(baos.toByteArray()));        if (logClient != null) &#123;            logClient.setResponse(baos.toString(StandardCharsets.UTF_8));            logClientJPARepository.save(logClient);        &#125;</code></pre><ul><li>Cần 1 localCache để điều phối việc log sync khi chạy trên instance &#x3D;&gt; Sử dụng CaffeeIn</li></ul><pre><code class="java">@Configurationpublic class CacheConfig &#123;    public static final int CACHE_DURATION_MINUTES = 10;    @Bean(&quot;syncRequestCache&quot;)    public LoadingCache&lt;String, Boolean&gt; syncRequestCache() &#123;        return Caffeine.newBuilder()                .expireAfterWrite(CACHE_DURATION_MINUTES, TimeUnit.MINUTES)                .build(key -&gt; Boolean.FALSE);    &#125;&#125;</code></pre><ul><li>Cần 1 JsonConverter để đọc msg từ Kafka</li></ul><pre><code class="java">    @Primary    @Bean    public JsonConverter jsonConverter() &#123;        var jsonConverter = new JsonConverter();        var configs = new HashMap&lt;String, Object&gt;();        configs.put(&quot;schemas.enable&quot;, &quot;true&quot;);        jsonConverter.configure(configs, false);        return jsonConverter;    &#125;</code></pre><pre><code class="java">var oder = KafkaConnectHelper.convertValueToModel(record.topic(), record.value(), jsonConverter, OrderMsg.class);</code></pre><ul><li>Cần Override lại config messageConverter khi call api</li></ul><pre><code class="java">@Configuration@EnableSpringDataWebSupportpublic class WebConfig extends WebMvcConfigurationSupport &#123;    @Qualifier(&quot;json&quot;)    @Autowired    private ObjectMapper json;    @Override    public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123;        var converter = new MappingJackson2HttpMessageConverter();        converter.setObjectMapper(json);        var stringHttpMessageConverter = new StringHttpMessageConverter();        stringHttpMessageConverter.setWriteAcceptCharset(false);        converters.add(new ByteArrayHttpMessageConverter());        converters.add(stringHttpMessageConverter);        converters.add(new ResourceHttpMessageConverter());        converters.add(new SourceHttpMessageConverter&lt;&gt;());        converters.add(converter);    &#125;    @Override    public void addArgumentResolvers(List&lt;HandlerMethodArgumentResolver&gt; argumentResolvers) &#123;        argumentResolvers.add(new PageableHandlerMethodArgumentResolver());    &#125;&#125;</code></pre><ul><li>Sử dụng <code>org.zalando.problem</code> để handler message lỗi cho api, cần khai báo ObjectMapper</li></ul><pre><code class="java">    @Bean(name = &#123;&quot;json&quot;&#125;)    public ObjectMapper json() &#123;        ObjectMapper objectMapper = new ObjectMapper();        objectMapper.disable(SerializationFeature.WRAP_ROOT_VALUE);        objectMapper.setPropertyNamingStrategy(PropertyNamingStrategies.SNAKE_CASE);        objectMapper.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);        objectMapper.configure(DeserializationFeature.READ_ENUMS_USING_TO_STRING, true);        objectMapper.configure(SerializationFeature.WRITE_ENUMS_USING_TO_STRING, true);        objectMapper.setDateFormat(new StdDateFormat());        SimpleFilterProvider filters = new SimpleFilterProvider();        filters.addFilter(&quot;empty&quot;, SimpleBeanPropertyFilter.serializeAllExcept(new HashSet&lt;&gt;()));        filters.addFilter(&quot;field&quot;, SimpleBeanPropertyFilter.serializeAllExcept(new HashSet&lt;&gt;()));        objectMapper.setFilterProvider(filters);        objectMapper.registerModules(new ProblemModule().withStackTraces(false), new ConstraintViolationProblemModule());        return objectMapper;    &#125;</code></pre><ul><li>Customize 1 problem</li></ul><pre><code class="java">public class CallHttpProblem extends AbstractThrowableProblem &#123;    public CallHttpProblem(SourceType sourceType, FeignException exception) &#123;        super(null,                &quot;Call internal api has problem&quot;,                Status.FAILED_DEPENDENCY,                String.format(&quot;Call api to &#39;%s&#39; has problem&quot;, &quot;xxx&quot;),                null, null,                Stream.of(new String[][]&#123;                        &#123;&quot;dependency_status&quot;, String.valueOf(exception.status())&#125;,                        &#123;&quot;dependency_uri&quot;, Util.getPath(exception.request().url())&#125;,                &#125;).collect(Collectors.toMap(data -&gt; data[0], data -&gt; data[1]))        );    &#125;&#125;</code></pre><ul><li>Tại các class <code>Entity</code>, khai báo thêm 2 method sau, để khi update&#x2F;create xuống db, không phải setter value</li></ul><pre><code class="java">    @PreUpdate    private void preUpdate() &#123;        this.modifiedOn = Util.getVNTime();    &#125;    @PrePersist    private void prePersist() &#123;        if (this.getCreatedOn() == null) this.setCreatedOn(Util.getVNTime());    &#125;</code></pre><ul><li>Khai báo class Exception, kiểu Builder pattern, hạn chế việc mutable</li></ul><pre><code class="java">public class SyncProductException extends RuntimeException &#123;    private static final long serialVersionUID = 1L;    @Getter    private final SyncProductErrorType errorType;    @Getter    private FeignException feignException;    @Getter    private SourceType errorFrom;    public SyncProductException(String message, SyncProductErrorType errorType, FeignException feignException,                                SourceType errorFrom) &#123;        super(message);        this.errorType = errorType;        this.feignException = feignException;        this.errorFrom = errorFrom;    &#125;    public SyncProductException(SyncProductErrorType errorType) &#123;        super(errorType.getExceptionMessage());        this.errorType = errorType;    &#125;    public static Builder builder() &#123;        return new Builder();    &#125;    public static class Builder &#123;        private SyncProductErrorType errorType;        private String message;        private FeignException feignException;        private SourceType errorFrom;        public Builder errorType(SyncProductErrorType errorType) &#123;            this.errorType = errorType;            return this;        &#125;        public Builder message(String message) &#123;            this.message = message;            return this;        &#125;        public Builder feignException(FeignException feignException, SourceType errorFrom) &#123;            this.errorFrom = errorFrom;            this.feignException = feignException;            return this;        &#125;        public SyncProductException build() &#123;            if (this.errorType == null) throw new IllegalArgumentException(&quot;errorType not null&quot;);            if (this.message == null) this.message = this.errorType.getExceptionMessage();            return new SyncProductException(this.message, this.errorType, this.feignException, this.errorFrom);        &#125;    &#125;&#125;</code></pre><ul><li>class <code>SyncCoordinator</code> , chuyển toàn bộ các <code>handler</code> thực chất là các scheduler riêng rẽ, scan db để lấy các request<br>sync, sang duy nhất 1 scheduler, scheduler <code>coordinator</code> này sẽ quét toàn bộ db, và quản lý chúng trên local memmory,<br>để điều phối việc sync.<ul><li>Nhược điểm hiện tại: đang dùng localmem &#x3D;&gt; chưa thể scale được</li><li>Luôn phải suy nghĩ, và tính toán sao cho trong trường hợp SyncCoordinator này có lỗi, do việc quản lý mem, và điều<br>phối không tốt, thì cũng không làm sai lệch dữ liệu đồng bộ.</li></ul></li><li>class <code>ReTriggerTask</code> dùng để trigger, restart lại các job bị lỗi, hoặc bị treo.</li></ul><pre><code class="java">@Component@CommonsLog(topic = &quot;topic.operate&quot;)public class ReTriggerTask implements CommandLineRunner &#123;    public static final int FAIL_THRESHOLD = 10;    @Autowired    private SyncRequestJPARepository syncRequestJPARepository;    @Override    public void run(String... args) throws Exception &#123;        log.info(&quot;---Restart all handling request &quot;);        syncRequestJPARepository.restartAllHandling(ProgressStatus.processing);    &#125;    @Scheduled(cron = &quot;1 1 * ? * *&quot;)    public void restartHangingRequest() &#123;        int counter = syncRequestJPARepository.countAllByProgressStatusAndPrevFireTimeBefore(ProgressStatus.processing, Util.getVNTime());        log.info(&quot;---Restart all handling request - via cron schedule - total = &quot; + counter);        var now = Util.getVNTime();        Calendar c = Calendar.getInstance();        c.setTime(now);        c.add(Calendar.HOUR, -1);        syncRequestJPARepository.restartAllHandling(c.getTime());    &#125;    @Scheduled(cron = &quot;1 9 * ? * *&quot;)    public void restartFailRequest() &#123;        int counter = syncRequestJPARepository.countAllByProgressStatusAndFireCounterIsLessThanEqual(ProgressStatus.fail, FAIL_THRESHOLD);        log.info(&quot;---Restart all fail request - via cron schedule - total = &quot; + counter);        var now = Util.getVNTime();        Calendar c = Calendar.getInstance();        c.setTime(now);        c.add(Calendar.HOUR, -1);        syncRequestJPARepository.restartAllFail(FAIL_THRESHOLD);    &#125;&#125;</code></pre><ul><li><code>ExceptionHandlerAdvice</code> - advice controller</li></ul><pre><code class="java">@ControllerAdvice(basePackages = &quot;service.controller.frontend.api&quot;)public class ExceptionHandlerAdvice implements ProblemHandling &#123;    private final StoreJPARepository storeJPARepository;    public ExceptionHandlerAdvice(StoreJPARepository storeJPARepository) &#123;        this.storeJPARepository = storeJPARepository;    &#125;    @ModelAttribute(value = &quot;store&quot;, binding = false)    public Store addStore(@RequestHeader(value = &quot;X-App-StoreId&quot;, required = false) Integer storeIdHeader) &#123;        if (NumberUtils.isBlank(storeIdHeader)) &#123;            throw Problem.builder()                    .withTitle(Status.BAD_REQUEST.getReasonPhrase())                    .withStatus(Status.BAD_REQUEST)                    .withDetail(&quot;Header &#39;X-App-StoreId&#39; is missing or invalid&quot;)                    .build();        &#125;        val authentication = SecurityContextHolder.getContext().getAuthentication();        JwtUser u = (JwtUser) authentication.getPrincipal();        if (storeIdHeader != u.getId()) &#123;            throw Problem.builder()                    .withTitle(Status.FORBIDDEN.getReasonPhrase())                    .withStatus(Status.FORBIDDEN)                    .withDetail(&quot;This token have not permission for this store!&quot;)                    .build();        &#125;        var storeOp = storeJPARepository.findById(storeIdHeader);        if (storeOp.isEmpty()) &#123;            throw Problem.builder()                    .withTitle(Status.NOT_FOUND.getReasonPhrase())                    .withStatus(Status.NOT_FOUND)                    .withDetail(String.format(&quot;Store %s not found&quot;, storeIdHeader))                    .build();        &#125;        return storeOp.get();    &#125;    @ExceptionHandler(AccessDeniedException.class)    public void handlerError() &#123;        throw Problem.builder()                .withTitle(Status.FORBIDDEN.getReasonPhrase())                .withStatus(Status.FORBIDDEN)                .withDetail(&quot;Denied&quot;)                .build();    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> stories </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sync </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nhật ký xây dựng app Backup &amp; Restore</title>
      <link href="/2020/12/Stories/App_Backup_Restore/"/>
      <url>/2020/12/Stories/App_Backup_Restore/</url>
      
        <content type="html"><![CDATA[<h1 id="Bai-toan"><a href="#Bai-toan" class="headerlink" title="Bài toán"></a>Bài toán</h1><ul><li>Xây dựng ứng dụng phục vụ việc sao lưu &amp; khôi phục dữ liệu.</li><li>Đối tượng cần sao lưu: dữ liệu của khách hàng được chứa trên hệ thống X. Việc lấy dữ liệu (backup) &amp; chỉnh sửa dữ<br>liệu (restore) được thực hiện qua Restful API</li><li>Người thực hiện thao tác ứng dụng: khách hàng của hệ thống X</li><li>Format dữ liệu khi lấy từ hệ thống X về: <code>json</code></li><li>Mỗi khách hàng chủ động lịch định kỳ <code>sao lưu </code> và tự <code>khôi phục</code> khi khách hàng muốn</li></ul><h2 id="Mot-so-dac-thu-ky-thuat"><a href="#Mot-so-dac-thu-ky-thuat" class="headerlink" title="Một số đặc thù kỹ thuật"></a>Một số đặc thù kỹ thuật</h2><ul><li>Authen&#x2F;author tới hệ thống X là <code>oauth</code>. App sao lưu cần xin quyền trước.</li><li>App tự động backup dữ liệu theo lịch cấu hình của từng khách hàng. Hoặc thực hiện backup dữ liệu ngay lập tức khi<br>khách hàng request.</li></ul><h1 id="Thiet-ke"><a href="#Thiet-ke" class="headerlink" title="Thiết kế"></a>Thiết kế</h1><h2 id="10-06-2020"><a href="#10-06-2020" class="headerlink" title="10&#x2F;06&#x2F;2020"></a>10&#x2F;06&#x2F;2020</h2><ul><li>Sử dụng <code>OpenFeign</code> (FeignClient) để wrap việc call http api từ App tới hệ thống X. Vì api get data tới hệ thống X có<br>giới hạn số record trả về trên 1 request. mình quyết định viết ra 1 method parallel gồm 2 bước chính:<ul><li>call api lần thứ 1 để lấy ra total record &#x3D;&gt; tính toán ra tổng số M request api cần call để có thể lấy được hết dữ<br>liệu.</li><li>Sử dụng <code>CompletableFuture</code> cho chạy song song M request cùng lúc</li></ul></li></ul><pre><code class="java">//code examplepublic ProductList getAllProduct(int storeId, String createdOnMax) throws ExecutionException, InterruptedException &#123;        final var totalProductResult = productClient.getTotalCount(storeId, createdOnMax, authen);        final var totalProduct = totalProductResult.getCount();        final int totalPage = (int) Math.ceil((double) totalProduct / Constants.BACKUP_CLIENT_LIMIT);        List&lt;CompletableFuture&lt;ProductList&gt;&gt; listAsync = new ArrayList&lt;&gt;();        for (int i = 1; i &lt;= totalPage; i++) &#123;            listAsync.add(clientAsyncGetProduct(storeId, i, createdOnMax));        &#125;        CompletableFuture&lt;Void&gt; allAsync = CompletableFuture                .allOf(listAsync.toArray(new CompletableFuture[listAsync.size()]));        CompletableFuture&lt;List&lt;ProductList&gt;&gt; allClientAsync = allAsync.thenApply(v -&gt; &#123;            return listAsync.stream().map(CompletableFuture::join)                    .collect(Collectors.toList());        &#125;).exceptionally(ex -&gt; &#123;            try &#123;                throw new Exception(ex.getMessage());            &#125; catch (Exception e) &#123;                e.printStackTrace();            &#125;            return null;        &#125;);        var resultList = allClientAsync.get();        var result = new ProductList();        resultList.forEach(result::addAll);        return result;    &#125;</code></pre><ul><li>Queue: sử dụng Kafka (sau này mình đã chuyển sang RabbitMQ). Lý do ban đầu chọn Kafka đơn giản chỉ vì t nghĩ cần 1 hệ<br>thống message queue để chạy job backup&#x2F;restore. Nên là thằng nào cũng được. Nhưng vì sau này hiểu sâu hơn về cơ chế<br>hoạt động Kafka, mình nhận ra t việc lựa chọn nó là không phù hợp. Về cơ bản tư tưởng mình sử dụng 3 queue:<ul><li>queue cho việc backup auto</li><li>queue cho việc backup manual</li><li>queue cho việc restore</li></ul></li><li>Backup mình tách ra làm 2 queue, đơn giản chỉ là muốn các queue mà người dùng request sẽ có 1 kênh riêng, có ưu tiên<br>hơn. Sau này chuyển sang rabbitMQ thì rabbitmq có hỗ trợ priority, tuy nhiên do kiến trúc code cũ, nên mình vẫn giữ<br>việc độc lập này.</li><li>Nơi lưu trữ dữ liệu backup: AWS S3. Ban đầu mình có suy nghĩ tới việc backup dữ liệu sang 1 database rdbms khác. Tuy<br>nhiên ban đầu bài toán là muốn dữ liệu được lưu trữ ở 1 nơi nào đó <code>an toàn </code> hơn. Đã gọi là backup&#x2F;restore thì nó<br>được dùng cho case nguy hiểm rồi. Với cả mình cũng chưa thấy được lợi ích gì nổi bật hơn giữa việc lựa chọn S3 hay<br>Rdbms. Mình dự định file backup sẽ là 1 file data gì đó (csv, excel, json…), nếu sau này kiểu khách hàng cần<br>download file thì cũng tiện hơn. Cuối cùng mình chốt sử dụng file csv. Một phần cũng vì ban đầu tìm hiểu thấy S3 hỗ<br>trợ query select trực tiếp từ file csv. (cho tới thời điểm hiện tại thì cũng chưa khai thác được tính năng này)<ul><li>Khi sử dụng S3, mình cũng áp dụng parallel giống như <code>OpenFeign</code> bên trên.</li><li>S3 không hỗ trợ việc copy object 1 cách <code>bulk/batch</code>. Nên phải request từng object 1. Và thêm 1 cái dở là nếu<br>request quá nhanh thì sẽ bị AWS chặn lại. &#x3D;&gt; hơi tù.</li></ul></li><li>Quartz: để phục vụ cho mục đích là mỗi khách hàng tự cấu hình lịch backup data của họ, mình sử dụng Quartz. Nếu như mà<br>tất cả khách hàng đều có lịch backup giống nhau, mình sẽ sử dụng Spring schedule, hoặc cronjob, nó sẽ đơn giản hơn rất<br>nhiều. Để implement quartz phức tạp hơn, nó có database riêng. Thậm chí với bài toán hệ thống lớn, có thể sẽ phải<br>thiết kế 1 cluster riêng cho nó với nhiều node. Quartz này mình sử dụng loại schedule là <code>cron Expression</code>.</li><li>OpenCSV: mỗi file backup sẽ được lưu trữ trong file csv. Vì sợ dữ liệu sẽ bị lỗi unicode, nên mình quyết định encode<br>base64 thông tin json, trước khi lưu vào csv.</li></ul><pre><code class="java">@Getter@Setter@AllArgsConstructorpublic class BackupCsv &#123;    @CsvBindByName(column = &quot;NO&quot;)    public int rowNo;    @CsvBindByName(column = &quot;OBJECT_ID&quot;)    public int objectId;    @CsvBindByName(column = &quot;DATA&quot;)    public String data;    public BackupCsv() &#123;    &#125;&#125;</code></pre><h3 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h3><ul><li><code>BackupLogs</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/BackupLogs.png" alt="BackupLogs"></li><li><code>BackupLogDetails</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/BackupLogDetails.JPG" alt="BackupLogDetails"><br>Mình tách độc lập ra 2 bảng. BackupLogs lưu trữ thông tin tổng của 1 <code>request</code> backup. Nó chứa thông tin tên của bản<br>backup, nguồn gốc, status tổng… Trong khi bảng <code>detail</code>, sẽ chứa chi tiết từng hạng mục backup riêng (product,<br>collection, customer…vvv). Queue được thiết kế sử dụng cho layer detail này.(tới thời điểm hiện tại, mình nghĩ có<br>thể queue thiết kế ở layer BackupLog cũng được)</li><li><code>Media</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/Media.JPG" alt="Media"></li><li><code>MediaDependencyLogs</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/MediaDependencyLogs.JPG" alt="MediaDependencyLogs"><br>Có 1 vấn đề đặt ra là dữ liệu khi được lưu trữ trên CloudStorage, thì mình nên giảm tỉ lệ việc duplicated dữ liệu. Ví<br>dụ khi backup data product, và product P111, có chứa thông tin link ảnh cdn media. Vậy ngày hôm nay khi backup dữ<br>liệu, ngoài việc backup các thông tin cơ bản trong json, thì mình cũng cần backup thêm cả dữ liệu file media kia nữa.<br>Vậy nếu ngày mai backup data tiếp, việc lại thực hiện backup file media 1 lần nữa là không cần thiết. Vì file cdn ảnh<br>không hề thay đổi. Việc duplicated dữ liệu file media này sẽ tốn thêm rất nhiều cost storage. Vì thế nên 2 table:<br>Media, MediaDependencyLogs ra đời, mục đích để lưu lại các metadata này.</li><li><code>RestoreLogs</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/RestoreLog.JPG" alt="RestoreLogs"></li><li><code>RestoreLogDetails</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/RestoreLogDetails.JPG" alt="RestoreLogDetails"></li><li><code>RestoreErrorLogs</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/RestoreErrorLogs.JPG" alt="RestoreErrorLogs"><br>Tương tự như chiều Backup, thì chiều Restore các table cũng được thiết kế tương ứng. Việc restore này tương đối nguy<br>hiểm, vì có thể sẽ làm sai lệch dữ liệu của khách hàng không như ý muốn. Nên với mỗi lệnh Restore, sẽ cần thực hiện<br>lệnh backup toàn bộ data trước đó. (column <code>BackupLogIdDisasterHelper</code>). Và có thêm table <code>RestoreErrorLogs</code> để rõ<br>ràng hơn trong việc xác định lỗi.</li></ul><h2 id="16-06-2020"><a href="#16-06-2020" class="headerlink" title="16&#x2F;06&#x2F;2020"></a>16&#x2F;06&#x2F;2020</h2><ul><li>App gồm 2 phần, frontend giao diện, và service. 2 phần này chạy độc lập với nhau. Cũng không có lý do gì đặc biệt cho<br>lựa chọn này. Frontend sử dụng reactjs. Service dùng java. 2 thành phần này được container độc lập. Vậy nên bài toán<br>cần 1 cơ chế authen từ reactjs (client) tới service. &#x3D;&gt; Quyết định lựa chọn sử dụng JWT</li><li>Vậy việc cấp token jwt diễn ra như thế nào?<ul><li>Đầu tiên user cần login vào hệ thống X. Từ hệ thống X user truy cập vào app. Giữa hệ thống X và app có 1 cơ chế<br>authen riêng. là <code>oauth</code>. 2 bên đã trao đổi vs nhau 1 secretkey trước đấy. Khi hệ thống X redirect về app sẽ<br>truyền theo thông tin, kèm hmac. App sẽ dùng secretkey để validate thông tin. Và redirect về reactjs kèm theo<br>token jwt.</li><li>Reacjts dùng jwt lưu vào session&#x2F;local storage của browser, và sử dụng nó để call tới service lấy thông tin.</li></ul></li></ul><h2 id="01-07-2020"><a href="#01-07-2020" class="headerlink" title="01&#x2F;07&#x2F;2020"></a>01&#x2F;07&#x2F;2020</h2><ul><li>Bộ Garbage Cleaner ra đời. Lấy ý tưởng từ bộ GC của java. Mình đặt tên class cũng là <code>GarbageCleanerStrategy</code>. Bộ GC<br>của mình này có nhiệm vụ là dọn dẹp dữ liệu trên hệ thống cloud storage (s3). Như đã đề cập, mình sử dụng 2<br>table <code>Media</code> và <code>MediaDependencyLogs</code> để lưu metadata các dữ liệu file media trên json. Vậy bài toán là khi file<br>media không còn được <code>reference</code> tới bất kỳ bản backupLog nào nữa. Thì mình cần phải xóa chúng đi trên S3.<ul><li>Với các file backup với origin&#x3D;AUTO, MANUAL sẽ có “chiến lược” khác với file backup có origin&#x3D;BEFORE_RESTORE</li><li>cần đảm bảo transaction tuyệt đối ở đây. Chỉ khi nào file media trên cloudstorage bị xóa thành công, thì mới thực<br>hiện update db tương ứng.</li><li>Bộ GC này chạy async, và độc lập. Được trigger khi có 1 backupLog bị xóa.</li></ul></li></ul><h2 id="07-07-2020"><a href="#07-07-2020" class="headerlink" title="07&#x2F;07&#x2F;2020"></a>07&#x2F;07&#x2F;2020</h2><ul><li>Vấn đề đặt ra khi user trên hệ thống X không còn <code>active</code> nữa. Nhưng job backup của user này theo quartz vẫn chạy định<br>kỳ, dẫn tới việc backup dữ liệu là vô nghĩa. &#x3D;&gt; cần có cơ chế để hủy job backup. Rất may là hệ thống X có hỗ trợ sẵn<br>điều này. Chỉ cần đăng ký webhook với hệ thống X. và lắng nghe chúng. Khi có webhook từ hệ thống X gửi tới App và báo<br>user đã bị inactive, thì cấu hình cron job trên quartz của user đó sẽ bị hủy bỏ.<ul><li>Hiện tại thì chưa có cơ chế xóa toàn bộ dữ liệu liên quan nếu user inactive. (chưa xác định được thực sự khi nào<br>cần xóa)</li><li>việc lắng nghe webhook này là 1 giải pháp mình thấy có độ tin tưởng không được tuyệt đối. Về lâu dài, sẽ dẫn tới<br>sự thiếu nhất quán dữ liệu giữa App và X</li><li>bổ sung thêm việc handler lỗi khi app call API tới hệ thống X mà gặp lỗi. Ví dụ lỗi 401, thì cũng sẽ trigger việc<br>hủy chạy quartz.</li></ul></li></ul><h2 id="12-07-2020"><a href="#12-07-2020" class="headerlink" title="12&#x2F;07&#x2F;2020"></a>12&#x2F;07&#x2F;2020</h2><ul><li>Implement Redis: phía client cần biết được progress (%) của tiến trình backup&#x2F;restore. Mình quyết định tích hợp Redis<br>cho feature này. Với mỗi task backup&#x2F;restore dữ liệu. Mình tự đặt ra quy ước, khi hoàn thành xong 1 phần nào đó, thì<br>sẽ append kết quả vào redis. Đồng thời viết ra 1 api controller, để client call vào đó lấy progress statistic. Khi<br>client call vào api, thì tầng service sẽ get value từ redis và trả về cho client.<ul><li>Client lấy thông tin progress này bằng cách interval call api. Điều này làm mình không hài lòng lắm. Mình muốn<br>implement socket. Nhưng tới h vẫn chưa có effort làm.</li><li>Việc get value từ redis sẽ giúp giảm tải tới hệ thống hơn</li></ul></li></ul><h2 id="13-07-2020"><a href="#13-07-2020" class="headerlink" title="13&#x2F;07&#x2F;2020"></a>13&#x2F;07&#x2F;2020</h2><ul><li>Big refactor: chuyển từ Kafka sang RabbitMQ. Sau quá trình tìm hiểu và thực tế sử dụng, nhận thấy kafka không hợp với<br>case của mình.<ul><li>Khi các job chạy bị lâu tốn nhiều thời gian, kafka client bắt đầu throw ra lỗi, hoặc log báo là message được<br>rebalance sang consumer khác. Điều này thực sự không phải là vấn đề chính của quyết định chuyển sang RabbitMQ.<br>Nhưng để handler nó, mình thấy tốn nhiều công sức tìm hiểu, và cuối cùng vẫn không tự tin rằng đã control được nó.<br>Nhưng về lý thuyết thì mình chỉ cần commit msg cho kafka ngay khi nhận được msg thôi. Chứ không cần phải đợi chạy<br>xong job mới commit msg. Mình không có ý định sử dụng queue retry gì nhiều hơn ở đây. Đã thất bại việc chạy job là<br>thất bại, Muốn retry thì con người phải manual lại nó. Mình cũng đã có các table để log lại các job rồi. Nên không<br>mong đợi gì nhiều hơn ở Kafka nữa</li><li>Khi chạy job, việc sử dụng kafka bị hạn chế việc scale. Giả sử mình sử dụng 10 partition cho 1 topic. Vậy thì mình<br>chỉ có thể scale tối đa 10 consumer cùng lúc. Đây là lý do chính</li></ul></li></ul><h2 id="03-08-2020"><a href="#03-08-2020" class="headerlink" title="03&#x2F;08&#x2F;2020"></a>03&#x2F;08&#x2F;2020</h2><ul><li>Leaky Bucket: việc call api từ App tới hệ thống X bị hạn chế. Hệ thống X chỉ cho phép call N api trong khoảng X giây.<br>&#x3D;&gt; Nếu việc backup&#x2F;restore mà mình cho call api paralell quá nhanh, thì sẽ gặp lỗi ở đây.<ul><li>Sử dụng thư việc Xsync để intercepter và synchonize việc sleep request.</li></ul><pre><code class="xml">        &lt;dependency&gt;            &lt;groupId&gt;com.antkorwin&lt;/groupId&gt;            &lt;artifactId&gt;xsync&lt;/artifactId&gt;            &lt;version&gt;1.1&lt;/version&gt;        &lt;/dependency&gt;</code></pre><ul><li>Đây là 1 giải pháp không triệt để. Hiện tại app chỉ chạy vs 1 instance. Nhưng nếu scale lên, thì cần có 1 middle<br>trung gian cho việc này. Ví dụ 1 proxy riêng. Hoặc là dùng redis để counter.</li><li>Cũng may do việc sử dụng OpenFeign, nên interceptor tương đối nhẹ nhàng hơn.</li></ul><pre><code class="java"> public void timeWaitLimit(String storeAlias) &#123;    xSync.execute(storeAlias, () -&gt; &#123;        LeakyBucketModel model = BucketLeakyStatic.get(storeAlias);        if (model == null) &#123;            BucketLeakyStatic.put(storeAlias, new LeakyBucketModel(bucketSize));        &#125; else &#123;            if (model.getAllowCounter() &lt;= 0) &#123;                try &#123;                    TimeUnit.MILLISECONDS.sleep(1200);                &#125; catch (InterruptedException e) &#123;                    e.printStackTrace();                &#125;                int diffSecond = (int) TimeUnit.SECONDS.convert(Util.getUTC().getTime() - model.getModifiedTime(), TimeUnit.MILLISECONDS);                int reNewCounter = Math.min(diffSecond * drainRate, bucketSize);                model.reInitCounter(reNewCounter);            &#125;            model.decrementCounter();        &#125;    &#125;);&#125;</code></pre></li></ul><h2 id="31-08-2020-Diff"><a href="#31-08-2020-Diff" class="headerlink" title="31&#x2F;08&#x2F;2020 - Diff"></a>31&#x2F;08&#x2F;2020 - Diff</h2><ul><li>Check Diff: Một nghiệp vụ lớn có sự thay đổi. Trước đây thì khi restore, flow là chỉ cần user lựa chọn bản backup và<br>submit. Hệ thống sẽ tự động restore. Và báo kết quả cuối cho user. Giờ thay đổi, sẽ cần thêm 1 bước user confirm nữa.<br>User lựa chọn bản backuplog -&gt; app tính toán ra dữ liệu hiện tại có thay đổi gì so với dữ liệu từ bản backup log, hiên<br>thị sự thay đổi cho user &#x3D;&gt; user tích chọn từng đối tượng có sự thay đổi và submit chỉ restore chúng &#x3D;&gt; app thực hiện<br>restore và trả về kết quả</li><li>Thiết kế lại database<ul><li><code>DiffRequests</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/DiffRequests.JPG" alt="DiffRequests"></li><li><code>DiffRequestDetails</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/DiffRequestDetails.JPG" alt="DiffRequestDetails"><br>Tương tự như Backup và Restore, Diff Request cũng được thiết kế tương ứng.</li><li><code>Diffs</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/stories/backup_restore/Diff.JPG" alt="Diffs"><br>Bảng Diffs log lại chi tiết sự thay đổi của bản backup với dữ liệu hiện tại. Chi tiết tới từng đối tượng backup.</li></ul></li><li>Check Diff cũng có queue riêng để thực hiện</li><li>Bắt đầu xuất hiện sự phức tạp trong follow. Các job đều được trigger bởi queue. Nhưng lại cần đảm bảo sao cho thứ tự<br>backup-diff-restore được đúng follow.<ul><li>Với case trước khi restore lại cần backup trước 1 bản, lại càng phức tạp</li><li>Một sự phối hợp giữa publisher&#x2F;subscriber và thông tin trong column database để giải quyết việc trigger mọi thứ<br>được đúng flow</li></ul></li><li>Sử dụng <code>org.javers.core.diff</code> để check</li></ul><h2 id="14-09-2020"><a href="#14-09-2020" class="headerlink" title="14&#x2F;09&#x2F;2020"></a>14&#x2F;09&#x2F;2020</h2><ul><li>Compress data: dữ liệu được backup trong file csv và upload lên S3. Ban đầu được đánh giá là dữ liệu này chiếm ít tài<br>nguyên. Vì chủ yếu là plaintext. Dữ liệu tốn chủ yếu ở media file. Nhưng khi test với data trên live production, 1<br>file backup có thể hơn 100MB. Nhu cầu nén file ra đời.<ul><li>File sau khi được call api collect về, sẽ được nén gzip lại.</li><li>Toàn bộ việc nén file này thực hiện trên local mem, mà không đẩy ra hard disk. Vì sau khi test và đánh giá thấy<br>việc đẩy ra hard disk là không cần thiết. Việc thực hiện trực tiếp trên local memory vẫn đáp ứng được.</li><li>Kết quả việc nén gzip thực sự hiệu quả, file 100MB được nén xuống chỉ còn ~7MB.</li><li>Vì compress&#x2F;decompress đều được thực hiện trên local memmory, nên không có sự chênh lệch tốc độ cho phần này<br>nhiều. Thậm chí còn nhanh hơn, do time cho việc upload&#x2F;download file nhanh hơn.</li></ul></li></ul><pre><code class="java">public static byte[] gzipCompress(byte[] input) &#123;        try &#123;            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();            InputStream inputStream = new ByteArrayInputStream(input);            GZIPOutputStream gzipOS = new GZIPOutputStream(outputStream);            byte[] buffer = new byte[1024];            int len;            while ((len = inputStream.read(buffer)) != -1) &#123;                gzipOS.write(buffer, 0, len);            &#125;            gzipOS.close();            inputStream.close();            outputStream.close();            return outputStream.toByteArray();        &#125; catch (Exception ex) &#123;            throw new RuntimeException(ex);        &#125;    &#125;    public static byte[] gzipDecompress(byte[] input) &#123;        try &#123;            GZIPInputStream gis = new GZIPInputStream(new ByteArrayInputStream(input));            ByteArrayOutputStream fos = new ByteArrayOutputStream();            byte[] buffer = new byte[1024];            int len;            while ((len = gis.read(buffer)) != -1) &#123;                fos.write(buffer, 0, len);            &#125;            //close resources            fos.close();            gis.close();            return fos.toByteArray();        &#125; catch (Exception e) &#123;            throw new RuntimeException(e);        &#125;    &#125;    public static byte[] convertS3ObjectToBytes(S3Object s3Object) &#123;        byte[] data = null;        try &#123;            S3ObjectInputStream stream = s3Object.getObjectContent();            BufferedInputStream bis = new BufferedInputStream(stream);            ByteArrayOutputStream out = new ByteArrayOutputStream();            byte[] buffer = new byte[1024];            while (true) &#123;                int r = bis.read(buffer);                if (r == -1) break;                out.write(buffer, 0, r);            &#125;            out.flush();            return out.toByteArray();        &#125; catch (AmazonS3Exception ex) &#123;            if (!StringUtils.equals(&quot;NoSuchKey&quot;, ex.getErrorCode())) &#123;                ex.printStackTrace();            &#125;        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;        return data;    &#125;</code></pre><h2 id="08-10-2020"><a href="#08-10-2020" class="headerlink" title="08&#x2F;10&#x2F;2020"></a>08&#x2F;10&#x2F;2020</h2><ul><li>Confirm when remove backup: Thêm nghiệp vụ mới. Vì hệ thống X không control được user có thể truy cập được vào app.<br>Nguy cơ việc 2 user cùng có thể xem được bản backup, nhưng chỉ user owner mới có thể thực hiện xóa. Còn user được ủy<br>quyền kia chỉ có thể xem. &#x3D;&gt; phát triển thêm cơ chế khi user thực hiện xóa bản backup, thì service của app sẽ gửi 1<br>email chứa link xóa tới owner. Owner click vào link confirm thì bản backup đó mới được xóa<ul><li>Link xóa thực chất chỉ là 1 api remove, có thêm parameter là token.</li><li>Token sử dụng jwt</li></ul><pre><code class="java">@Overridepublic String genTokenRemoveBackup(int storeId, List&lt;Integer&gt; backupLogIds) &#123;    final Date createdDate = clock.now();    final Date expirationDate = new Date(createdDate.getTime() + 86400000);  // 24h    Map&lt;String, Object&gt; claims = new HashMap&lt;&gt;();    claims.put(&quot;storeId&quot;, storeId);    claims.put(&quot;backupLogIds&quot;, backupLogIds);    return Jwts.builder()            .setClaims(claims)            .setSubject(String.valueOf(storeId))            .setIssuedAt(createdDate)            .setExpiration(expirationDate)            .signWith(SignatureAlgorithm.HS256, jwtProperty.getSecret())            .compact();&#125;@Overridepublic boolean validTokenRemoveBackup(int storeId, List&lt;Integer&gt; backupLogIds, String token) &#123;    try &#123;        var claims = Jwts.parser()                .setSigningKey(jwtProperty.getSecret())                .parseClaimsJws(token)                .getBody();        final Date expirationClaim = claims.getExpiration();        if (expirationClaim.before(clock.now())) return false;        int storeIdClaim = (Integer) claims.get(&quot;storeId&quot;);        List&lt;Integer&gt; backupLogIdsClaim = (List&lt;Integer&gt;) claims.get(&quot;backupLogIds&quot;);        if (!backupLogIds.containsAll(backupLogIdsClaim) || storeIdClaim != storeId) return false;        return true;    &#125; catch (Exception ex) &#123;        return false;    &#125;&#125;</code></pre></li><li>Việc gửi email sử dụng AWS SES. (việc tích hợp này tương đối đơn giản)</li></ul>]]></content>
      
      
      <categories>
          
          <category> stories </category>
          
      </categories>
      
      
        <tags>
            
            <tag> backup restore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JPA - Hibernate - Persistence Note</title>
      <link href="/2020/12/Database/JPA_Persistance_note/"/>
      <url>/2020/12/Database/JPA_Persistance_note/</url>
      
        <content type="html"><![CDATA[<h1 id="Identifier-Use-Sequence-vs-Table-Generator"><a href="#Identifier-Use-Sequence-vs-Table-Generator" class="headerlink" title="Identifier: Use Sequence vs Table Generator"></a>Identifier: Use Sequence vs Table Generator</h1><ul><li><p><strong>Table Generator:</strong> Declare the ID column as IDENTITY (SQL Server) or AUTO INCREMENT (MySQL), and in Hibernate, declare <code>@GeneratedValue</code>.</p></li><li><p><strong>Sequence:</strong> Create a separate, independent sequence not tied to any table.</p></li><li><p><strong>Table Generator:</strong> Bound by the declared table, whereas a sequence is not. Using a sequence results in better performance (faster inserts, especially noticeable during concurrent inserts) because the table generator incurs more row-lock costs.</p></li><li><p><strong>Sequence:</strong> Can be reset, set to a maximum value, and shared across multiple tables.</p></li><li><p><strong>Sequence:</strong> A non-transactional object (operates outside the transaction context).</p></li><li><p><strong>Recommendation:</strong> Use Sequence for higher performance.</p></li></ul><h1 id="FetchType-LAZY"><a href="#FetchType-LAZY" class="headerlink" title="FetchType LAZY"></a>FetchType LAZY</h1><ul><li>For <code>@OneToMany</code> and <code>@ManyToMany</code> associations, Hibernate uses its own collection proxy implementations (e.g., <code>PersistentBag</code>, <code>PersistentList</code>, <code>PersistentSet</code>, <code>PersistentMap</code>) which can execute the lazy loading SQL statement on demand.</li><li><strong>N+1 Query Problem:</strong> If you have a collection entity, when iterating over them with a <code>forEach</code> loop, N+1 queries can occur. This affects both lazy and eager loading. To mitigate this issue, use <code>join</code>.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/Fix_N1_Query.JPG" alt="Fix N+1 Query"></li></ul><h2 id="LazyInitializationException"><a href="#LazyInitializationException" class="headerlink" title="LazyInitializationException"></a>LazyInitializationException</h2><ul><li>This exception occurs when the Persistence Context is closed (the context of the query entity is closed), and you attempt to fetch lazy-loaded data from the ORM (e.g., <code>OneToMany</code>). For instance, creating a method without <code>@Transactional</code>, fetching the main entity and its ORM in two separate transactions can throw this exception. (Conversely, if declared within the same transaction, it won’t throw an exception). This issue complicates business code integration because lazy loading must occur within the same service and transaction, potentially leading to long-running transactions.</li><li>Two ways to resolve this issue, though both are not recommended:<ul><li>Open Session in View Anti-Pattern</li><li>Temporary Session Lazy Loading Anti-Pattern: Configure with Spring by setting the property <code>hibernate.enable_lazy_load_no_trans=true</code>. This might increase DB connections and potential N+1 issues as it always starts new transactions and forces transaction logs to flush.</li></ul></li></ul><h1 id="Batching"><a href="#Batching" class="headerlink" title="Batching"></a>Batching</h1><ul><li>Hibernate uses <code>PreparedStatement</code> for automatic insert&#x2F;update&#x2F;delete DML operations.</li><li>By default, Hibernate is not configured for JDBC batch mode, leading to N queries for inserting a list of N entities:</li></ul><pre><code class="java">for (int i = 0; i &lt; 3; i++) &#123;    entityManager.persist(new Post(String.format(&quot;Post no. %d&quot;, i + 1)));&#125;// Hibernate log outputINSERT INTO post (title, id) VALUES (Post no. 1, 1)INSERT INTO post (title, id) VALUES (Post no. 2, 2)INSERT INTO post (title, id) VALUES (Post no. 3, 3)</code></pre><p>To optimize batching, configure <code>hibernate.jdbc.batch_size=5</code> (note: this is a global configuration). The Hibernate query log will then show:</p><pre><code class="sql">Query: [&quot;INSERT INTO post (title, id) VALUES (?, ?)&quot;],Params: [(&#39;Post no. 1&#39;, 1), (&#39;Post no. 2&#39;, 2), (&#39;Post no. 3&#39;, 3)]</code></pre><ul><li>Batch mode is disabled if the entity declares <code>@Id</code> (Persistence Context needs to manage the entity).</li><li>Batch mode is also disabled if ORM relationships (<code>@OneToMany</code>, <code>@ManyToOne</code>) are declared. To fix this, configure <code>hibernate.order_inserts=true</code>. The resulting query will be:</li></ul><pre><code class="sql">INSERT INTO post (title, id)VALUES (Post no. 0, 1), (Post no. 1, 3), (Post no. 2, 5)INSERT INTO post_comment (post_id, review, id)VALUES (1, Good, 2), (3, Good, 4), (5, Good, 6)</code></pre><ul><li>Similarly, configure <code>hibernate.order_updates=true</code> for updates.</li><li>With entities using <code>@Version</code> (to avoid optimistic locking), there is a risk of <code>StaleObjectStateException</code>. Adjust the configuration with <code>hibernate.jdbc.batch_versioned_data</code>.</li></ul><h1 id="Prevent-Optimistic-Lock"><a href="#Prevent-Optimistic-Lock" class="headerlink" title="Prevent Optimistic Lock"></a>Prevent Optimistic Lock</h1><ul><li>Split frequently locked tables into multiple tables to reduce the query rate on the locking column.</li><li>Use <code>@OptimisticLocking</code> on the entity. (Note: With <code>OptimisticLockType.ALL</code> and <code>OptimisticLockType.DIRTY</code>, <code>@DynamicUpdate</code> must be used as well).</li></ul><h1 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h1><p><em>TODO</em></p><h2 id="Pessimistic-Locking-vs-Optimistic-Locking"><a href="#Pessimistic-Locking-vs-Optimistic-Locking" class="headerlink" title="Pessimistic Locking vs Optimistic Locking"></a>Pessimistic Locking vs Optimistic Locking</h2><ul><li>Pessimistic locking offers more consistency than optimistic locking.</li><li>Optimistic locking uses the <code>version</code> row to detect conflicts during updates.</li><li>Pessimistic locking requires other queries to wait when a resource is locked by another query.</li></ul>]]></content>
      
      
      <categories>
          
          <category> database </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jpa </tag>
            
            <tag> hibernate </tag>
            
            <tag> persistence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transaction - Isolation &amp; Durability</title>
      <link href="/2020/12/Database/Transaction_Isolation/"/>
      <url>/2020/12/Database/Transaction_Isolation/</url>
      
        <content type="html"><![CDATA[<h1 id="Transaction-Isolation"><a href="#Transaction-Isolation" class="headerlink" title="Transaction - Isolation"></a>Transaction - Isolation</h1><h2 id="Concurrency-Control"><a href="#Concurrency-Control" class="headerlink" title="Concurrency Control"></a>Concurrency Control</h2><ul><li><p><strong>Two-Phase Locking (2PL):</strong> Avoids conflicts by using:</p><ul><li><strong>Shared Lock (Read Lock):</strong> Allows multiple transactions to read but not modify a resource.</li><li><strong>Exclusive Lock (Write Lock):</strong> Allows one transaction to modify a resource, preventing others from reading or modifying it.</li><li><strong>Deadlock:</strong> A situation where two or more transactions are waiting for each other to release locks, causing all of them to be blocked.</li></ul></li><li><p><strong>Multi-Version Concurrency Control (MVCC):</strong> Detects conflicts by using multiple versions of data.</p><ul><li><strong>Oracle:</strong> Uses the <code>undo segments</code> <a href="https://docs.oracle.com/database/121/CNCPT/consist.htm#CNCPT221">documentation</a>.</li><li><strong>SQL Server:</strong> Defaults to 2PL for all isolation levels.</li><li><strong>PostgreSQL:</strong> Stores both <code>current rows</code> and <code>previous versions</code> in the actual database. Each row has <code>xmin</code> and <code>xmax</code> columns to control row version. When a row is inserted, the transaction identifier is stored in the <code>xmin</code> column. When a row is deleted or updated, a new row is created with the transaction identifier in the <code>xmax</code> column.</li><li><strong>MySQL:</strong> Uses rollback segments.</li></ul></li></ul><h2 id="Phenomena"><a href="#Phenomena" class="headerlink" title="Phenomena"></a>Phenomena</h2><h3 id="Dirty-Write"><a href="#Dirty-Write" class="headerlink" title="Dirty Write"></a>Dirty Write</h3><ul><li>Occurs when two transactions simultaneously write to the same row, potentially leading to one transaction’s changes being overwritten by the other.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/dirty_write1.JPG" alt="Dirty Write"></li><li>If one of the transactions needs to roll back, it can be unclear which changes to retain.</li></ul><h3 id="Dirty-Read"><a href="#Dirty-Read" class="headerlink" title="Dirty Read"></a>Dirty Read</h3><ul><li>Occurs when a transaction reads data that has been modified by another transaction that has not yet committed.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/drity_read1.JPG" alt="Dirty Read"></li><li>To avoid this, uncommitted changes should only be visible to the transaction that made them.</li><li><strong>2PL:</strong> Rows in an uncommitted state are locked by the writing transaction, blocking other transactions from reading them.</li><li><strong>MVCC:</strong> Uses undo logs to provide the previous version of the row to reading transactions.</li></ul><h3 id="Non-Repeatable-Read"><a href="#Non-Repeatable-Read" class="headerlink" title="Non-Repeatable Read"></a>Non-Repeatable Read</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/Non_reapeatable_read1.JPG" alt="Non-Repeatable Read"></p><ul><li><strong>JPA&#x2F;Hibernate:</strong> Cache row data in the Persistence Context to avoid this issue.</li></ul><h3 id="Phantom-Read"><a href="#Phantom-Read" class="headerlink" title="Phantom Read"></a>Phantom Read</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/PhantomRead1.JPG" alt="Phantom Read"></p><h3 id="Read-Skew"><a href="#Read-Skew" class="headerlink" title="Read Skew"></a>Read Skew</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/ReadSkew1.JPG" alt="Read Skew"></p><h3 id="Write-Skew"><a href="#Write-Skew" class="headerlink" title="Write Skew"></a>Write Skew</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/WriteSkew1.JPG" alt="Write Skew"></p><h3 id="Lost-Update"><a href="#Lost-Update" class="headerlink" title="Lost Update"></a>Lost Update</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/LostUpdate1.JPG" alt="Lost Update"></p><ul><li><strong>Hibernate&#x2F;JPA:</strong> Use Row Version to implement Optimistic Locking, handling this issue at the application level.</li></ul><h2 id="Isolation-Levels"><a href="#Isolation-Levels" class="headerlink" title="Isolation Levels"></a>Isolation Levels</h2><table><thead><tr><th>Isolation Level</th><th>Dirty Read</th><th>Non-Repeatable Read</th><th>Phantom Read</th></tr></thead><tbody><tr><td>Read Uncommitted</td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td>Read Committed</td><td>No</td><td>Yes</td><td>Yes</td></tr><tr><td>Repeatable Read</td><td>No</td><td>No</td><td>Yes</td></tr><tr><td>Serializable</td><td>No</td><td>No</td><td>No</td></tr></tbody></table><h3 id="Read-Uncommitted"><a href="#Read-Uncommitted" class="headerlink" title="Read Uncommitted"></a>Read Uncommitted</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/ReadUncommited_Table1.JPG" alt="Read Uncommitted"></p><ul><li><strong>Oracle:</strong> Does not support Dirty Read.</li></ul><h3 id="Read-Committed"><a href="#Read-Committed" class="headerlink" title="Read Committed"></a>Read Committed</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/ReadCommited_Table1.JPG" alt="Read Committed"></p><ul><li><strong>Oracle:</strong> Ensures that all statements have a <code>start_timestamp</code> to create a database snapshot at a point in time. When two transactions update the same record, the first transaction locks the record to prevent a dirty write.</li></ul><h3 id="Repeatable-Read"><a href="#Repeatable-Read" class="headerlink" title="Repeatable Read"></a>Repeatable Read</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/ReapeabtaleRead_Table1.JPG" alt="Repeatable Read"></p><h3 id="Serializable"><a href="#Serializable" class="headerlink" title="Serializable"></a>Serializable</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/persistence/sqldb/Serializable_table1.JPG" alt="Serializable"></p><h1 id="Transaction-Durability"><a href="#Transaction-Durability" class="headerlink" title="Transaction - Durability"></a>Transaction - Durability</h1><ul><li><strong>Oracle:</strong> Uses redo logs. For performance reasons, redo records are stored in a buffer, and the Log Writer flushes from in-memory records to redo log files. There is a risk of data loss if the buffer isn’t flushed before a crash. Oracle maintains at least two redo files, but only one is active at a time. When a transaction is committed, the database flushes the buffer to persist the data.</li><li><strong>SQL Server:</strong> Combines undo and redo logs in a single <code>transaction log</code> file. When a transaction is committed, the data is flushed to disk before returning to the client. Since SQL Server 2014, it supports <code>configurable durability</code>, allowing log flush delays to improve I&#x2F;O performance <a href="https://msdn.microsoft.com/en-us/library/dn449490.aspx">documentation</a>.</li><li><strong>PostgreSQL:</strong> Uses Write-Ahead Log (WAL). Log entries are buffered in memory and flushed to disk when a transaction is committed <a href="http://www.postgresql.org/docs/current/static/wal-intro.html">documentation</a>.</li><li><strong>MySQL:</strong> Redo log entries are linked to single transactions.</li></ul>]]></content>
      
      
      <categories>
          
          <category> database </category>
          
      </categories>
      
      
        <tags>
            
            <tag> persistence </tag>
            
            <tag> transaction </tag>
            
            <tag> isolation </tag>
            
            <tag> durability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDos, Stress test, distribution test với Jmeter, Docker, AWS ECS</title>
      <link href="/2020/11/Stories/Ddos_Jmeter_ECS/"/>
      <url>/2020/11/Stories/Ddos_Jmeter_ECS/</url>
      
        <content type="html"><![CDATA[<h1 id="DDos-Stress-Test-Distribution-Test-with-Jmeter-Docker-AWS-ECS"><a href="#DDos-Stress-Test-Distribution-Test-with-Jmeter-Docker-AWS-ECS" class="headerlink" title="DDos, Stress Test, Distribution Test with Jmeter, Docker, AWS ECS"></a>DDos, Stress Test, Distribution Test with Jmeter, Docker, AWS ECS</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>This article notes the steps I experienced with the combination of these three tools to stress test my web app system.<br>The initial goal was to perform performance tests and stress tests from the perspective of an end user (from the outside in).<br>While it may not achieve true DDos or distribution testing due to limited infrastructure resources,<br>the architecture is aimed towards that goal. I hope that by sharing my experiences, this guide will help you save time in<br>finding solutions for stress testing and performance testing.</p><p>Some basic knowledge you should have if you intend to follow this guide:</p><ul><li>Basic knowledge of Jmeter</li><li>Basic knowledge of Docker</li><li>Basic knowledge of AWS ECS</li></ul><h2 id="2-Quick-Scenario"><a href="#2-Quick-Scenario" class="headerlink" title="2. Quick Scenario"></a>2. Quick Scenario</h2><ul><li>Jmeter: A tool that supports stress testing. You can write a script to access a website (or an API) and perform actions (GET, POST, etc.). It supports CPU, thread, loop iterations, and exports test results. It also supports exporting a test plan file (<code>.jmx</code>), suitable for running on servers.</li><li>Docker: Using Jmeter’s <code>.jmx</code> file to build a container.</li><li>AWS ECS: A service from AWS to run containers. This will provide the infrastructure resources to run the script (CPU, RAM, network). Using ECS allows easy scaling without managing individual server nodes. You only run tasks when testing, saving costs. You can fully utilize ECS by creating multiple clusters across different regions. Besides ECS, you can use other services like K8s or Docker Swarm.</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/ddos_overview.jpg" alt="Overview"></p><h2 id="3-Step-by-Step"><a href="#3-Step-by-Step" class="headerlink" title="3. Step by Step"></a>3. Step by Step</h2><h2 id="3-1-Writing-the-Jmeter-Script"><a href="#3-1-Writing-the-Jmeter-Script" class="headerlink" title="3.1 Writing the Jmeter Script"></a>3.1 Writing the Jmeter Script</h2><ul><li>Download Jmeter: <a href="https://jmeter.apache.org/download_jmeter.cgi">https://jmeter.apache.org/download_jmeter.cgi</a><ul><li>Requires Java (minimum Java 8)</li><li>Provides a UI. Simply download, extract, and run the <code>.jar</code> file in the &#x2F;bin directory.</li><li>When installing on Linux, be aware of the version (default on Ubuntu is 2.x, which is outdated and may cause script errors).</li></ul></li><li>Writing the script (not detailing how to use Jmeter here)<ul><li>Jmeter has a recording mode</li><li>Scenario: Using <a href="https://webhook.site/">https://webhook.site/</a> as the target website for testing. This site generates a unique URL for you, and any requests to this URL will be visualized on the webhook.site dashboard.</li><li>Example <code>jmx</code> file: <a href="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/test.jmx">https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/test.jmx</a></li><li>I also wrote a script to create orders on a shopping site (no captcha required) and it worked fine, showing Jmeter’s versatility.</li></ul></li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/jmeter_tool.JPG" alt="jmetertool"></p><h2 id="3-2-Testing-the-jmx-Script"><a href="#3-2-Testing-the-jmx-Script" class="headerlink" title="3.2 Testing the .jmx Script"></a>3.2 Testing the <code>.jmx</code> Script</h2><ul><li>Run the script using the UI tool on your PC for simplicity. After execution, check webhook.site for messages to confirm success.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/webhook.site.JPG" alt="webHook.site"></li><li>Run the script on a Linux server using CLI:</li></ul><pre><code class="bash">bash ~/jmeter/apache-jmeter-5.3/bin/jmeter.sh -n -t ~/jmeter/01/test.jmx -j tmp1.log -l tmp2.xml</code></pre><pre><code>- `test.jmx` is the test plan file- `tmp1.log` is the log file- `tmp2.xml` is the result file (showing HTTP request statuses like 200, 404, 403)- Use `-n` for non-GUI mode on the server</code></pre><h2 id="3-3-Dockerfile"><a href="#3-3-Dockerfile" class="headerlink" title="3.3 Dockerfile"></a>3.3 Dockerfile</h2><ul><li>After testing the CLI and everything works fine, build the Dockerfile:</li></ul><pre><code class="Dockerfile">FROM openjdk:8-jre-alpine3.7RUN apk update &amp;&amp; \    apk add ca-certificates wget &amp;&amp; \    update-ca-certificatesENV JMETER_HOME=/usr/share/apache-jmeter \    JMETER_VERSION=5.3 \    TEST_SCRIPT_FILE=/var/jmeter/test.jmx \    TEST_LOG_FILE=/var/jmeter/test.log \    TEST_RESULTS_FILE=/var/jmeter/test-result.xml \    PATH=&quot;~/.local/bin:$PATH&quot; \    PORT=443     RUN wget https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-$&#123;JMETER_VERSION&#125;.tgz &amp;&amp; \    tar zxvf apache-jmeter-$&#123;JMETER_VERSION&#125;.tgz  &amp;&amp; \    rm -f apache-jmeter-$&#123;JMETER_VERSION&#125;.tgz &amp;&amp; \     mv apache-jmeter-$&#123;JMETER_VERSION&#125; $&#123;JMETER_HOME&#125;COPY test.jmx $&#123;TEST_SCRIPT_FILE&#125;EXPOSE 443CMD export PATH=~/.local/bin:$PATH &amp;&amp; \    $JMETER_HOME/bin/jmeter -n \    -t=$TEST_SCRIPT_FILE \    -j $TEST_LOG_FILE \    -l=$TEST_RESULTS_FILE  &amp;&amp; \    echo -e &quot;\n\n======TEST LOGS========\n\n&quot;  &amp;&amp; \    cat  $TEST_LOG_FILE &amp;&amp; \    echo -e &quot;\n\n======TEST RESULTS========\n\n&quot;  &amp;&amp; \    cat $TEST_RESULTS_FILE</code></pre><ul><li>Since I use AWS ECS, I also use the Docker repository ECR for convenience.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/ecr.JPG" alt="ecr"></li></ul><h3 id="3-4-Build-ECS-Cluster-and-Run-Task"><a href="#3-4-Build-ECS-Cluster-and-Run-Task" class="headerlink" title="3.4 Build ECS Cluster and Run Task"></a>3.4 Build ECS Cluster and Run Task</h3><ul><li>Create an ECS Cluster<ul><li>Use Fargate or EC2 as the base. Fargate means you only care about the network. With EC2, you also need to manage instances. Both have resource limits, and my account can handle up to 40 vCPUs. For more resources, clone the cluster across regions.</li><li>To configure different outbound IPs for accessing the target website, set up a more complex VPC network.</li></ul></li><li>Create Task Definitions<ul><li>Create tasks to run the Jmeter containers pushed to ECR</li><li>Configure logs to CloudWatch for monitoring results.</li></ul></li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/ecs_task.JPG" alt="ecsTask"></p><ul><li>Run the task<ul><li>Configure scaling to maximize resource usage for the script.</li></ul></li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/ecs_run_task.JPG" alt="ecsRunTask"></p><h3 id="3-5-Check-CloudWatch-Logs"><a href="#3-5-Check-CloudWatch-Logs" class="headerlink" title="3.5 Check CloudWatch Logs"></a>3.5 Check CloudWatch Logs</h3><ul><li>Logs declared in the Dockerfile are pushed to CloudWatch.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jmeter/cloudwatch.JPG" alt="Log"></li></ul>]]></content>
      
      
      <categories>
          
          <category> stories </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ecs </tag>
            
            <tag> jmeter </tag>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Concurrency</title>
      <link href="/2020/09/Java/Concurrency/"/>
      <url>/2020/09/Java/Concurrency/</url>
      
        <content type="html"><![CDATA[<h2 id="ThreadLocal"><a href="#ThreadLocal" class="headerlink" title="ThreadLocal"></a>ThreadLocal</h2><ul><li><p>Mechanism is each thread has a “memory zone&#x2F;separate”. All methods of all classes&#x2F;instances, if running in the same<br>thread can access that “memory zone”.</p></li><li><p>Example case: create the context for the application. (ApplicationContext.getCurrentSession). It is very flexible.<br>Replace for a way to get value via a parameter of the method (that has a bit lengthy. IMO).</p></li><li><p>Improve thread-safe</p></li><li><p>WARNING: Be careful when using it with ThreadPool. A situation may happen Task A and Task B, both running in the same<br>thread (support by ThreadPool). Then TaskA can get the value that makes by TaskB.</p></li><li><p>Some reference:</p><ul><li><a href="http://drunkkid2000.blogspot.com/2013/07/thread-local_2564.html">http://drunkkid2000.blogspot.com</a></li><li><a href="http://tutorials.jenkov.com/java-concurrency/threadlocal.html">jenkov</a></li></ul></li></ul><h3 id="Code-example"><a href="#Code-example" class="headerlink" title="Code example"></a>Code example</h3><ul><li><code>InheritableThreadLocal</code>:<ul><li>Using it when you want to ChildThread (created by ParentThread) can using “clone copy” from ParentThread. (When<br>ChildThread modify the value - NO modified the ParentThread)</li><li>The different ChildThread does not share the same “memory zone”.</li></ul></li></ul><pre><code class="java">public static void main(String[] args) &#123;        ThreadLocal&lt;String&gt; threadLocal = new ThreadLocal&lt;&gt;();        InheritableThreadLocal&lt;String&gt; inheritableThreadLocal =                new InheritableThreadLocal&lt;&gt;();        Thread thread1 = new Thread(() -&gt; &#123;            System.out.println(&quot;===== Thread 1 =====&quot;);            threadLocal.set(&quot;Thread 1 - ThreadLocal&quot;);            inheritableThreadLocal.set(&quot;Thread 1 - InheritableThreadLocal&quot;);            System.out.println(threadLocal.get());            System.out.println(inheritableThreadLocal.get());            Thread childThread = new Thread(() -&gt; &#123;                System.out.println(&quot;===== ChildThread =====&quot;);                System.out.println(threadLocal.get());                System.out.println(inheritableThreadLocal.get());                inheritableThreadLocal.set(&quot;TUNG&quot;);                System.out.println(inheritableThreadLocal.get());            &#125;);            Thread childThread2 = new Thread(() -&gt; &#123;                try &#123;                    TimeUnit.SECONDS.sleep(3);                &#125; catch (InterruptedException e) &#123;                    e.printStackTrace();                &#125;                System.out.println(&quot;===== ChildThread2 =====&quot;);                System.out.println(threadLocal.get());                System.out.println(&quot;Check: &quot; + inheritableThreadLocal.get());            &#125;);            childThread.start();            childThread2.start();            try &#123;                TimeUnit.SECONDS.sleep(1);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            System.out.println(inheritableThreadLocal.get());            inheritableThreadLocal.set(&quot;TUNG2&quot;);        &#125;);        thread1.start();//        Thread thread2 = new Thread(() -&gt; &#123;//            try &#123;//                Thread.sleep(3000);//            &#125; catch (InterruptedException e) &#123;//                e.printStackTrace();//            &#125;////            System.out.println(&quot;===== Thread2 =====&quot;);//            System.out.println(threadLocal.get());//            System.out.println(inheritableThreadLocal.get());//        &#125;);//        thread2.start();    &#125;</code></pre><h2 id="Volatile"><a href="#Volatile" class="headerlink" title="Volatile"></a>Volatile</h2><ul><li>Volatile helps the variable will get read&#x2F;write from the main memory. (Default, maybe it can read&#x2F;write from CPU<br>Cache, that designed for performance, that is a reason for threads read&#x2F;write not the latest value)</li></ul><p><img src="https://images.viblo.asia/59d1214d-4438-4f46-878f-5db8af35fa1c.png" alt="Volatile"></p><h2 id="ThreadSafe"><a href="#ThreadSafe" class="headerlink" title="ThreadSafe"></a>ThreadSafe</h2><h3 id="XSync"><a href="#XSync" class="headerlink" title="XSync"></a>XSync</h3><ul><li>This is a library that supports threadsafe, threads will wait for each other for using the same resource. (It will be helpful more <code>synchronization</code> tag)</li></ul><pre><code class="xml">        &lt;dependency&gt;            &lt;groupId&gt;com.antkorwin&lt;/groupId&gt;            &lt;artifactId&gt;xsync&lt;/artifactId&gt;            &lt;version&gt;1.1&lt;/version&gt;        &lt;/dependency&gt;</code></pre><pre><code class="java">// methodpublic void timeWaitLimit(String storeAlias) &#123;        xSync.execute(storeAlias, () -&gt; &#123;            LeakyBucketModel model = BucketLeakyStatic.get(storeAlias);            if (model == null) &#123;                log.debug(storeAlias + &quot;-init&quot;);                BucketLeakyStatic.put(storeAlias, new LeakyBucketModel(bucketSize));            &#125; else &#123;                log.debug(storeAlias + &quot;-&quot; + model.getAllowCounter() + &quot;/&quot; + bucketSize);                if (model.getAllowCounter() &lt;= 0) &#123;                    try &#123;                        TimeUnit.MILLISECONDS.sleep(1200);                    &#125; catch (InterruptedException e) &#123;                        e.printStackTrace();                    &#125;                    int diffSecond = (int) TimeUnit.SECONDS.convert(Util.getUTC().getTime() - model.getModifiedTime(), TimeUnit.MILLISECONDS);                    int reNewCounter = Math.min(diffSecond * drainRate, bucketSize);                    model.reInitCounter(reNewCounter);                &#125;                model.decrementCounter();            &#125;        &#125;);    &#125;// LeakyBucketModel.classpublic class LeakyBucketModel &#123;    private int allowCounter;    private long modifiedTime;    public LeakyBucketModel(int allowCounter) &#123;        this.allowCounter = allowCounter;        this.modifiedTime = Util.getUTC().getTime();    &#125;    public void decrementCounter() &#123;        this.allowCounter -= 1;        this.modifiedTime = Util.getUTC().getTime();    &#125;    public void reInitCounter(int allowCounter) &#123;        this.allowCounter = allowCounter;        this.modifiedTime = Util.getUTC().getTime();    &#125;    public int getAllowCounter() &#123;        return this.allowCounter;    &#125;    public long getModifiedTime() &#123;        return this.modifiedTime;    &#125;&#125;</code></pre><h2 id="CompletableFuture"><a href="#CompletableFuture" class="headerlink" title="CompletableFuture"></a>CompletableFuture</h2><h3 id="Example-API-for-calculating-the-shipping-delivery-fee"><a href="#Example-API-for-calculating-the-shipping-delivery-fee" class="headerlink" title="Example: API for calculating the shipping-delivery fee"></a>Example: API for calculating the shipping-delivery fee</h3><pre><code class="java"> private GHNV2CalFeeDetailResponse calculator(int storeId, GHNV2CalculatorFeeRequest request) &#123;        String token = getToken(storeId);        var clientResponse = client.calculatorFee(token, request);        try &#123;            log.debug(&quot;--- calculator fee request --Token=&quot; + token +                    &quot;, &quot; + json.writeValueAsString(request) + &quot;\n --- response &quot; + json.writeValueAsString(clientResponse));        &#125; catch (Exception ignored) &#123;        &#125;        if (clientResponse == null || clientResponse.getCode() != 200) return null;        var result = mapper.toDetailResponse(clientResponse);        result.setRequest(request);        return result;    &#125;    private CompletableFuture&lt;GHNV2CalFeeDetailResponse&gt; calculatorFeeAsync(int storeId, GHNV2CalculatorFeeRequest request) &#123;        return CompletableFuture.supplyAsync(() -&gt; calculator(storeId, request));    &#125;    public List&lt;GHNV2CalFeeDetailResponse&gt; calculatorFee(int storeId, List&lt;GHNV2CalculatorFeeRequest&gt; requests) &#123;        if (CollectionUtils.isEmpty(requests)) return null;        List&lt;CompletableFuture&lt;GHNV2CalFeeDetailResponse&gt;&gt; listAsync = new ArrayList&lt;&gt;();        requests.forEach(e -&gt; &#123;            listAsync.add(calculatorFeeAsync(storeId, e));        &#125;);        CompletableFuture&lt;Void&gt; allAsync = CompletableFuture                .allOf(listAsync.toArray(new CompletableFuture[listAsync.size()]));        CompletableFuture&lt;List&lt;GHNV2CalFeeDetailResponse&gt;&gt; allClientAsync = allAsync.thenApply(v -&gt; listAsync.stream().map(CompletableFuture::join)                .collect(Collectors.toList()))                .handle((voidResult, throwable) -&gt;                        (throwable == null ?                                listAsync.stream() :                                listAsync.stream().filter(f -&gt; !f.isCompletedExceptionally()))                                .map(CompletableFuture::join)                                .filter(Objects::nonNull)                                .collect(Collectors.toList()));        List&lt;GHNV2CalFeeDetailResponse&gt; result = new ArrayList&lt;&gt;();        try &#123;            var temp = allClientAsync.get();            if (!CollectionUtils.isEmpty(allClientAsync.get())) &#123;                result = temp;            &#125;        &#125; catch (InterruptedException | ExecutionException e) &#123;            throw new RuntimeException(e);        &#125;        return result;    &#125;</code></pre><h2 id="Striped-Locks"><a href="#Striped-Locks" class="headerlink" title="Striped Locks"></a>Striped Locks</h2><p>Get an idea from each instance we need one Lock, so 1000 instances, you need 1000 Lock -&gt; This is spent more memory.</p><p>The mechanism of StripedLock (Guava) is managed by the group. One Lock managed N element &#x3D;&gt; Example: instances 1-100<br>using 1 Lock, instances 101-200 using 1 Lock &#x3D;&gt; So, 1000 instances we just spent 10 Lock. This is the balancing between<br>performance and memory.</p><pre><code class="java">private Striped&lt;Lock&gt; stripedLocks = Striped.lock(10);public void update(Bag bag)&#123;    Lock lock = stripedLocks.get(bag.getId());    lock.lock();    if (!bag.hasBlueCandy())&#123;        bag.add(new Candy(color&quot;blue&quot;));    &#125;    lock.unlock();&#125;</code></pre><ul><li>Many locks &#x3D; More memory, good throughput</li><li>Fewer locks &#x3D; Better memory, more contention</li><li>Striped locks &#x3D; Middle ground</li><li>Imp: Choose obj to retrieve the lock</li><li>Need to have hashcode &amp; equals</li><li>Striped versions of Lock, Semaphore and ReadWriteLock</li><li>Guava also has corresponding weak versions for easy GC</li></ul><h2 id="ReadWriteLock"><a href="#ReadWriteLock" class="headerlink" title="ReadWriteLock"></a>ReadWriteLock</h2><ul><li><code>java.util.concurrent.locks.ReentrantLock</code> is the one implementation of Lock interface. This class has a structural<br>method called <code>ReentrantLock(boolean fair)</code>. While if <code>fair</code> is true, threads will access with FIFO sequence.</li><li><code>ReadWriteLock</code> using two keys for the lock. One for <code>read</code> and one for <code>write</code>. Read key can acquire by many thread<br>in the same time, while Write key can acquire by only one thread at the same time.</li></ul><h2 id="Lock-vs-synchronized"><a href="#Lock-vs-synchronized" class="headerlink" title="Lock vs synchronized"></a>Lock vs synchronized</h2><ul><li>Synchronized blocks must be contained within a single method. lock.lock() and lock.unlock() can be called from<br>different method</li><li>lock.lock() and lock.unlock() provides the same visibility and happens before guarantees as entering and exiting a<br>synchronized block</li><li>Synchronized blocks are always reentrant, Lock could decide not to be</li><li>Synchronized blocks do not guarantee fairness. Lock can<br>(fairness is the happened when one thread keeps a waiting status forever, it can not access the object, that is<br>locked. The reason is other threads always have higher priority. Not guarantee the FIFO)</li><li>Example: <code>ReentrantLock</code>, when initial, we can set <code>fair</code> parameter is true</li></ul><pre><code class="java"> ReentrantLock lock = new ReentrantLock(true);</code></pre><h2 id="Blocking-Threads-is-Expensive"><a href="#Blocking-Threads-is-Expensive" class="headerlink" title="Blocking Threads is Expensive"></a>Blocking Threads is Expensive</h2><ul><li>Entering a synchronized block is not that expensive - if the thread is allowed access. But if the thread is blocked<br>because another thread is already executing inside the synchronized block - the blocking of the thread is expensive.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/BlockingThreadsisExpensive.png" alt="Blocking Threads is Expensive"></li></ul><h2 id="Threads-Evaluation"><a href="#Threads-Evaluation" class="headerlink" title="Threads Evaluation"></a>Threads Evaluation</h2><p>(update 18&#x2F;08&#x2F;2021)</p><p>Number of threads &lt;&#x3D; (Number of cores) &#x2F; [1 - Blocking Factor (BF)]</p><ul><li>Blocking Factor is the fraction of time a thread is blocked on IO operations</li><li>If your tasks are computation-intensive, BF is 0 and # of Thread &lt;&#x3D; # of Cores</li><li>If your tasks are IO intensive, and if BF is 0.9, # of Thread &lt;&#x3D; 10 * # of Cores</li><li>If your tasks are IO intensive, and if BF is 0.5, # of Thread &lt;&#x3D; 2 * # of Cores</li><li>Normally the maximum number of requests that you can handle concurrently &lt;&#x3D;10k (2k, 4k, 5k)</li></ul><h2 id="ForkJoinPool"><a href="#ForkJoinPool" class="headerlink" title="ForkJoinPool"></a>ForkJoinPool</h2><pre class="mermaid">graph TDA(Object - java.lang)A --> B1(abstract AbstractExecutorService)A --> B2(abstract ForkJoinTask)B1 --> C1(ForkJoinPool)B1 --> C2(ThreadPoolExecutor)C2 --> D1(ScheduledThreadPoolExecutor)B2 --> E1(abstract CountedCompleter)B2 --> E2(abstract RecursiveAction)B2 --> E3(abstract RecursiveTask)</pre><ul><li><code>Work Stealing</code>:</li></ul><pre><code>Result solve(Problem problem) &#123;    if (problem is small)        directly solve problem    else &#123;        split problem into independent parts        fork new subtasks to solve each part        join all subtasks        compose result from subresults    &#125;&#125;</code></pre><ul><li><code>ExecutorService::newFixedThreadPool(int nThreads)</code>: Creates a thread pool that reuses a fixed number of threads<br>operating off a shared unbounded queue.</li><li><code>ScheduledExecutorService::newScheduledThreadPool(int corePoolSize)</code>: Creates a thread pool that can schedule commands<br>to run after a given delay, or to execute periodically.</li><li><code>ExecutorService::newCachedThreadPool(ThreadFactory threadFactory)</code>: Creates a thread pool that creates new threads as<br>needed, but will reuse previously constructed threads when they are available, and uses the provided ThreadFactory to<br>create new threads when needed</li><li><code>ExecutorService::newWorkStealingPool(int parallelism)</code>: Creates a thread pool that maintains enough threads to<br>support the given parallelism level, and may use multiple queues to reduce contention.</li></ul><h3 id="Some-use-cases"><a href="#Some-use-cases" class="headerlink" title="Some use cases"></a>Some use cases</h3><ol><li>If you want to process all submitted tasks in order of arrival, just use newFixedThreadPool(1)</li><li>If you want to optimize performance of big computation of recursive tasks, use ForkJoinPool or newWorkStealingPool</li><li>If you want to execute some tasks periodically or at certain time in the future, use newScheduledThreadPool</li></ol><h2 id="Notify-vs-notifyAll"><a href="#Notify-vs-notifyAll" class="headerlink" title="Notify vs notifyAll ?"></a>Notify vs notifyAll ?</h2><p>If you are not sure which to use, then use notifyAll</p><h2 id="Reactor-Schedulers"><a href="#Reactor-Schedulers" class="headerlink" title="Reactor Schedulers"></a>Reactor Schedulers</h2><ul><li><code>Schedulers.parallel()</code>: The number of parallel threads is equal to the number of machine threads. Example &#x3D; 8, 12, 16.<br>In time, we can have many threads, (example 100), but only N threads can be parallel. (max N&#x3D;8,12,16…)</li><li><code>Schedulers.boundedElastic()</code>: by default is x10 machine threads. Example &#x3D; 80, 120, 160.</li><li>By default, <code>parallel</code> and <code>boundedElastic</code> is daemon thread. If we using <code>.newParall</code> or <code>.newBoundedElastic</code>, daemon thread &#x3D; false </li><li><code>Schedulers.elastic</code> - Deprecated - unlimited threads we can create. (Note: The default time-to-live for unused thread pools is 60 seconds, use the appropriate factory to set a different value.)</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> concurrency </tag>
            
            <tag> thread local </tag>
            
            <tag> volatile </tag>
            
            <tag> thread safe </tag>
            
            <tag> completable future </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - JVM Internal</title>
      <link href="/2020/08/Java/JVM_Internal/"/>
      <url>/2020/08/Java/JVM_Internal/</url>
      
        <content type="html"><![CDATA[<p><img src="https://blog.jamesdbloom.com/images_2013_11_17_17_56/JVM_Internal_Architecture_small.png" alt="https://blog.jamesdbloom.com/images_2013_11_17_17_56/JVM_Internal_Architecture_small.png"></p><ul><li>Việc mapping giữa Thread trong Java và Thread của OS được thể hiện trong Hotspot JVM. Sau khi chuẩn bị đủ state của 1<br>java thread (thread-local storage, allocation buffer, synchronization object, stack, programme counter) thì Native<br>Thread được tạo. (native thread là OS thread)</li><li>Native Thread sẽ được <code>reclaim</code> sau khi Java Thread <code>terminates</code>.</li><li>Hệ điều hành chịu trách nhiệm lập lịch cho các thread, và gửi chúng cho CPU đang <code>available</code></li><li>Native thread sẽ được khởi tạo khi Java thread được gọi <code>run()</code> method.</li><li>Sau khi method <code>run()</code> được chạy xong (return), Native Thread sẽ confirm vs JVM về việc terminated, khi đó tất cả các<br>resource được sử dụng ở cả Native Thread và Java Thread sẽ được <code>released</code></li><li>Các JVM Thread được chạy background:<ul><li>VM Thread:</li><li>Periodic task thread</li><li>GC Thread</li><li>Compiler Thread</li><li>Signal dispatcher Thread</li></ul></li></ul><h2 id="Moi-Thread-se-gom-cac-thanh-phan-sau"><a href="#Moi-Thread-se-gom-cac-thanh-phan-sau" class="headerlink" title="Mỗi Thread sẽ gồm các thành phần sau"></a>Mỗi Thread sẽ gồm các thành phần sau</h2><ul><li><code>Program Counter</code>: có nhiệm vụ <code>counter</code> cho CPU, và lưu vị trí của code đã được bytes (). Thường thì PC sẽ tăng.</li><li><code>Stack</code>:<ul><li>Giữ frame khi method được chạy trên Thread đó</li><li>Frame mới sẽ được tạo và thêm vào stack (ngăn xếp LIFO). Và sẽ bị remove khi method trả về kết quả. Hoặc xảy ra<br>Exception ??</li><li>Frame objects được phân bổ trong Heap mà không cần phải liền kề</li></ul></li><li><code>Native Stack</code>: không phải JVM nào cũng hỗ trợ</li><li><code>Stack Restrictions</code><ul><li>Stack có thể dynamic, hoặc fixed size</li><li>Nếu thread yêu cầu stack lớn hơn &#x3D;&gt; StackOverflowError</li><li>Nếu thread yêu cầu frame mới và không đủ bộ nhớ &#x3D;&gt; OutOfMemoryError</li></ul></li><li><code>Frame</code>: được tạo và thêm vào top của stack cho mỗi <code>method invocation</code>. Sẽ được xóa khi method return. Thành phần:<ul><li>local variable array</li><li>return value</li><li>operand stack</li><li>reference to runtime constant pool for class of the current method</li></ul></li><li><code>Local Variables Array</code>: boolean, byte, char, long, short, int, float, double…</li><li><code>Operand Stack</code></li><li><code>Dynamic Linking</code></li></ul><h2 id="Shared-Between-Threads"><a href="#Shared-Between-Threads" class="headerlink" title="Shared Between Threads"></a>Shared Between Threads</h2><ul><li><code>Heap</code>: Array và Object không bao giờ lưu trên stack bởi vì frame được thiết kế là không thay đổi size sau khi đã được<br>tạo. Frame chỉ lưu trữ con trỏ tới object và mảng trên heap. Heap được sử dụng để <code>allocate</code> class instance, và mảng<br>khi runtime.</li><li><code>Memory Management</code><ul><li>Object và Arrays sẽ không bao giờ được <code>de-allocated</code> , trừ khi bị GC gọi.</li><li>Typically, this works as follows:<ul><li>New objects and arrays are created into the young generation</li><li>Minor garbage collection will operate in the young generation. Objects, that are still alive, will be moved<br>from the eden space to the survivor space.</li><li>Major garbage collection, which typically causes the application threads to pause, will move objects between<br>generations. Objects, that are still alive, will be moved from the young generation to the old (tenured)<br>generation.</li><li>The permanent generation is collected every time the old generation is collected. They are both collected when<br>either becomes full.</li></ul></li></ul></li><li><code>Non-Heap Memory</code>:<ul><li>Permanent Generation</li><li>Code Cache</li></ul></li><li><code>Just In Time (JIT) Compilation</code></li></ul><p><a href="https://blog.jamesdbloom.com/JVMInternals.html">https://blog.jamesdbloom.com/JVMInternals.html</a><br><a href="http://tutorials.jenkov.com/java-concurrency/thread-signaling.html">http://tutorials.jenkov.com/java-concurrency/thread-signaling.html</a></p><hr><h2 id="Some-Note"><a href="#Some-Note" class="headerlink" title="Some Note"></a>Some Note</h2><p>JVM - JAVA VIRTUAL MACHINE</p><ol><li>LIFE CYCLE</li></ol><ul><li>source -&gt; javac -&gt; bytecode -&gt; classloader -&gt; interpreter -&gt; JIT -&gt; optimized natived code</li></ul><ol start="2"><li>JAVAC</li></ol><ul><li>convert source code into byte code</li><li>check</li><li>simple optimizations</li></ul><ol start="3"><li>BYTECODE<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/jvm/bytecode.JPG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/jvm/bytecode.JPG"></li><li>CLASS</li></ol><ul><li>meta info: java version, file format</li><li>type info: access flags, name, superclass, interfaces</li><li>content: fields, method, signatures, bytecode</li></ul><ol start="5"><li>CLASSLOADER</li></ol><ul><li>dynamically loads classes</li><li>hierarchies</li></ul><ol start="6"><li>CLASSLOADING PHASES</li></ol><ul><li>loading -&gt; reads class file</li><li>linking<ul><li>verifying -&gt; verifies bytecode correctness</li><li>preparing -&gt; allocates memory</li><li>resolving -&gt; links with classes, interfaces fields, methods</li></ul></li><li>initializing -&gt; static initializers</li></ul><ol start="7"><li>INTERPRETER</li></ol><ul><li>template interpreter</li><li>detect the critical hot spots in the program</li></ul><ol start="8"><li>JIT</li></ol><ul><li>Just In Time</li><li>compiles methods into native code</li><li>client C1 &#x2F; client C2</li><li>runs up to 20 times faster</li></ul><ol start="9"><li>JIT OPTIMIZATIONS</li></ol><ul><li>inlining</li><li>loop unrolling</li><li>escape analysis (scalar replacement)</li><li>dead-code elimination</li><li>lock elision</li><li>osr</li></ul><ol start="10"><li>TIERED COMPILATION</li></ol><ul><li>LEVELS:<ul><li>0: interpreter code</li><li>1: Simple C1 compiled code</li><li>2: Limited C1 compiled code</li><li>3: Full C1 compiled code</li><li>4: C2 compiled code</li></ul></li></ul><ol start="11"><li>THREADS</li></ol><ul><li>direct mapping with native OS thread</li><li>scheduling and dispatching delegated to OS</li><li>application (user) threads</li><li>maintenance threads: GC, compiler, etc</li></ul><ol start="12"><li>SYNCHRONIZATION</li></ol><ul><li>described in Java Memory Model</li><li>extremely hard to understand</li></ul><ol start="13"><li>MEMORY BLOCKS</li></ol><ul><li>PC register</li><li>frame</li><li>stack</li></ul><ol start="14"><li>MEMORY LAYOUT<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/jvm/memory_Layout.JPG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/jvm/memory_Layout.JPG"></li><li>GARBAGE COLLECTOR</li></ol><ul><li>cleans memory</li><li>important performance factor</li><li>vector algorithm</li><li>stop the world in safe-points</li></ul><ol start="16"><li>GC ALGORITHMS</li></ol><ul><li>serial</li><li>parallel</li><li>Concurrent Mark sweep</li><li>G1</li></ul><p>Other topic: <a href="https://tschatzl.github.io/2021/02/26/early-prune.html">https://tschatzl.github.io/2021/02/26/early-prune.html</a></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> jvm internal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rabbitmq - Cluster High Availability</title>
      <link href="/2020/07/RabbitMQ/RabbitMQ_Build_HA_Cluster/"/>
      <url>/2020/07/RabbitMQ/RabbitMQ_Build_HA_Cluster/</url>
      
        <content type="html"><![CDATA[<h1 id="Trien-khai-Cluster-RabbitMQ-High-Availability"><a href="#Trien-khai-Cluster-RabbitMQ-High-Availability" class="headerlink" title="Triển khai Cluster RabbitMQ - High Availability"></a>Triển khai Cluster RabbitMQ - High Availability</h1><ul><li>Mình vừa trải qua một khoảng thời gian dài kha khá, để nghiên cứu và dựng lab về việc xây dựng rabbitmq cluster. Dưới<br>đây là toàn bộ “nhật ký” mình note lại.</li></ul><h2 id="1-Loi-gioi-thieu"><a href="#1-Loi-gioi-thieu" class="headerlink" title="1. Lời giới thiệu"></a>1. Lời giới thiệu</h2><ul><li>Bài viết mình sẽ không viết lại các khái niệm cơ bản.</li><li>Bài viết này mình nghĩ có lẽ hợp với:<ul><li>Ai muốn tìm hiểu nhanh việc xây dựng cluster rabbitmq, để nắm được overview, trước khi muốn nghiên cứu (hoặc cấu<br>hình) sâu hơn.</li><li>Hiểu tư tưởng, các vấn đề của vận hành 1 cluster Message Queue nói chung. Mà mình nghĩ có thể trở thành background<br>để tiếp cận với 1 hệ thống khác.</li></ul></li><li>Kiến thức nền để hiểu bài viết:<ul><li>Một chút về Rabbitmq</li><li>Một chút về Docker</li><li>Một chút về Networking</li></ul></li><li>Các thông tin có trong bài viết:<ul><li>Một chút lý thuyết về Quorum Queue trong RabbitMQ<ul><li>Lý do ra đời</li><li>So sánh với Mirror Queue (support ở version cũ)</li></ul></li><li>Dựng hệ thống cluster<ul><li>Sơ đồ thiết kế</li><li>Step by step triển khai</li></ul></li><li>Lab một số kịch bản khi cluster gặp sự cố<ul><li>Có node bị down&#x2F; và reUp</li><li>Network Partitions - Split Brain</li></ul></li></ul></li></ul><h2 id="2-Quorum-Queue"><a href="#2-Quorum-Queue" class="headerlink" title="2. Quorum Queue"></a>2. Quorum Queue</h2><h3 id="2-1-Mot-vai-y-chinh"><a href="#2-1-Mot-vai-y-chinh" class="headerlink" title="2.1 Một vài ý chính"></a>2.1 Một vài ý chính</h3><ul><li>Rabbitmq từ version 3.8.0 trở đi, có 1 thay đổi lớn về các feature support. Trong đó đặc biệt có Quorum Queue. Cái ra<br>đời để giải quyết các vấn đề của Mirror Queue trong các version trước đó gặp phải. (Quorum Queue là 1 “type”<br>, <code>không liên quan</code> gì tới Exchange Type: Direct, Topic, Fangout )</li><li>Khi tạo Quorum Queue, sẽ không có lựa chọn <code>Durability</code> như Classic Queue. (khi node bị lỗi, hoặc sự cố khởi động lại<br>thì <code>queue</code> sẽ được “load” lại khi startup, nếu <code>non-durable</code> thì sẽ bị mất)</li><li>Khi sử dụng Quorum Queue sẽ không phải upgrade client. &#x3D;&gt; tương thích ngược. Việc setup Quorum Queue nằm ở các node<br>server. Client không tham gia.</li><li>Khi 1 node fail, xong quay lại, thì nó chỉ đồng bộ các message mới. Mà không phải sync lại từ đầu. Và quá trình sync<br>các message mới này không bị blockking.</li><li>Nếu broker bị lỗi gì đó làm mất dữ liệu, thì toàn bộ messge trên broker đó sẽ mất vĩnh viễn. Khi broker đó online trở<br>lại, thì không thể đồng bộ lại data từ leader từ đầu.</li><li>Khi xây dựng cluster để triển khai Quorum Queue, các định nghĩa như 1 node master, các node khác slave hay replicates<br>sẽ không còn đúng nữa.</li><li>Quorum queues do not currently support priorities, including consumer priorities.</li></ul><h3 id="2-2-So-sanh-Quorum-Queue-vs-Mirror-Queue"><a href="#2-2-So-sanh-Quorum-Queue-vs-Mirror-Queue" class="headerlink" title="2.2 So sánh Quorum Queue vs Mirror Queue"></a>2.2 So sánh Quorum Queue vs Mirror Queue</h3><ul><li>Mirror Queue &#x3D;&gt; Mình nghĩ là nó đã Depreciation (quan điểm chủ quan). Tiền thân là Replicated queue</li><li>Quorum queue dữ message mãi mãi trên disk. Còn Mirror Queue, thì với các lựa chọn Durable Queue và Persistent Message<br>sẽ có các cách tính khác nhau:<ul><li>Non-durable Queue + Non-Persistent message &#x3D; mất Queue + mất Message (sau khi Broker restart)</li><li>Durable queue + Non-Persistent message &#x3D; Còn Queue + mất Message</li><li>Durable queue + Persistent message &#x3D; Còn Queue + Còn Message</li><li>Mirrored queue + Persistent message &#x3D; Còn Queue + Còn Message\</li></ul></li><li>Khi có 1 node lỗi, và sau đó quay trở lại bình thường. Với Mirror Queue, sẽ block cả cluster. Vì nó cần đồng bộ lại<br>toàn bộ message trong khoảng thời gian sự cố. Ngược lại với Quorum Queue thì nó không block. Nó chỉ đồng bộ các<br>message mới.</li><li>Khi gặp sự cố Network Partition. (mạng các node không kết nối được với nhau). Với Mirror Queue sẽ xảy ra tình<br>huống <code>split-brain</code>. (1 queue&#x2F;cluster &gt; 1 master). Với Quorum Queue, cung cấp các policy <code>autoheal</code>, <code>pause_minority</code><br>, <code>pause_if_all_down</code>, để người quản trị tự cấu hình hướng xử lý.</li><li>Với Mirror Queue, node master sẽ nhận tất cả các request đọc&#x2F;ghi. Các node mirror sẽ nhận tất cả message từ node<br>master và ghi vào disk. Các node mirror không có nhiệm vụ giảm tải “read”&#x2F;“write” cho node master. Nó chỉ mirror<br>message để phụ vụ việc HA. Với Quorum Queue, Queue A có thể master trên Node 1. Nhưng Queue B có thể master trên Node<ol><li></li></ol></li></ul><h2 id="3-Xay-dung-he-thong-cluster-rabbitmq"><a href="#3-Xay-dung-he-thong-cluster-rabbitmq" class="headerlink" title="3. Xây dựng hệ thống cluster rabbitmq"></a>3. Xây dựng hệ thống cluster rabbitmq</h2><h3 id="3-1-So-do-thiet-ke"><a href="#3-1-So-do-thiet-ke" class="headerlink" title="3.1 Sơ đồ thiết kế"></a>3.1 Sơ đồ thiết kế</h3><ul><li>Viết sơ đồ nghe lớn lao, chứ đơn giản thôi:<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/rabbitmq_ha_qq.png" alt="Diagram"></li><li>Mình sử dụng 3 node. (nên lựa chọn là số lẻ. Để thuận lợi cho giải thuật bầu leader)</li><li>3 node này mình cài cụm Docker Swarm, với leader là Node 1. Và sử dụng <code>docker stack</code> để triển khai cluster. (Bạn hoàn<br>toàn có thể không sử dụng Docker Swarm, mà chỉ dùng mỗi Docker container thường cũng được. Ở các bước step-by-step<br>mình sẽ giải thích chi tiết để bạn có thể tùy chỉnh).</li><li>Nên set up số node (broker) là số lẻ. Ví dụ 3,5,7 để thuận lợi cho giải thuật bầu leader</li><li>Môi trường triển khai:<ul><li>Các node chạy Ubuntu Server 18.04</li><li>Các node cài Docker</li><li>Node1 chạy rabbitMq, có hostname container là <code>rabbitmq1</code>. Tương tự với node2, node3 là <code>rabbitmq2</code>, <code>rabbitmq3</code>.</li></ul></li></ul><h3 id="3-2-Step-by-step"><a href="#3-2-Step-by-step" class="headerlink" title="3.2 Step by step"></a>3.2 Step by step</h3><h4 id="1-Chuan-bi"><a href="#1-Chuan-bi" class="headerlink" title="1). Chuẩn bị"></a>1). Chuẩn bị</h4><ul><li>Mình lab sử dụng 3 instance AWS EC2. (cho mạng khỏe, máy nhanh, đỡ phải đợi chờ). Bạn thay thế bằng máy chủ nào cũng<br>được, dùng docker-machine, hay ảo hóa vmware cũng được. Không quan trọng lắm. Miễn 3 node thông mạng nhau, và có<br>internet để download là được.<ul><li>Thông tin hostname (ở bước triển khai docker-stack cần dùng).<ul><li>Node1 &#x3D; ip-172-31-11-205</li><li>Node2 &#x3D; ip-172-31-3-230</li><li>Node3 &#x3D; ip-172-31-1-3</li><li>(Lấy thông tin này bằng cách ssh vào server và gõ command <code>hostname</code>.)</li></ul></li></ul></li><li>Cài Docker Engineer<ul><li>Google cách cài hoặc run script mình viết này cho nhanh cũng được</li></ul><pre><code class="bash">wget -O - https://raw.githubusercontent.com/tungtv202/MyNote/master/Docker/docker_install.sh | bash</code></pre></li><li>Cài cụm Docker Swarm trên 3 node.<ul><li>Lựa chọn node1 làm leader.</li></ul><pre><code class="bash">docker swarm init --advertise-addr=172.31.11.205</code></pre><ul><li>Node2 và Node3 join cluster</li></ul><pre><code class="bash">docker swarm join --token SWMTKN-1-5xv7z2ijle1dhivalkl5cnwhoadp6h8ae0p7bs5tmanvkpbi3l-5ib6sjrd3w0wdhfsnt8ga7ybd 172.31.11.205:2377</code></pre><ul><li>Kết quả<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/docker-swarm.JPG" alt="docker-swarm"></li><li>Chi tiết hơn có thể tham khảo bài hướng dẫn của<br>thầy <a href="https://xuanthulab.net/tim-hieu-ve-docker-swarm-khoi-tao-va-su-dung.html">xuanthulab.net</a></li></ul></li></ul><h4 id="2-Dockerfile"><a href="#2-Dockerfile" class="headerlink" title="2). Dockerfile"></a>2). Dockerfile</h4><ul><li><p>Dockerfile</p><pre><code class="Dockerfile">FROM rabbitmq:3-managementCOPY rabbitmq-qq.conf /etc/rabbitmq/rabbitmq.confCOPY rabbitmq-qq-definitions.json /etc/rabbitmq/rabbitmq-definitions.json#ENV RABBITMQ_CONF_ENV_FILE /etc/rabbitmq/rabbitmq-env.confENV RABBITMQ_ERLANG_COOKIE cookieSecret#RUN apt-get update &amp;&amp; apt-get install -y iputils-ping &amp;&amp; apt-get install -y telnet &amp;&amp; apt-get install -y nano</code></pre><ul><li>Lưu ý biến môi trường <code>RABBITMQ_ERLANG_COOKIE</code> rất quan trọng. Các node rabbitmq muốn giao tiếp được với nhau thì<br>giá trị cookie này cần phải giống nhau thì mới có thể <code>authen</code> được. (Nếu không set giá trị này, thì giá<br>trị <code>erlang.cookie</code> sẽ được sinh ngẫu nhiên, và khác nhau trên mỗi node). Thông tin các biến môi trường khác mà<br>rabbitmq hỗ trợ, tham khảo<br>tại <a href="https://www.rabbitmq.com/configure.html#customise-environment">https://www.rabbitmq.com/configure.html#customise-environment</a></li></ul></li><li><p>File cấu hình <code>rabbitmq-qq.conf</code></p><pre><code class="conf">loopback_users.guest = falselisteners.tcp.default = 5672management.listener.port = 15672management.listener.ssl = falsevm_memory_high_watermark.absolute = 1536MBcluster_name = rabbitmq-qqcluster_formation.peer_discovery_backend = rabbit_peer_discovery_classic_configcluster_formation.classic_config.nodes.1 = rabbit@rabbitmq1cluster_formation.classic_config.nodes.2 = rabbit@rabbitmq2cluster_formation.classic_config.nodes.3 = rabbit@rabbitmq3management.load_definitions = /etc/rabbitmq/rabbitmq-definitions.json# background_gc_enabled = true# Increase the 5s default so that we are below Prometheus&#39; scrape interval,# but still refresh in time for Prometheus scrape# This is linked to Prometheus scrape interval &amp; range used with rate()collect_statistics_interval = 10000# Enable debugginglog.file = rabbit.loglog.dir = /var/log/rabbitmqlog.console.level = infocluster_partition_handling = pause_minority</code></pre><ul><li><code>listeners.tcp.default=5672</code>. port để client kết nối vào broker. Mặc định port này là 5672. Có thể đổi sang port<br>khác nếu conflict</li><li><code>management.listener.port = 15672</code>. port để vào webadmin gui</li><li>Có 1 port là <code>epmd</code> mình không để trong file config. Mặc định port này là <code>4369</code>. Port này rất quan trọng, các<br>node dùng port này để <code>discovery</code> nhau. Bắt buộc số port phải là giống nhau trên các node</li><li>Thông tin các port có thể tham khảo thêm<br>tại <a href="https://www.rabbitmq.com/networking.html#ports">https://www.rabbitmq.com/networking.html#ports</a></li><li><code>rabbitmq1</code>, <code>rabbitmq2</code>, <code>rabbitmq3</code> lần lượt là hostname của 3 node. (Lưu ý 3 hostname này khác với hostname của<br>instance ec2, mình viết bên trên). Mặc định rabbitmq không hỗ trợ FQDN. Muốn dùng hostname dài. thì cần set<br>env <code>RABBITMQ_USE_LONGNAME =true</code>. Chi tiết<br>hơn <a href="https://www.rabbitmq.com/clustering.html#node-names">https://www.rabbitmq.com/clustering.html#node-names</a></li><li><code>cluster_partition_handling=pause_minority</code>: khi có sự cố networking partition, rabbitmq cung cấp 3 policy để<br>handler, là <code>pause-minority</code>, <code>pause-if-all-down</code>, <code>autoheal</code>. Nếu bạn không khai báo cấu hình này thì mặc định nó<br>sẽ <code>ignore</code>, không làm gì cả. Sau khi mình tham khảo thì thấy <code>pause-minority</code> có lẽ mình sẽ dùng nhiều nhất. Với<br>mode này thì bên phía các node có số lượng ít hơn, sẽ bị <code>down</code> luôn. Các message, queue sẽ được gửi về bên phía<br>có số lượng node nhiều hơn. Thông tin về vấn đề này mình sẽ viết chi tiết hơn bên dưới. Có thể tham<br>khảo <a href="https://www.rabbitmq.com/partitions.html">https://www.rabbitmq.com/partitions.html</a></li></ul></li><li><p>File <code>rabbitmq-qq-definitions.json</code></p><pre><code class="json">&#123;&quot;global_parameters&quot;: [    &#123;&quot;name&quot;: &quot;cluster_name&quot;, &quot;value&quot;: &quot;rabbitmq-qq&quot;&#125;],&quot;permissions&quot;: [    &#123;    &quot;configure&quot;: &quot;.*&quot;,    &quot;read&quot;: &quot;.*&quot;,    &quot;user&quot;: &quot;tungtv&quot;,    &quot;vhost&quot;: &quot;/&quot;,    &quot;write&quot;: &quot;.*&quot;    &#125;],&quot;users&quot;: [    &#123;    &quot;name&quot;: &quot;tungtv&quot;,    &quot;password&quot;: &quot;tungtv&quot;,    &quot;tags&quot;: &quot;administrator&quot;    &#125;],&quot;vhosts&quot;: [&#123;&quot;name&quot;: &quot;/&quot;&#125;]&#125;</code></pre><ul><li>File này mình chỉ để define account đăng nhập</li><li>Bạn có thể khai báo file này để <code>declare</code> các queue, và nhiều hơn nữa.</li></ul></li><li><p>Có thể build docker image bằng command</p></li></ul><pre><code class="bash"> docker build -t rabbitmq_ha_qq -f Dockerfile .</code></pre><ul><li>Hoặc có thể dùng trực tiếp docker image mà mình đã build sẵn, và public<br>tại <a href="https://hub.docker.com/repository/docker/tungtv202/rabbitmq_ha_qq">https://hub.docker.com/repository/docker/tungtv202/rabbitmq_ha_qq</a></li></ul><h4 id="3-Docker-stack-file"><a href="#3-Docker-stack-file" class="headerlink" title="3). Docker stack file"></a>3). Docker stack file</h4><ul><li>docker-compose.yml file</li></ul><pre><code class="yaml">    version: &#39;3.7&#39;    volumes:    rabbitmq_volume:    services:    rabbitmq1:        image: tungtv202/rabbitmq_ha_qq        ports:        - &quot;5672:5672&quot;        - &quot;15672:15672&quot;        hostname: rabbitmq1        volumes:        - rabbitmq_volume:/var/lib/rabbitmq        deploy:        replicas: 1        placement:            constraints:            - node.hostname == ip-172-31-11-205        resources:            limits:            cpus: &#39;1&#39;            memory: &#39;500MB&#39;            reservations:            cpus: &#39;0.5&#39;            memory: &#39;50MB&#39;        restart_policy:            condition: on-failure    rabbitmq2:        image: tungtv202/rabbitmq_ha_qq        ports:        - &quot;5677:5672&quot;        - &quot;15677:15672&quot;        hostname: rabbitmq2        volumes:        - rabbitmq_volume:/var/lib/rabbitmq        deploy:        replicas: 1        placement:            constraints:            - node.hostname == ip-172-31-3-230        resources:            limits:            cpus: &#39;1&#39;            memory: &#39;500MB&#39;            reservations:            cpus: &#39;0.5&#39;            memory: &#39;50MB&#39;        restart_policy:            condition: on-failure    rabbitmq3:        image: tungtv202/rabbitmq_ha_qq        ports:        - &quot;5666:5672&quot;        - &quot;15666:15672&quot;        hostname: rabbitmq3        volumes:        - rabbitmq_volume:/var/lib/rabbitmq        deploy:        replicas: 1        placement:            constraints:            - node.hostname == ip-172-31-1-3        resources:            limits:            cpus: &#39;1&#39;            memory: 500MB            reservations:            cpus: &#39;0.5&#39;            memory: 50MB        restart_policy:            condition: on-failure</code></pre><ul><li><code>image: tungtv202/rabbitmq_ha_qq</code> : docker image mà mình đã build sẵn</li><li><code>rabbitmq_volume</code> : tạo volume để persistent data. (trường hợp bạn lab bị lỗi gì đó, thì nên xóa volume đi, rồi tạo<br>lại volume mới)</li><li>ports: route thêm port nếu cần thêm public port nào đó khác</li><li><code>replicas: 1</code> : chỉ cần 1 container trên mỗi node là đủ.</li><li>Sử dụng <code>constraints.constraints</code> để chỉ định các container được deploy trải đều trên 3 node riêng biệt. Thông tin<br>hostname ở đây chính là hostname EC2, mình remind bên trên.</li><li>Vì mình sử dụng docker-swarm triển khai, nên khi chung 1 network, các container sẽ tự hiểu các hostname của nhau. (<br>rabbitmq1, rabbitmq2, rabbitmq3). Trường hợp bạn không sử dụng docker swarm, có thể sửa file trên thành file docker<br>compose. Và thêm thông tin</li></ul><pre><code>        extra_hosts:         - rabbitmq1:172.31.11.205         - rabbitmq2:172.31.3.230         - rabbitmq3:172.31.1.3</code></pre><ul><li><p>Cuối bài mình có share source file config sẵn 3 file <code>docker-compose.yml</code> để chạy độc lập trên 3 instance. Trong<br>trường hợp bạn không sử dụng docker-swarm, docker stack</p></li><li><p>Chạy <code>docker stack</code> để triển khai service</p></li></ul><pre><code class="bash">    docker stack deploy --compose-file docker-compose.yml rabbitmq</code></pre><h4 id="4-Kiem-tra-ket-qua"><a href="#4-Kiem-tra-ket-qua" class="headerlink" title="4). Kiểm tra kết quả"></a>4). Kiểm tra kết quả</h4><ul><li><p>docker</p><ul><li>node1<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node1.JPG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node1.JPG"></li><li>node2<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node2.JPG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node2.JPG"></li><li>node3<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node3.JPG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node3.JPG"></li></ul></li><li><p>Web admin</p><ul><li>172.31.11.205:15672</li><li>172.31.3.230:15677</li><li>172.31.1.3:15666<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/webadmin.JPG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/webadmin.JPG"></li><li>Thông tin các node trong cluster được show ở tab <code>Overview</code></li></ul></li><li><p>Tạo Quorum Queue</p><ul><li>Vào tab Queues để tạo queue, và trải nghiệm bật tắt các node. Để test việc Hight Avalibility của queue<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/quorumadmin.JPG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/quorumadmin.JPG"></li><li>Lưu ý: chọn bất kỳ 1 node để làm leader cho queue. (không quan trọng, sau này có sự cố tự động cluster sẽ bầu lại<br>leader mới)</li></ul></li></ul><h4 id="5-Mot-vai-command-de-debug-loi-trong-qua-trinh-setup"><a href="#5-Mot-vai-command-de-debug-loi-trong-qua-trinh-setup" class="headerlink" title="5.) Một vài command để debug lỗi trong quá trình setup"></a>5.) Một vài command để debug lỗi trong quá trình setup</h4><ul><li>Check log container<pre><code class="bash">docker logs CONTAINER_ID</code></pre></li><li>Truy cập vào container và sử dụng <code>rabbitmqctl</code> cli. Tham<br>khảo <a href="https://www.rabbitmq.com/rabbitmqctl.8.html">https://www.rabbitmq.com/rabbitmqctl.8.html</a><pre><code class="bash">rabbitmq statusepmd -port 4369 -names</code></pre></li></ul><h2 id="3-Lab-mot-so-kich-ban"><a href="#3-Lab-mot-so-kich-ban" class="headerlink" title="3. Lab một số kịch bản"></a>3. Lab một số kịch bản</h2><h3 id="3-1-Co-node-bi-down-va-reUp"><a href="#3-1-Co-node-bi-down-va-reUp" class="headerlink" title="3.1 Có node bị down&#x2F; và reUp"></a>3.1 Có node bị down&#x2F; và reUp</h3><ul><li>Kịch bản này khá đơn giản, mình thấy không có gì phức tạp. Bạn có thể stop container. Hoặc scale service&#x3D;0 để test.</li><li>Ví dụ ban đầu Queue A, có node master <code>rabbit@rabbitmq1</code>, sau đó stop container trên node1. Thì node master được<br>chuyển sang node2, hoặc node3. Và message không bị mất</li></ul><h3 id="3-2-Network-parttion"><a href="#3-2-Network-parttion" class="headerlink" title="3.2 Network parttion"></a>3.2 Network parttion</h3><ul><li><p>Kịch bản này có thể tái hiện bằng cách “drop network” giữa node3 vs 2 node còn lại. Mình dùng aws ec2, nên vào sửa<br>Secure Group là được. Hoặc không bạn có thể tạo firewall trên các node. Để chặn, không cho network kết nối.</p></li><li><p>(1) Nếu trong file <code>rabbitmq-qq.conf</code> mình không có cấu hình <code>cluster_partition_handling = pause_minority</code> thì kịch<br>bản sau sẽ diễn ra:</p><ul><li><p>node3 nghĩ rằng 2 node kia down. Nó tự nó làm leader của cluster đó.</p></li><li><p>cụm node1 + node2, nghĩ rằng node3 down. 2 thằng này tự bầu nhau làm leader.</p></li><li><p>Vấn đề này gọi là <code>split-brain</code></p></li><li><p>Và khi client tạo queue mới, hoặc ghi message vào queue trên node3. Thì sẽ không có đồng bộ data tương ứng với cụm<br>node1+node2. Và ngược lại.</p></li><li><p>Lúc này cả 2 phe node3, và node1+node2 đều nghĩ rằng bên kia down. Chứ chưa phát hiện ra sự cố <code>network partition</code><br>. Chỉ tới khi chúng ta cho thông lại network giữa 3 node với nhau. Lúc này cluster mới phát hiện được. (Rabbitmq<br>viết rằng, họ sử dụng Mnesia database để phát hiện vấn đề này)</p></li><li><p>Ảnh chụp webadmin của<br>node1+node2 <img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node1_np.PNG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node1_np.PNG"></p></li><li><p>Ảnh chụp webadmin của<br>node3 <img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node3_np.PNG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/node3_np.PNG"></p></li><li><p><code>Hướng xử lý</code> trong tình huống này là gì?. Bạn phải chọn 1 bên làm chuẩn. Sau đó restart lại rabbitmq bên còn lại,<br>để rabbbitmq bên còn lại rejoin lại cluster. Và đồng bộ lại message từ bên chuẩn sang. Và chấp nhận việc mất data.<br>Reference <a href="https://www.rabbitmq.com/partitions.html#recovering">https://www.rabbitmq.com/partitions.html#recovering</a></p></li></ul></li><li><p>(1) Nếu trong file <code>rabbitmq-qq.conf</code> mình CÓ cấu hình <code>cluster_partition_handling = pause_minority</code> thì kịch bản sau<br>sẽ diễn ra:</p><ul><li>node3 thấy nó chỉ có mình nó. Cả cluster khai báo 3 node. Vì 1 bé hơn 2. Nên rabbitmq trên node3 cho down luôn.<br>Còn cụm node1+node2 vẫn chạy bình thường. (lưu ý là rabbitmq bị shutdown, chứ container vẫn chạy bình thường. Có<br>thể kiểm tra bằng cách sử dụng <code>rabbitmqctl</code>)</li><li>Lúc này việc route từ client sẽ được chuyển về cụm node1+node2. (Để việc route này diễn ra đọc tiếp phần 4)</li></ul></li></ul><h2 id="4-Setup-Nginx"><a href="#4-Setup-Nginx" class="headerlink" title="4. Setup Nginx"></a>4. Setup Nginx</h2><ul><li><p>Khi mình sử dụng java springboot cấu hình rabbitmq client. Mình chỉ cần khai báo danh sách các broker các rabbitmq1,<br>rabbitmq2, rabbitmq3 là được. Và thư viện tự động route cho mình tới broker đang “available”.</p><ul><li>Đây là log của application khi có node bị down. Như bạn thấy thì nó ERROR báo shutdown, xong lập tức restart lại<br>để kết nối tới broker khác<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/rbmq_spring.PNG" alt="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/rbmq_spring.PNG"></li><li>Nếu sử dụng spring boot cấu hình rabbit, thì rất đơn giản</li></ul><pre><code>spring:      rabbitmq:        addresses: 192.168.1.225:5775,192.168.1.245:5776,192.168.1.249:5777        username: tungtv        password: tungtv</code></pre></li><li><p>Trường hợp thư viện không hỗ trợ, chúng ta cần 1 endpoint đứng ngoài hứng. Và check trước khi route vào broker đang<br>available.</p></li><li><p>Có thể sử dụng nginx. Với cấu hình đơn giản sau</p><pre><code>    events &#123;    &#125;    stream &#123;    upstream myrabbit &#123;        server 172.31.11.205:5672;        server 172.31.3.230:5677;        server 172.31.1.3:5666;    &#125;    server &#123;        listen 5000;        proxy_pass myrabbit;    &#125;    &#125;</code></pre></li></ul><h2 id="Bonus"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus</h2><p>(cái này mình chưa lab)</p><ul><li>Cân nhắc khi sử dụng Quorum Queue cho Fanout Exchange. (Vì bộ nhớ để chứa message được nhân lên rất nhiều &#x3D;&gt; tốn<br>resource)</li><li>Mặc định message trên Quorum Queue lưu trên memory&#x2F;disk mãi mãi. Cần setup giới hạn (và hệ thống 3rd giám sát) để khi<br>tới ngưỡng, rabbitmq release tài nguyên<ul><li><code>x-max-in-memory-length</code> sets a limit as a number of messages. Must be a non-negative integer.</li><li><code>x-max-in-memory-bytes</code> sets a limit as the total size of message bodies (payloads), in bytes. Must be a<br>non-negative integer.</li></ul></li><li>Source code <a href="https://github.com/tungtv202/ops_rabbitmq_ha_qq">https://github.com/tungtv202/ops_rabbitmq_ha_qq</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> rabbitmq </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ha </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> quorum queue </tag>
            
            <tag> stories </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Spring Security</title>
      <link href="/2020/07/Java/SpringSecurity/"/>
      <url>/2020/07/Java/SpringSecurity/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Ly-thuyet"><a href="#1-Ly-thuyet" class="headerlink" title="1. Lý thuyết"></a>1. Lý thuyết</h1><ul><li>Nếu sử dụng đồng thời HttpSecurity và dùng <code>@PreAuthorize</code>, thì <code>HttpSecurity</code> sẽ được chạy trước</li><li>HttpSecurity khai báo dựa trên endpoint url, còn @PreAuthorize khai báo trước method</li><li><code>@PreAuthorize</code> sử dụng <code>SPEL</code> (Spring Expression Language)</li><li><code>@PostAuthorize</code> được assessed sau khi method được execute xong</li><li>Expressions<pre><code>hasRole, hasAnyRolehasAuthority, hasAnyAuthoritypermitAll, denyAllisAnonymous, isRememberMe, isAuthenticated, isFullyAuthenticatedprincipal, authenticationhasPermission</code></pre></li><li>@PreFilter and @PostFilter</li></ul><pre><code class="java">@PostFilter(&quot;filterObject.assignee == authentication.name&quot;)List&lt;Task&gt; findAll()&#123;    ...    &#125;///@PostFilter(&quot;hasRole(&#39;MANAGER&#39;) or filterObject.assignee == authentication.name&quot;)List&lt;Task&gt; findAll()&#123;    // ...    &#125;///@PreFilter(&quot;hasRole(&#39;MANAGER&#39;) or filterObject.assignee == authentication.name&quot;)Iterable&lt;Task&gt; save(Iterable&lt;Task&gt; entities)&#123;    // ...    &#125;</code></pre><h1 id="2-Code-template"><a href="#2-Code-template" class="headerlink" title="2. Code template"></a>2. Code template</h1><h2 id="2-1-Spring-security-vs-JWT"><a href="#2-1-Spring-security-vs-JWT" class="headerlink" title="2.1. Spring security vs JWT"></a>2.1. Spring security vs JWT</h2><ul><li><code>WebSecurityConfig.java</code></li></ul><pre><code class="java">@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(prePostEnabled = true)public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123;    @Autowired    private JwtAuthenticationEntryPoint unauthorizedHandler;    @Autowired    private JwtTokenUtil jwtTokenUtil;    @Autowired    private StoreRepository storeDao;    @Autowired    private JwtProperty jwtProperty;    @Autowired    public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception &#123;        //auth.userDetailsService(jwtUserDetailsService).passwordEncoder(passwordEncoderBean());    &#125;    @Bean    @Override    public AuthenticationManager authenticationManagerBean() throws Exception &#123;        return super.authenticationManagerBean();    &#125;    @Override    protected void configure(HttpSecurity httpSecurity) throws Exception &#123;        httpSecurity.cors().and()            // we don&#39;t need CSRF because our token is invulnerable            .csrf().disable()            .exceptionHandling().authenticationEntryPoint(unauthorizedHandler).and()            // don&#39;t create session            .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS).and()            .authorizeRequests()            .antMatchers(&quot;/api/install/**&quot;).permitAll()            .anyRequest().authenticated();        // Custom JWT based security filter        JwtAuthorizationTokenFilter authenticationTokenFilter = new JwtAuthorizationTokenFilter(storeDao, jwtTokenUtil, jwtProperty.getHeader());        httpSecurity.addFilterBefore(authenticationTokenFilter, UsernamePasswordAuthenticationFilter.class);        httpSecurity.headers().frameOptions().disable();    &#125;    @Bean    public HttpFirewall allowUrlEncodedSlashHttpFirewall() &#123;        StrictHttpFirewall firewall = new StrictHttpFirewall();        firewall.setAllowUrlEncodedSlash(true);        firewall.setAllowSemicolon(true);        return firewall;    &#125;    @Bean    public CorsConfigurationSource corsConfigurationSource() &#123;        CorsConfiguration configuration = new CorsConfiguration();        configuration.setAllowedOrigins(Arrays.asList(&quot;*&quot;));        configuration.setAllowedMethods(Arrays.asList(&quot;GET&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;PATCH&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot;));        configuration.setAllowedHeaders(Arrays.asList(&quot;authorization&quot;, &quot;content-type&quot;, &quot;x-auth-token&quot;));        configuration.setExposedHeaders(Arrays.asList(&quot;x-auth-token&quot;));        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();        source.registerCorsConfiguration(&quot;/**&quot;, configuration);        return source;    &#125;//    @Override//    public void configure(WebSecurity web) throws Exception &#123;//        super.configure(web);//        web.httpFirewall(allowUrlEncodedSlashHttpFirewall());//        // AuthenticationTokenFilter will ignore the below paths//        web.ignoring().antMatchers(//                HttpMethod.GET,//                &quot;/favicon.ico&quot;,//                &quot;/robots.txt&quot;,//                &quot;/**/*.css&quot;,//                &quot;/**/*.js&quot;//        );//    &#125;&#125;</code></pre><ul><li><code>JwtAuthorizationTokenFilter.java</code></li></ul><pre><code class="java">public class JwtAuthorizationTokenFilter extends OncePerRequestFilter &#123;    private JwtTokenUtil jwtTokenUtil;    private String tokenHeader;    private StoreRepository storeDao;    public JwtAuthorizationTokenFilter(StoreRepository storeDao, JwtTokenUtil jwtTokenUtil, String tokenHeader) &#123;        this.storeDao = storeDao;        this.jwtTokenUtil = jwtTokenUtil;        this.tokenHeader = tokenHeader;    &#125;    @Override    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws ServletException, IOException &#123;        if (StringUtils.equals(request.getMethod().toLowerCase(), &quot;options&quot;)) &#123;            response.setStatus(200);            response.setHeader(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;);            response.setHeader(&quot;Access-Control-Max-Age&quot;, &quot;600&quot;);            response.setHeader(&quot;Access-Control-Allow-Headers&quot;, &quot;*&quot;);            response.setHeader(&quot;Access-Control-Allow-Methods&quot;, &quot;*&quot;);            return;        &#125;        final String requestHeader = request.getHeader(this.tokenHeader);        String username = null;        if (requestHeader != null) &#123;            try &#123;                username = jwtTokenUtil.getUsernameFromToken(requestHeader);            &#125; catch (IllegalArgumentException | JwtException ignored) &#123;            &#125;        &#125;        if (username != null &amp;&amp; SecurityContextHolder.getContext().getAuthentication() == null) &#123;            val store = storeDao.findbyStoreAlias(username);            if (store != null) &#123;                // It is not compelling necessary to load the use details from the database. You could also store the information                // in the token and read it from it. It&#39;s up to you ;)                UserDetails userDetails = new JwtUser(store.getId(), username);                // For simple validation it is completely sufficient to just check the token integrity. You don&#39;t have to call                // the database compellingly. Again it&#39;s up to you ;)                if (jwtTokenUtil.validateToken(requestHeader, userDetails)) &#123;                    UsernamePasswordAuthenticationToken authentication = new UsernamePasswordAuthenticationToken(userDetails, null, userDetails.getAuthorities());                    authentication.setDetails(new WebAuthenticationDetailsSource().buildDetails(request));                    SecurityContextHolder.getContext().setAuthentication(authentication);                &#125;            &#125;        &#125;        chain.doFilter(request, response);    &#125;&#125;</code></pre><ul><li><code>JwtTokenUtil.java</code></li></ul><pre><code class="java">@Componentpublic class JwtTokenUtil implements Serializable &#123;    private static final long serialVersionUID = -3301605591108950415L;    private final JwtProperty jwtProperty;    private Clock clock = DefaultClock.INSTANCE;    @Autowired    public JwtTokenUtil(JwtProperty jwtProperty) &#123;        this.jwtProperty = jwtProperty;    &#125;    public String getUsernameFromToken(String token) &#123;        return getClaimFromToken(token, Claims::getSubject);    &#125;    public Date getIssuedAtDateFromToken(String token) &#123;        return getClaimFromToken(token, Claims::getIssuedAt);    &#125;    public Date getExpirationDateFromToken(String token) &#123;        return getClaimFromToken(token, Claims::getExpiration);    &#125;    public &lt;T&gt; T getClaimFromToken(String token, Function&lt;Claims, T&gt; claimsResolver) &#123;        final Claims claims = getAllClaimsFromToken(token);        return claimsResolver.apply(claims);    &#125;    private Claims getAllClaimsFromToken(String token) &#123;        return Jwts.parser()            .setSigningKey(jwtProperty.getSecret())            .parseClaimsJws(token)            .getBody();    &#125;    private Boolean isTokenExpired(String token) &#123;        final Date expiration = getExpirationDateFromToken(token);        return expiration.before(clock.now());    &#125;    private Boolean isCreatedBeforeLastPasswordReset(Date created, Date lastPasswordReset) &#123;        return (lastPasswordReset != null &amp;&amp; created.before(lastPasswordReset));    &#125;    private Boolean ignoreTokenExpiration(String token) &#123;        // here you specify tokens, for that the expiration is ignored        return false;    &#125;    public String generateToken(UserDetails userDetails) &#123;        Map&lt;String, Object&gt; claims = new HashMap&lt;&gt;();        return doGenerateToken(claims, userDetails.getUsername());    &#125;    private String doGenerateToken(Map&lt;String, Object&gt; claims, String subject) &#123;        final Date createdDate = clock.now();        final Date expirationDate = calculateExpirationDate(createdDate);        return Jwts.builder()            .setClaims(claims)            .setSubject(subject)            .setIssuedAt(createdDate)            .setExpiration(expirationDate)            .signWith(SignatureAlgorithm.HS256, jwtProperty.getSecret())            .compact();    &#125;    public Boolean canTokenBeRefreshed(String token) &#123;        return !isTokenExpired(token) || ignoreTokenExpiration(token);    &#125;    public String refreshToken(String token) &#123;        final Date createdDate = clock.now();        final Date expirationDate = calculateExpirationDate(createdDate);        final Claims claims = getAllClaimsFromToken(token);        claims.setIssuedAt(createdDate);        claims.setExpiration(expirationDate);        return Jwts.builder()            .setClaims(claims)            .signWith(SignatureAlgorithm.HS256, jwtProperty.getSecret())            .compact();    &#125;    public Boolean validateToken(String token, UserDetails userDetails) &#123;        JwtUser user = (JwtUser) userDetails;        final String username = getUsernameFromToken(token);        return (username.equals(user.getUsername()) &amp;&amp; !isTokenExpired(token));    &#125;    private Date calculateExpirationDate(Date createdDate) &#123;        return new Date(createdDate.getTime() + jwtProperty.getExpiration() * 1000);    &#125;&#125;</code></pre><p>.</p><h2 id="2-2-Basic-Security"><a href="#2-2-Basic-Security" class="headerlink" title="2.2. Basic Security"></a>2.2. Basic Security</h2><ul><li><code>HttpBasicConfig.java</code></li></ul><pre><code class="java">@Configuration@EnableWebSecurity@Order(2)public class HttpBasicConfig extends WebSecurityConfigurerAdapter &#123;    private static final String[] PUBLIC_RESOURCES = new String[]&#123;        &quot;/admin/payment-integrations/momo/ipn-listener&quot;,        &quot;/admin/payment-integrations/momo/ipn-listener.json&quot;,        &quot;/admin/payment-integrations/zpay/update_merchant&quot;,        &quot;/admin/payment-integrations/zpay/update_merchant.json&quot;,        &quot;/admin/momoaccuracy&quot;,        &quot;/admin/momoaccuracy.json&quot;    &#125;;    private static final String[] PRIVATE_RESOURCES = new String[]&#123;        &quot;/admin/payment-integrations/**&quot;    &#125;;    @Autowired    private UserConfig config;    @Autowired    private UserDetailsService userDetailsService;    @Autowired    private SessionUserService sessionUserService;    @Autowired    private PasswordEncoder passwordEncoder;    @Autowired    private CustomAccessDeniedHandler accessDeniedHandler;    @Autowired    private AuthenticationExceptionHandler authenticationExceptionHandler;    @Override    protected void configure(HttpSecurity http) throws Exception &#123;        http.requestMatcher(new BasicRequestMatcher(sessionUserService))            .authorizeRequests()            .antMatchers(HttpMethod.GET, PRIVATE_RESOURCES).access(genRole(&quot;read_orders&quot;))            .antMatchers(HttpMethod.POST, PRIVATE_RESOURCES).access(genRole(&quot;write_orders&quot;))            .antMatchers(HttpMethod.PUT, PRIVATE_RESOURCES).access(genRole(&quot;write_orders&quot;))            .antMatchers(HttpMethod.DELETE, PRIVATE_RESOURCES).access(genRole(&quot;write_orders&quot;))            .antMatchers(&quot;/**&quot;).hasRole(config.getRole())            .and().sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS)            .and().csrf().disable().httpBasic()            .and().exceptionHandling().accessDeniedHandler(accessDeniedHandler).authenticationEntryPoint(authenticationExceptionHandler);    &#125;    private String genRole(String role) &#123;        return String.format(&quot;hasRole(&#39;%s&#39;) or hasAuthority(&#39;%s&#39;)&quot;, config.getRole(), role);    &#125;    @Autowired    public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception &#123;        inMemoryConfigurer()            .passwordEncoder(passwordEncoder)            .withUser(config.getName()).password(config.getPassword()).roles(config.getRole()).and()            .configure(auth);        auth.userDetailsService(userDetailsService);    &#125;    private InMemoryUserDetailsManagerConfigurer&lt;AuthenticationManagerBuilder&gt; inMemoryConfigurer() &#123;        return new InMemoryUserDetailsManagerConfigurer&lt;&gt;();    &#125;    @Override    public void configure(WebSecurity web) throws Exception &#123;        super.configure(web);        web.ignoring().antMatchers(HttpMethod.POST, PUBLIC_RESOURCES);    &#125;&#125;</code></pre><ul><li><code>SessionUserServiceImpl.java</code></li></ul><pre><code class="java">@Servicepublic class SessionUserServiceImpl implements SessionUserService &#123;    @Autowired    @Qualifier(&quot;redis_template_common&quot;)    private StringRedisTemplate redisCommonTemplate;    @Autowired    @Qualifier(&quot;json&quot;)    private ObjectMapper json;    @Autowired    private UserDao userDao;    private ValueOperations&lt;String, String&gt; hashValue;    @PostConstruct    private void init() &#123;        hashValue = redisCommonTemplate.opsForValue();    &#125;    @Override    public CurrentUser getUser(Cookie cookie) &#123;        if (cookie != null &amp;&amp; cookie.getValue() != null) &#123;            String jsonStr = hashValue.get(cookie.getValue());            try &#123;                if (!StringUtils.isEmpty(jsonStr)) &#123;                    SessionModel sessionModel = json.readValue(jsonStr, SessionModel.class);                    if (sessionModel != null) &#123;                        vn.z.service.generic.domain.User domainUser = userDao                            .getByEmail(sessionModel.getUsername(), sessionModel.getStoreId());                        if (domainUser != null) &#123;                            User user = new User();                            user.setId(domainUser.getId());                            user.setStoreId(sessionModel.getStoreId());                            user.setEmail(sessionModel.getUsername());                            user.setPassword(domainUser.getPassword());                            user.setPermissions(domainUser.getPermissions());                            user.setFirstName(domainUser.getFirstName());                            user.setLastName(domainUser.getLastName());                            user.setEmployee(sessionModel.getEmployee());                            user.setEmpoyeeSource(sessionModel.getEmployeeSource());                            return new CurrentUser(user);                        &#125;                    &#125;                &#125;            &#125; catch (Exception e) &#123;                // unhandled            &#125;        &#125;        return null;    &#125;    @Override    public boolean containCookie(String s) &#123;        return redisCommonTemplate.hasKey(s);    &#125;&#125;</code></pre><pre><code class="java">@Getter@Setterpublic class SessionModel &#123;    private int storeId;    private String username;    private String employee;    private String employeeSource;&#125;</code></pre><ul><li><code>AuthenticationExceptionHandler.java</code></li></ul><pre><code class="java">@Componentpublic class AuthenticationExceptionHandler implements AuthenticationEntryPoint, Serializable &#123;    @Autowired    private ObjectMapper jsonMain;    @Override    public void commence(HttpServletRequest httpServletRequest, HttpServletResponse response, AuthenticationException e) throws IOException, ServletException &#123;        response.setStatus(HttpStatus.UNAUTHORIZED.value());        response.setContentType(MediaType.APPLICATION_JSON_UTF8_VALUE);        response.getWriter().write(jsonMain.writeValueAsString(            ErrorModel.newInstance().add(&quot;base&quot;, HttpStatus.UNAUTHORIZED.getReasonPhrase())));    &#125;&#125;</code></pre><ul><li><code>CustomAccessDeniedHandler.java</code></li></ul><pre><code class="java">@Componentpublic class CustomAccessDeniedHandler implements AccessDeniedHandler &#123;    @Autowired    private ObjectMapper jsonMain;    @Override    public void handle(HttpServletRequest httpServletRequest, HttpServletResponse response, AccessDeniedException e) throws IOException, ServletException &#123;        response.setStatus(HttpStatus.FORBIDDEN.value());        response.setContentType(MediaType.APPLICATION_JSON_UTF8_VALUE);        response.getWriter().write(jsonMain.writeValueAsString(            ErrorModel.newInstance().add(&quot;base&quot;, HttpStatus.FORBIDDEN.getReasonPhrase())));    &#125;&#125;</code></pre><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><p><img src="https://www.marcobehler.com/images/filterchain-1a.png" alt="15Filter"></p><ul><li><a href="https://www.marcobehler.com/guides/spring-security">Spring Security: Authentication and Authorization In-Depth</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> spring </tag>
            
            <tag> security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka - Rebalance Protocol</title>
      <link href="/2020/07/Kafka/Kafka_RebalanceProtocol/"/>
      <url>/2020/07/Kafka/Kafka_RebalanceProtocol/</url>
      
        <content type="html"><![CDATA[<h2 id="Rebalancing"><a href="#Rebalancing" class="headerlink" title="Rebalancing"></a>Rebalancing</h2><ul><li>When do we rebalance?<ul><li>Member “dies” (doesn’t heartbeat for “long” period of time)</li><li>Member leaves</li><li>New member joins</li><li>Topic metada or subscription changes</li></ul></li><li>How do we discover we need to reblance?<ul><li>While polling<ul><li>HearbeatResponse (REBALANCE_IN_PROGRESS)</li><li>CommitOffset (REBALANCE_IN_PROGRESS)</li></ul></li><li>Initiate a rebalance by re-joinning</li></ul></li><li>When we rebalance - stop everything and rejoin</li><li>Sau khi process msg xong, sẽ <code>revokerd</code>, sau đó lại <code>assigned</code> lại 1 chu kỳ mới.</li></ul><h2 id="Rebalance-protocol"><a href="#Rebalance-protocol" class="headerlink" title="Rebalance protocol"></a>Rebalance protocol</h2><ul><li>FindCoordinator</li><li>JoinGroup<ul><li>Config:<ul><li>session.timeout.ms: The timeout used to detect consumer failures when using Kafka’s group management facility.<br>The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received<br>by the broker before the expiration of this session timeout, then the broker will remove this consumer from<br>the group and initiate a rebalance</li><li>max.poll.interval.ms: The maximum delay between invocations of poll() when using consumer group management.<br>This places an upper bound on the amount of time that the consumer can be idle before fetching more records.<br>If poll() is not called before expiration of this timeout, then the consumer is considered failed and the<br>group will rebalance in order to reassign the partitions to another member.<br>(<a href="https://stackoverflow.com/questions/39730126/difference-between-session-timeout-ms-and-max-poll-interval-ms-for-kafka-0-10">https://stackoverflow.com/questions/39730126/difference-between-session-timeout-ms-and-max-poll-interval-ms-for-kafka-0-10</a>)</li></ul></li><li>Được <code>coordinator</code> sử dụng để kick member ra khỏi group, nếu nó không có phản hồi</li></ul></li><li>SyncGroup</li><li>Heartbeat<ul><li>Định kỳ consumer gửi <code>heartbeat</code> về cho <code>coordinator</code> để duy trì session active (heartbeat.interval.ms)</li></ul></li><li>LeaveGroup<ul><li>Được consumer gửi tới coordinator trước khi stop</li><li>Sau khi leaveGroup, thì cần thực hiện lại JoinGroup, SyncGroup cho lần sau</li></ul></li></ul><h2 id="Cong-dung-Rebalance"><a href="#Cong-dung-Rebalance" class="headerlink" title="Công dụng Rebalance"></a>Công dụng Rebalance</h2><ul><li>Confluent Schema Registry sử dụng rebalace protocol để chọn leader node</li><li>Kafka Connect sử dụng rebalace protocol để phấn bố các tasks và connectors một cách phù hợp trên các workers node</li><li>Kafka Stream sử dụng rebalace protocol để gán tasks và partitions đến các instances</li></ul><h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><ul><li>Static Membership<ul><li>consumer instance sẽ được định danh bởi <code>group.instance.id</code></li><li>Khi sảy ra sự cố tạm thời, làm <code>transient failures</code>, thì coordinator sẽ không reblance ngay lập tức cho các<br>consumer khác, mà nó sẽ đợi cho tới khi hết <code>session timeout</code> của consumer đang xảy ra lỗi tạm thời.</li><li>Vì được đinh danh, nên khi consumer hết lỗi, comeback, sẽ không cần phải yêu cầu joinGroup, lại nữa. Bộ<br>coordinator sẽ trả cache về cho consumer</li><li>Yêu cầu là consumer khi lỗi, không được gửi request <code>leaveGroup</code>, và nên tăng <code>sesssion timeout</code> lên</li><li>Ưu điểm: tránh việc rebalance không cần thiết</li><li>Nhược điểm: tăng tính <code>unavailability</code> của <code>partition</code>, vì <code>coordinator</code> phải đợi tới hết session timeout mới phát<br>hiện ra lỗi.</li></ul></li><li>Incremental Cooperative Rebalancing<ul><li>The Incremental Cooperative Rebalancing attempts to solve this problem in two ways :<ul><li>only stop tasks&#x2F;members for revoked resources.</li><li>handle temporary imbalances in resource distribution among members, either immediately or deferred (useful for<br>rolling restart).</li></ul></li><li>For doing that, the Incremental Cooperative Rebalancing principal is actually declined into three concrete<br>designs:<ul><li>Design I: Simple Cooperative Rebalancing</li><li>Design II: Deferred Resolution of Imbalance</li><li>Design III: Incremental Resolution of Imbalance</li></ul></li></ul></li></ul><h2 id="Coding-Template"><a href="#Coding-Template" class="headerlink" title="Coding Template"></a>Coding Template</h2><ul><li>Quản lý việc commit offset manual<ul><li>Nếu không commitAsync manual chủ động trước</li></ul></li></ul><pre><code class="java">@KafkaListener(            topics = &quot;$&#123;kafka.app.backup.product.manual.topic&#125;&quot;,            groupId = &quot;$&#123;kafka.app.backup.product.manual.group&#125;&quot;,            concurrency = &quot;$&#123;kafka.app.backup.product.manual.thread&#125;&quot;    )    public void productManualListen(ConsumerRecord&lt;String, String&gt; record, Consumer&lt;?, ?&gt; consumer) &#123;        consumer.commitAsync();        productBackupStrategy.doBackup(backupLogDetailId);    &#125;</code></pre><ul><li>Cấu hình Consumer</li></ul><pre><code class="java">@Configuration@EnableKafkapublic class KafkaConfig &#123;    @Value(&quot;$&#123;kafka.broker.address&#125;&quot;)    private String kafkaServer;    @Bean    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactory(@Qualifier(&quot;json&quot;) ObjectMapper objectMapper) &#123;        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory =                new ConcurrentKafkaListenerContainerFactory&lt;&gt;();        factory.setConsumerFactory(consumerFactory());        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.BATCH);        factory.getContainerProperties().setAckOnError(true);        factory.getContainerProperties().setSyncCommits(true);        factory.setMessageConverter(new StringJsonMessageConverter(objectMapper));        factory.setErrorHandler(new SeekToCurrentErrorHandlerCustom());        factory.getContainerProperties().setConsumerRebalanceListener(myConsumerRebalanceListener());        return factory;    &#125;    @Bean    public CustomRebalance myConsumerRebalanceListener() &#123;        return new CustomRebalance() &#123;        &#125;;    &#125;    @Bean    public ConsumerFactory&lt;String, String&gt; consumerFactory() &#123;        return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs());    &#125;    @Bean    public Map&lt;String, Object&gt; consumerConfigs() &#123;        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServer);        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, org.apache.kafka.common.serialization.StringDeserializer.class);        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, org.apache.kafka.common.serialization.StringDeserializer.class);        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;latest&quot;);//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 10 * 1024 * 1024);//        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 10 * 1000);        props.put(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG, 10 * 1000);        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 10 * 1000);        return props;    &#125;&#125;</code></pre><ul><li></li></ul><pre><code class="java">public class CustomRebalance implements ConsumerRebalanceListener &#123;    @Override    public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; collection) &#123;        System.out.println(&quot;TUNGTUNG revokerd&quot;);    &#125;    @Override    public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; collection) &#123;        System.out.println(&quot;TUNGTUNG assigned&quot;);    &#125;&#125;</code></pre><h2 id="Linh-tinh"><a href="#Linh-tinh" class="headerlink" title="Linh tinh"></a>Linh tinh</h2><p>Chưa hiểu tại sao khi cấu hình</p><pre><code>        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 5 * 1000);        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;latest&quot;);</code></pre><p>tức 5s sẽ auto commit, nhưng nếu consumer xử lý msg hết hơn &gt;5s, thì vẫn bị rebalance msg. Trong khi nếu chủ<br>động <code>onsumer.commitAsync();</code> thì không bị reblance????</p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
            <tag> rebalance </tag>
            
            <tag> coordinator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS - Note Total</title>
      <link href="/2020/06/AWS/AWS_Note/"/>
      <url>/2020/06/AWS/AWS_Note/</url>
      
        <content type="html"><![CDATA[<h1 id="AWS-ElasticCache"><a href="#AWS-ElasticCache" class="headerlink" title="AWS ElasticCache"></a>AWS ElasticCache</h1><p>&#x2F;&#x2F; Dịch vụ cung cấp Redis, NameCached trên AWS</p><h2 id="1-1-Su-khac-biet-giua-Memcached-voi-Redis"><a href="#1-1-Su-khac-biet-giua-Memcached-voi-Redis" class="headerlink" title="1.1 Sự khác biệt giữa Memcached với Redis"></a>1.1 Sự khác biệt giữa Memcached với Redis</h2><p>3 khác biệt cơ bản</p><ul><li>Memcached thì cung cấp dữ liệu dạng key&#x2F;value đơn giản hơn. Đơn giản chỉ là chuỗi string hoặc binary data. Trong khi<br>Redis thì phức tạp hơn, nó còn có set, lists, zset, hash… dữ liệu có nhiều loại hơn, có tổ chức hơn, có sort, có rank.</li><li>Memcached thì không hỗ trợ persis data (dữ liệu không được save, không được snapshoot, 1 khi đã mất là mất). Trong khi<br>Redis thì có cơ chế backup, có cơ chế snapshoot và lưu vào disk, có thể tạo ra 1 cluster Redis mới bằng việc khởi tạo<br>từ bản snapshoot được lưu.</li><li>Memcached thì có thể dựng nhiều node (trong sách thấy ghi là tối đa 20 node) để tạo thành 1 cluster. Còn Redis thì mỗi<br>1 cluster là 1 single node. Nhưng nhiều cluster thì có thể group lại thành 1 nhóm (Redis replication group).</li></ul><h2 id="1-2-Scaling"><a href="#1-2-Scaling" class="headerlink" title="1.2 Scaling"></a>1.2 Scaling</h2><p>ElasticCache cho phép việc scaling theo chiều ngang.<br>Bị hạn chế scaling theo chiều dọc.</p><h2 id="1-3-Khi-co-node-loi-ElastiCache-lam-gi"><a href="#1-3-Khi-co-node-loi-ElastiCache-lam-gi" class="headerlink" title="1.3 Khi có node lỗi, ElastiCache làm gì?"></a>1.3 Khi có node lỗi, ElastiCache làm gì?</h2><p>Nó tự phát hiện ra lỗi, khi có lỗi, nó sẽ thay thế và tự thêm mới 1 cluster. Trong suốt thời gian này, truy vấn sẽ được<br>gửi tới database &#x3D;&gt; database sẽ bị tăng lưu lượng.<br>Riêng với ElastiCache chạy Redis, có 1 tính năng của Amazon là Multi-AZ replication group, nếu tính năng này được bật,<br>thì:</p><ul><li>Khi primary node bị lỗi &#x3D;&gt; read replica sẽ được tự động đẩy lên làm primary</li><li>Node lỗi &#x3D;&gt; sẽ tự động được thay thế bởi 1 node mới. Vậy primary node là gì? Read replica node là gì? Primary node đó<br>là node chạy Redis với đủ quyền read lẫn write, còn read replica thì chỉ có quyền read, 1 con primary có thể có tới 5<br>con read replica, mục đích để san sẻ tải.<br><img src="https://images.viblo.asia/d6e4b83d-c326-4ad2-a2a6-51473680d636.jpg" alt="EC"></li></ul><p>ElastiCache có kết hợp với DNS của Amazon, để khi node mới thay thế node lỗi, thì node mới sẽ có IP, policy của node cũ<br>luôn, ứng dụng của developer sẽ được chạy thông suốt, mà developer không phải config lại cái gì. Lưu ý: việc sao chép<br>data giữa các cluster là không đồng bộ và sẽ mất 1 khoảng thời gian delay.</p><h2 id="1-4-ElastiCache-backup-va-recovery-data-the-nao"><a href="#1-4-ElastiCache-backup-va-recovery-data-the-nao" class="headerlink" title="1.4 ElastiCache backup và recovery data thế nào?"></a>1.4 ElastiCache backup và recovery data thế nào?</h2><p>Với Memcached thì không hỗ trợ tính năng này, khi tạo mới 1 Memcached cluster, thì sẽ luôn là empty data.<br>Với Redis, ElastiCache cung cấp cửa sổ để developer cấu hình lịch định kỳ để tạo các bản snapshot, các file backup này<br>được lưu trữ trong S3. Các bản snapshots thực hiện thủ công manual, sẽ được lưu trữ mãi mãi, cho tới khi bạn xóa nó.</p><h2 id="1-5-Access-Control"><a href="#1-5-Access-Control" class="headerlink" title="1.5 Access Control"></a>1.5 Access Control</h2><p>Về việc cấp quyền access, thì ElastiCache dựa theo NETWORK. Nghĩa là nó dựa vào địa chỉ IP, subnetmask để xây dựng lên<br>các chính sách network, việc access sẽ dựa vào security groups này.<br>Có thể hạn chế xâm nhập vào bằng cách cấu hình trong ACL (Acess List) .<br>Về việc manage, sử dụng service IAM, AWS Identity của Amazon để định nghĩa các chính sách cho các AWS User.</p><h1 id="Amazon-Route-53"><a href="#Amazon-Route-53" class="headerlink" title="Amazon Route 53"></a>Amazon Route 53</h1><p>Là một dịch vụ tên miền DNS</p><h2 id="chuc-nang-chinh"><a href="#chuc-nang-chinh" class="headerlink" title="chức năng chính"></a>chức năng chính</h2><ul><li>Register domain names</li><li>Route internet traffic to the resources for your domain</li><li>Check the health of your resources</li></ul><h2 id="Tinh-nang"><a href="#Tinh-nang" class="headerlink" title="Tính năng"></a>Tính năng</h2><ul><li>Kết nối hiệu quả với EC2, S3, ELB, Cloudfront</li><li>Using Traffic Flow to Route DNS Traffic: Có thể sử dụng để redirect traffic, định tuyến enduser tới endpoint tốt nhất<br>dựa theo: geoproximity, latency, health, and other considerations</li><li>DNS failover: Route 53 tự động phát hiện sự cố của website và redirect truy cập user tới 1 locations khác. Khi bật<br>tính năng này, thì Route 53 sẽ thành 1 helth checking agent, để giám sát tính “availabale” của các endpoint. (Cần tạo<br>policy healcheck, sau đó vào hostzone add record A primary và secondary). Điểm khác biệt với ELB?</li><li>Private DNS for Amazon VPC (A private hosted zone) dùng để làm DNS private cho các service trong cùng 1 VPC.</li><li>Domain Name Registration: trả tiền để thuê tên miền</li></ul><h2 id="2-3-Cac-Routing-policy"><a href="#2-3-Cac-Routing-policy" class="headerlink" title="2.3 Các Routing policy:"></a>2.3 Các Routing policy:</h2><ul><li>Simple routing policy: dùng cho 1 resource, định tuyến domain tới server</li><li>Failover routing policy: dùng để định tuyến traffic khi có server bị deactive, nó sẽ redirect tới server active (DNS<br>Failover)</li><li>Geolocation routing policy: định tuyến theo vị trí địa lý</li><li>Geoproximity routing policy : định tuyến theo vị trí tài nguyên của mình, tùy ý</li><li>Latency routing policy: được sử dụng với Multi Region, dùng để định tuyến tới region có đệ trễ thấp nhất</li><li>Multivalue answer routing policy: định tuyến random, kết quả trả về là 1 trong 8 record của DNS</li><li>Weighted routing policy: định tuyến truy cập tới các server theo hệ số khác nhau mà mình config</li></ul><h1 id="AWS-Lambda"><a href="#AWS-Lambda" class="headerlink" title="AWS Lambda"></a>AWS Lambda</h1><p>Sự khác biệt giữa EC2 và Lambda là gì?</p><ul><li>Lambda là serverless (không có các tài nguyên như RAM, CPU, Disk…), còn EC2 thì ngược lại, nó là server.</li></ul><p>Lambda có công dụng gì?</p><ul><li>Lambda chỉ để chạy các đoạn code đã được developer lập trình. Nghĩa là sẽ không thể cài đặt được software, library,<br>tool của bên thứ 3 như vẫn hay thường cài trên 1 server truyền thống. Có vẻ giống với hosting của php…</li></ul><p>Lambda support những ngôn ngữ gì?</p><ul><li>Hiện tại thì: Node.js (JavaScript), Python, Java (Java 8 compatible), C# (.NET Core).</li></ul><p>Một vài điểm nổi bật của Lambda</p><ul><li>Không phải lo lắng về việc scale server, vì nó là serverless.</li><li>Nó tận dụng được hệ thống Infra của AWS, nên có thể xem là không cần quan tâm tới các tài nguyên vật lý cho ứng dụng.</li><li>Cách giới hạn scale của Lambda là setup giới hạn hóa đơn thanh toán</li><li>Khi deploy code trên Lambda nếu code không chạy, thì sẽ không bị mất tiền</li><li>Code sau khi được deploy lêm Lambda, tùy thuộc vào traffic, và effort của request mà tính tiền.</li><li>Cần phải tính toán về RAM và khai báo cho ứng dụng trước khi chạy. Việc tính toán CPU là tự động.</li><li>Lambda sử dụng BeanStalk (1 dịch vụ khác của AWS) để deploy code. Và code được lưu trữ ở S3</li></ul><h2 id="Use-case"><a href="#Use-case" class="headerlink" title="Use case"></a>Use case</h2><ul><li>combo kết hợp Lambda + API Gateway</li></ul><h1 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h1><ul><li>Dynamic DB: 10 000 beyond</li><li>Stored on SSD storage</li></ul><h1 id="EC2"><a href="#EC2" class="headerlink" title="EC2"></a>EC2</h1><h2 id="EC2-Enhanced-Networking"><a href="#EC2-Enhanced-Networking" class="headerlink" title="EC2 Enhanced Networking"></a>EC2 Enhanced Networking</h2><ul><li>Lợi ích của SR-I&#x2F;OV :<ul><li>Higher Packet per Second (PPS) performance (inter-instance)</li><li>Lower inter-instance latencies</li><li>Very low network jitter</li></ul></li><li>Enhanced networking requirements:<ul><li>Instances be launched from an HVM AMI (not PV)</li><li>Is only supported in a VPC</li></ul></li><li>Enhanced networking is not suppored on all EC2 instances</li><li>Enhanced networking does NOT cost extra</li><li>Enhanced networking can be enabled on Instance-store backed or EBS-backed EC2 instances</li></ul><h2 id="EC2-Bootstrapping"><a href="#EC2-Bootstrapping" class="headerlink" title="EC2 Bootstrapping"></a>EC2 Bootstrapping</h2><ul><li>Là user data</li><li>Pass script chạy sau khi EC2 instance launched (ví dụ như script sau khi chạy thì update OS, run shell script…).</li></ul><h2 id="EC2-Placement-Groups"><a href="#EC2-Placement-Groups" class="headerlink" title="EC2 - Placement Groups"></a>EC2 - Placement Groups</h2><ul><li>Là một tính năng cho phép các EC2 liên quan có thể kết nối với nhau với băng thông cao, độ trễ thấp, trong cùng 1 AZ (<br>cả 3 loại đều cùng 1AZ)</li><li>Yêu cầu cùng 1 AZ</li><li>Có thể sử dụng Enhanced networking instances trong Placement groups</li><li>There are only specific instance type which can be launched inside the placement group</li><li>We cannot move existing instance into placement group, in such case we need to create an AMI of instance and launch<br>new instance from that AMI inside the p group</li><li>Maximum network throughput traffic between two instance in placement group is limited by the slower of the two<br>instance</li><li>Có 3 type tạo placementGroup: cluster, partition, spread.<ul><li>Cluster: cho performance về network cao nhất (do bố trí vật lý gần nhau) &#x3D;&gt; Chỉ triển trong cùng 1 AZ</li><li>spread: là một nhóm các instances được đặt trên phần cứng cơ bản riêng biệt, có thể trải rộng trên nhiều AZ. (phù<br>hợp hệ thống nhỏ nhưng thích HA)</li><li>partition: độc lập. Có thể có tối đa 7 partitions cho mỗi Availability Zone và có thể trải rộng trên nhiều<br>Availability Zones trong cùng một Region.<br>(phù hợp hệ thống lớn cần HA)</li></ul></li><li>The name that we specify for placement group must be unique across your AWS account.</li><li>AWS recommends instances with same type to be launched within a placement group</li><li>We cannot merge placement groups</li><li>Đi kèm với Instance support Enhance Networking. (tức là chỉ instance HVM ?)</li></ul><h2 id="EC2-Bastion-Host"><a href="#EC2-Bastion-Host" class="headerlink" title="EC2 - Bastion Host"></a>EC2 - Bastion Host</h2><ul><li>1 Computer được cấu hình đặc biệt, thuộc miền external&#x2F; public (DMZ) hoặc bên ngoài firewall, hoạt động như một server<br>trung gian, cho phép bạn connect vào các Instance nằm trong Private Subnet</li><li>Trường hợp Instance bị terminated, nhưng Auto Scaling Group đang launches, thì Elastic IP sẽ được đính lên cho<br>instances mới</li></ul><h2 id="EC2-Spot-instances"><a href="#EC2-Spot-instances" class="headerlink" title="EC2 - Spot instances"></a>EC2 - Spot instances</h2><ul><li>Đấu thầu để được chạy instances (các tài nguyên mà AWS đang dư thừa), giá rẻ hơn Instances on-demaind rất nhiều. Tuy<br>nhiên khi nào có người khác trả giá cao hơn giá mình thầu, thì instance đó sẽ bị terminated</li><li>Spot instance không đảm bảo luôn luôn khả dụng, nhưng giá rẻ</li></ul><h2 id="EC2-Rerserved-Instances"><a href="#EC2-Rerserved-Instances" class="headerlink" title="EC2 - Rerserved Instances"></a>EC2 - Rerserved Instances</h2><ul><li>Là Instances on-demand, nhưng có thuê bao, trả trước sẽ có giá rẻ hơn. Ví dụ đảm bảo dùng trong 12 tháng.</li><li>They can be used to launch AS Group instances or standalone ones</li><li>Có thể Reversed theo lịch, ví dụ thuê instance sẽ được chạy auto scale vào 1 ngày hàng tuần…</li></ul><h2 id="EC2-IAM-Roles"><a href="#EC2-IAM-Roles" class="headerlink" title="EC2 - IAM Roles"></a>EC2 - IAM Roles</h2><ul><li>Gán quyền để application access read&#x2F;write S3, SQS, DynamoDB, SNS…</li><li>Default IAM Roles cho phép EC2 instances access vào các service khác</li><li>You can add the IAM role while the instance is running</li></ul><h2 id="EC2-Charge"><a href="#EC2-Charge" class="headerlink" title="EC2 - Charge"></a>EC2 - Charge</h2><ul><li>Được tính tiền từ lúc bắt đầu boot ec2 (không phải là sau khi instand đã start xong). Tới lúc shutdown hoàn toàn</li><li>Mỗi lần bật tắt instance, sẽ bị tính tiền tối thiểu cho 1 tiếng. Ví dụ trong 10 phút, bật tắt instance 2 lần, thì bị<br>tính tiền 2 tiếng sử dụng.</li></ul><h2 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h2><ul><li><p>Một vài lỗi khiến EC2 bị terminate khi launch:</p><ul><li>AMI thiếu 1 số part</li><li>Limit volume EBS</li><li>Bản snapshot EBS bị lỗi</li></ul></li><li><p>Để tìm lỗi termination:</p><ul><li>From Console: Go to Instances (select the instance) -&gt; Description tab -&gt; State Transition reason</li><li>From CLI use the “describe-instance command</li></ul></li><li><p>Lỗi: Insufficient Instance Capacity : aws hết tài nguyên, đợi, hoặc reserved instances</p></li></ul><h1 id="Elastic-Network-Interface"><a href="#Elastic-Network-Interface" class="headerlink" title="Elastic Network Interface"></a>Elastic Network Interface</h1><ul><li>Là card mạng ảo, được đính vào EC2 (vd: eth0, eth1…)</li><li>Khi EC2 bị terminated &#x3D;&gt; Nếu ENI tạo bằng console thì cũng terminate theo, nếu tạo bằng command line thì không bị<br>terminated</li><li>Có thể được cấu hình khi: instance running, stopped, launched</li><li>1 ENI chỉ được cho 1 Insntace, nhưng 1 instance có thể attached nhiều ENI</li><li>Subnet có thể khác nhau nhưng phải chung VPC, chung AZ</li><li>Instance type khác nhau thì sẽ có số ENIs có thể đính vào khác nhau</li></ul><h1 id="RDS"><a href="#RDS" class="headerlink" title="RDS"></a>RDS</h1><ul><li><p>DB instance class maximum size: 6TB</p></li><li><p>Service cung cấp hệ quản trị SQL: MySQL, PostgreSQL…</p></li><li><p>HA luôn được implies Multi-AZ</p></li><li><p>Read Replicas có thể được sử dụng để scale READ performance, tuy nhiên</p><ul><li>Không thể với WRITE</li><li>Có sự bất đồng bộ giữa các node</li></ul></li><li><p>AWS quản lý fully managed service, tức là dev không thể can thiệp được vào OS, instance chạy RDS &#x3D;&gt; chỉ access được<br>vào RDS enginer</p></li><li><p>Primary và standby có thể khác AZs (nhưng phải cùng chung region)</p></li><li><p>Không nên sử dụng IP address làm point để kết nối, mà nên sử dụng endpoint</p></li><li><p>Có thể sử dụng CloudWatch Alarm để monitor metric, và alarm</p></li><li><p>CloudTrail để logs all AWS RDS API</p></li><li><p>Có thể read replica (được với MySQL, MariaDB, PostgreSQL (MyMaPo) )</p></li><li><p>Quá trình scale, hay chuyển giao primary-standby sẽ <code>mất vài phút</code></p></li><li><p>Khi chạy Multi-AZ RDS thì chế độ backups và snapshots sẽ được tự động chạy</p></li><li><p>Không thể read&#x2F;write tới Standby RDS DB instance</p></li><li><p>Tất cả RDS db engines đều có thể có dung lượng lưu trữ lên &gt; 6GB, trừ MS SQL</p></li><li><p>MS SQL DB engine can have storage capacity up to 4TB</p></li><li><p>Không thể giảm size của RDS sau khi chạy, chỉ có thể tăng</p></li><li><p>Amazon RDS Provisioned IOPS Storage được dùng để tăng performance (ứng dụng nào yêu cầu I&#x2F;O cao, thì nên dùng )</p></li><li><p>Việc upgrade version RDS có 2 loại</p><ul><li>Major version Upgrades - admin phải upgrade manual, cant revert</li><li>Minor version Upgrades</li></ul></li><li><p>Không thể restore 1 bản snapshot tới 1 instance đã tồn tại DB (cần tạo mới, và restore vô cái mới)</p></li><li><p>Không thể thay đổi Storage type (magnetic, Provisioned IOPS, General purpose) trong suốt quá trình restore thực thi</p></li><li><p>Nếu set retention period &#x3D; 0, tương đương tắt chế độ automatic backups.</p></li><li><p>Khi bạn restore 1 DB instance, chỉ có các tham số mặc định và Security groups đã được liên kết mới có thể restore</p></li><li><p>Sau lưu tự động hiện tại chỉ support InnoDB , MySQL (không support cho MyISAM)</p></li><li><p>Tính năng khôi phục theo thời gian Point-In-Time chỉ được hỗ trợ cho MySQL, InnoDB</p></li><li><p>InnoDB có vẻ là chiến lược của AWS</p></li><li><p>Aurora là RDS mà tự động HA tới 3 AZ</p></li><li><p>Encrypting existing RDS is not currently supported</p></li><li><p>Achieved using asynchronous replication</p></li></ul><h1 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h1><ul><li>S3 Versioning once enabled, versioning cannot be disabled, only suspended</li></ul><h2 id="Encrypt"><a href="#Encrypt" class="headerlink" title="Encrypt"></a>Encrypt</h2><ul><li>Có 2 cách để mã hóa dữ liệu được lưu trữ trên S3 buckets<ul><li>Client side encryption (được mã hóa dưới client, trước khi upload lên S3)</li><li>Server Side Encryption (SSE)<ul><li>Data được mã hóa bởi S3 trước khi storage disks của S3</li><li>Data được giải mã khi bạn download nó</li></ul></li></ul></li><li>Tại bất kỳ thời điểm nào, cũng chỉ có thể áp dụng 1 loại mã hóa</li><li>Tùy thuộc vào cách quản lý khóa mã hóa, có 3 loại SSE<ul><li>SSE-S3: S3 quản lý encryption keys</li><li>SSE-KMS: sử dụng KM keys</li><li>SSE-C: Client cung cấp keys</li></ul></li><li>Glacier: chỉ có thể read</li></ul><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><ul><li><p>Nếu bucket có lượng truy cập dưới 100 PUT&#x2F;LIST&#x2F;DELETE cho mỗi giây, hoặc dưới 800 GET request mỗi giây, thì không cần<br>phải cấu hình gì cho S3 để nâng performance cả</p></li><li><p>Ngược lại:</p><ul><li>Random prefix để chúng được lưu vào các phân vùng khác nhau (hình như version mới nhất thì không cần phải random<br>nữa, mà S3 tự động performance)</li><li>Sử dụng CloudFront để phân phối tải tới S3</li></ul></li><li><p>Versioning is disable default</p></li><li><p>Bạn có thể truy suất data từ Glacier theo nhiều cách</p><ul><li>Expedited: 1-5 mts<ul><li>More expensive</li><li>Use for urgent requests only</li></ul></li><li>Standard: 3-5 Hrs<ul><li>Less expensive than Expedited</li><li>You get 10GB data retrieval free&#x2F; month</li></ul></li><li>Bulk retrieval: 5-12 Hrs<ul><li>Cheapest</li><li>Use to retrieve large amounts up to Petabytes in a day</li></ul></li></ul></li></ul><h2 id="S3-Cross-Region-Replication"><a href="#S3-Cross-Region-Replication" class="headerlink" title="S3 Cross Region Replication"></a>S3 Cross Region Replication</h2><ul><li><p>AWS sẽ không replicate dữ liệu ra ngoài region, nó chỉ sao chép trong nhiều facilities (AZ) (nhưng có feature cross<br>region)</p></li><li><p>Mặc định, tất cả objects sẽ gán quyền private, và chỉ có owner mới có thể access</p></li><li><p>Để chia sẻ object bạn có 2 cách</p><ul><li>Set quyền object public</li><li>Tạo pre-signed URL</li></ul></li><li><p>bucket</p><ul><li>global name, unique across all AWS accounts</li></ul></li><li><p>object</p><ul><li>0 bytes - 5 TB</li><li>Dung lượng lớn nhất cho 1 PUT request upload là 5GB (nếu dung lượng lớn hơn 100MB, cân nhắc nên sử dụng Multipart<br>Upload)</li></ul></li><li><p>Data tự động được replicated trong 1 region</p></li><li><p>Security</p><ul><li>ACL</li><li>BucketPolicies<ul><li>IP address range</li><li>AWS account</li><li>Objects with a specific prefix</li></ul></li><li>Encryption</li></ul></li><li><p>Có thể xài kèm Athena để query SQL trên S3 (dạng sheet)</p></li><li><p>Signed URL</p></li><li><p>Có chức năng Restric Viewer</p></li><li><p>Từ Standard sang IA phải ít nhất 30 ngày, nhưng từ Standard sang Glacier thì whenever</p></li></ul><h2 id="S3-Cross-Region-Replication-1"><a href="#S3-Cross-Region-Replication-1" class="headerlink" title="S3 Cross Region Replication"></a>S3 Cross Region Replication</h2><ul><li>Versioning must be enabled on both the source and destination buckets</li><li>Files in an existing bucket are not replicated automatically, all new and updated files will be replicated<br>automatically</li></ul><h2 id="S3-Versioning"><a href="#S3-Versioning" class="headerlink" title="S3 Versioning"></a>S3 Versioning</h2><ul><li>Tốn tiền hơn, 1 object có thể có nhiều version khác nhau,</li><li>Cách để vào mỗi version là url có thêm 1 parameter <code>versionId=xxx</code></li><li>Xóa logic là chỉ đánh dấu delete marker</li><li>Xóa vĩnh viễn thì không khôi phục được</li></ul><h2 id="S3-Transfer-Acceleration"><a href="#S3-Transfer-Acceleration" class="headerlink" title="S3 Transfer Acceleration"></a>S3 Transfer Acceleration</h2><ul><li>Khi upload object, sử dụng 1 endpoint khác với endpoint trực tiếp. Lúc đó aws sẽ route traffic sao cho upload được đưa<br>tới <code>edgate location</code> để có tốc độ cao nhất</li><li>Có nét tương đồng với CloudFront, nhưng có lẽ Cloudfront hợp cho việc download hơn. Theo khuyến cáo thì khi data nhiều<br>tới GB,TB thì nên sử dụng S3 Transfer Acceleration</li><li><a href="https://stackoverflow.com/questions/36882595/are-there-any-difference-between-amazon-cloudfront-and-amazon-s3-transfer-accele/36927340">https://stackoverflow.com/questions/36882595/are-there-any-difference-between-amazon-cloudfront-and-amazon-s3-transfer-accele/36927340</a></li></ul><h2 id="S3-Intelligent-Tiering"><a href="#S3-Intelligent-Tiering" class="headerlink" title="S3 Intelligent-Tiering"></a>S3 Intelligent-Tiering</h2><ul><li>Object được lưu đồng thời trên cả 2 tier. 1 cho frequent, 1 cho infrequent, sau 1 khoảng thời gian aws theo dõi, nó sẽ<br>tự động move các object ít truy cập về tier infrequent. Và ngược lại. (không tốn phí, nhưng tốn fee trước đó)</li></ul><h1 id="Storage-Gateway"><a href="#Storage-Gateway" class="headerlink" title="Storage Gateway"></a>Storage Gateway</h1><ul><li>dùng cho hệ thống lai (on premises vs on demand)</li><li>hay đi kèm vs NFS (base on S3) (ngoài ra có VolumeGateway (base on EBS)…)</li><li>Import to S3 or Export from S3</li><li>Snowball<ul><li>80TB, no compute</li></ul></li><li>Snowball Edge<ul><li>100TB, has compute</li></ul></li><li>Snowmobile<ul><li>100PB, semi-truck</li></ul></li><li>Type:<ul><li>File Gateway: NFS or SMB</li><li>Volume Gateway:<ul><li>Stored: EBS - 1GB-16TB</li><li>Cached: S3 - 1GB- 32GB</li></ul></li><li>Tape Gateway</li></ul></li></ul><h1 id="Snowball"><a href="#Snowball" class="headerlink" title="Snowball"></a>Snowball</h1><ul><li>Là thiết bị vật lý. Nó như 1 cái máy tính có tích hợp ổ cứng di động, ship về on premiss, xong copy data vào nó, rồi<br>ship về data center của Amazon, để copy lại. Dùng trong case mà data migrate rất rất lớn. Mà việc upload tốn nhiều<br>thời gian.</li><li>Snowball support mã hóa KMS, nên không lo về security.</li><li>Khi nào snowball ship về amazon xong, nhân viên sẽ upload data từ snowball vào bucket khách hàng, khi thành công sẽ có<br>SNS</li><li>Snowball là giải pháp vận chuyển dữ liệu ở cấp độ petabyte sử dụng các thiết bị được thiết kế bảo mật để truyền lượng<br>dữ liệu lớn vào và ra khỏi Đám mây AWS.</li><li>Snowball can:<ul><li>import to S3</li><li>export from S3</li></ul></li></ul><h1 id="VPC"><a href="#VPC" class="headerlink" title="VPC"></a>VPC</h1><h2 id="1-Security"><a href="#1-Security" class="headerlink" title="1. Security"></a>1. Security</h2><h3 id="Security-groups-vs-Network-ACLs"><a href="#Security-groups-vs-Network-ACLs" class="headerlink" title="Security groups vs Network ACLs"></a>Security groups vs Network ACLs</h3><table><thead><tr><th>Security groups</th><th>Network ACLs</th></tr></thead><tbody><tr><td>instance level</td><td>subnet level</td></tr><tr><td>stateful</td><td>stateless</td></tr></tbody></table><ul><li>Security groups STATEFUL: nếu đồng ý cho phép chiều đi vào, thì chiều đi ra cũng sẽ được đồng ý, ngược lại</li><li>Network ACL STATELESS: Nếu có rule đồng ý cho kết nối tới port 22 đi vào, thì cũng phải có rule cho phép đi ra</li></ul><h3 id="Security-groups"><a href="#Security-groups" class="headerlink" title="Security groups"></a>Security groups</h3><ul><li>stateful</li><li>Có thể sử dụng Security Group names như 1 khai báo “source” hoặc “destination” cho 1 Security Group khác</li><li>Chỉ có rule allow (không thể khai báo rule deny, nếu không khai báo allow thì mặc định traffic là deny)</li><li>Default là deny tất cả traffic inbound, và allow tất cả traffic outbound</li></ul><h3 id="NACL"><a href="#NACL" class="headerlink" title="NACL"></a>NACL</h3><ul><li>stateless</li><li>supports allow rules and deny rules</li><li>Hình như là default allow all cho EC2 cùng subnet</li><li>You can associate network ACL with multiple subnets, however subnet can only associate with one ACL at a time</li></ul><h2 id="2-VPC-peering"><a href="#2-VPC-peering" class="headerlink" title="2. VPC peering"></a>2. VPC peering</h2><ul><li>Dùng để kết nối giữa các VPCs. Ví dụ: kết nối các EC2 ở các Region khác nhau (kể cả khác Account)</li><li>Hạn chế:<ul><li>không thể định tuyến gói tin từ VPC B tới VPC C thông qua VPC A</li><li>không thể khởi tạo, nếu như có sự trùng lặp, conflict CIDR blocks giữa các VPC (ví dụ: cùng chung dải mảng<br>10.0.0.0&#x2F;16)</li><li>giữa 2 VPC, tại cùng 1 time, chỉ có thể có duy nhất 1 VPC peering</li></ul></li></ul><h2 id="3-AWS-Direct-Connect"><a href="#3-AWS-Direct-Connect" class="headerlink" title="3. AWS Direct Connect"></a>3. AWS Direct Connect</h2><ul><li>AWS cung cấp 1 số địa điểm (office vật lý) để khách hàng có thể tới trực tiếp cắm dây mạng vào để kết nối tới hệ thống<br>của AWS. &#x3D;&gt; giảm chi phí truyền tải băng thông trên internet .</li><li>Use case:<ul><li>Thao tác với bộ dữ liệu lớn</li><li>Nguồn cấp dữ liệu theo thời gian thực</li><li>Môi trường lai</li></ul></li><li>Chỉ cung cấp ở 1 số địa điểm vật lý nhất định trên toàn thế giới</li></ul><h2 id="4-Virtual-Private-Gateway"><a href="#4-Virtual-Private-Gateway" class="headerlink" title="4. Virtual Private Gateway"></a>4. Virtual Private Gateway</h2><h2 id="5-Configuration"><a href="#5-Configuration" class="headerlink" title="5. Configuration"></a>5. Configuration</h2><ul><li>Khi config VPC sử dụng VPC wizard và khai báo VPN-Only and VPN access implies (tức chỉ có VPN mới truy cập vào được).<br>Sẽ có các thông số sau:<ul><li>Không có public subnet</li><li>Không có NAT instance&#x2F;gateway</li><li>Tạo 1 Virtual Private Gateway (không có EIP)</li><li>Tạo 1 VPN connection</li></ul></li></ul><h2 id="6-VPC-CIDR-Block"><a href="#6-VPC-CIDR-Block" class="headerlink" title="6. VPC CIDR Block"></a>6. VPC CIDR Block</h2><ul><li>Không thể thay đổi size CIDR Block sau khi tạo. Nếu muốn tăng size thì tạo 1 VPC mới (cần thiết kế cẩn thận từ đầu)</li></ul><h3 id="7-VPC-Folow-log"><a href="#7-VPC-Folow-log" class="headerlink" title="7. VPC Folow log"></a>7. VPC Folow log</h3><ul><li>Nếu VPC Peering đang được bật, và chủ peering là Account khác, thì không thể bật <code>flow logs</code></li><li>cannot tag a flow log</li><li>after flow log is created, you canot change its configuration</li><li>(1 số IP đặc biệt, DHCP) sẽ không được monitor</li></ul><h3 id="8-Nat-gateway"><a href="#8-Nat-gateway" class="headerlink" title="8. Nat gateway"></a>8. Nat gateway</h3><ul><li>1 cải tiến của Nat Instance</li></ul><h3 id="VPC-Endpoint"><a href="#VPC-Endpoint" class="headerlink" title="VPC Endpoint"></a>VPC Endpoint</h3><ul><li>Không có VPCEndpoint cho RDS</li><li>Chỉ có S3 và dynamodb là có Gateway VPC endpoitn</li><li>còn lại là Interface VPC endpoint</li></ul><h1 id="ELB"><a href="#ELB" class="headerlink" title="ELB"></a>ELB</h1><ul><li>ELB có thể chạy khác Region được, nhưng cần phải có Route 53</li><li>Phân phối traffic cho các EC2 ở nhiều AZ</li><li>Sticky sessions</li><li>X-Forwarded-For:<ul><li>get client IP address</li><li>get previous Request IP Address</li><li>get Load Balancer IP Address</li></ul></li><li>Trả về <code>504</code> nếu EC2 không có response</li><li>không có IPv4</li></ul><h2 id="1-Cac-cach-de-monitoring-ELB"><a href="#1-Cac-cach-de-monitoring-ELB" class="headerlink" title="1. Các cách để monitoring ELB"></a>1. Các cách để monitoring ELB</h2><ul><li>AWS Cloud Watch:<ul><li>ELB gửi ELB metric tới Cloud Watch mỗi 1 phút</li><li>ELB gửi metric mỗi khi có request tới ELB</li><li>Có thể config triger SNS notification khi ELB đạt tới 1 ngưỡng nào đó</li></ul></li><li>Access Logs:<ul><li>Default tắt</li><li>Có thể config chọn S3 lưu trữ log</li><li>Có thể có được các thông tin như requester, thời gian request, IP request, loại request…</li><li>Sẽ không bị charged thêm tiền nếu đã trả tiền cho S3</li></ul></li><li>AWS Cloud Trail<ul><li>Để capture all API calls tới ELB</li><li>Có thể lưu trữ log trên S3</li></ul></li></ul><h2 id="2-Config"><a href="#2-Config" class="headerlink" title="2. Config"></a>2. Config</h2><ul><li>Nếu không có config đặc biệt, ELB sẽ sử dụng config của ELB gần nhất để define</li><li>Để cho phép Backend EC2 (Web layer) biết được thông tin chi tiết của Originator&#x2F; requester (ví dụ: source IP address,<br>port…) bạn có thể:<ul><li>Enable Proxy Protocol for TCP&#x2F;SSL Layer 4 listeners as supported on the ELB</li><li>Enable X-Forewaded-For headers for HTTP&#x2F;HTTPS listeners on the ELB</li></ul></li><li>Mặc định ELB được bật để load balancer giữa các AZ</li><li>The ELB hỗ trợ các SSL protocols:<ul><li>TLS 1.0, TLS 1.1, TLS 1.2, SSL 3.0</li><li>It does not support TLS 1.3 or SSL 2.0 (which is deprecated)</li></ul></li></ul><h2 id="3-Type"><a href="#3-Type" class="headerlink" title="3. Type"></a>3. Type</h2><ul><li>ALB &#x3D;&gt; layer 7 of OSI</li><li>NLB &#x3D;&gt; Layer 4 of OSI</li><li>CLB &#x3D;&gt; layer 7 mix 4</li></ul><h2 id="4-ELB-Proxy-Protocol"><a href="#4-ELB-Proxy-Protocol" class="headerlink" title="4. ELB - Proxy Protocol"></a>4. ELB - Proxy Protocol</h2><ul><li>Trước khi bật tính năng Proxy Protocal thì cần chắc chắn rằng trước Load Balancer chưa có proxy server</li></ul><h2 id="5-ELB-Sticky-sessions"><a href="#5-ELB-Sticky-sessions" class="headerlink" title="5. ELB - Sticky sessions"></a>5. ELB - Sticky sessions</h2><ul><li>Trường hợp BE instance bị chết, ELB sẽ định tuyến traffic tới 1 instance mới, khỏe mạnh, và sticky session trên<br>instance mới. (kể cả khi instance cũ đã khỏe lại)</li><li>For ELB, duration based, cookie stickiness</li></ul><h2 id="6-Connection-Draining"><a href="#6-Connection-Draining" class="headerlink" title="6. Connection Draining"></a>6. Connection Draining</h2><ul><li>Mặc định ELB sẽ check helth của EC2, nếu check lỗi, sẽ đưa EC2 ra <code>out of service</code>, có thể sẽ khởi tạo 1 EC2 khác thay<br>thế. Tuy nhiên trong 1 số trường hợp, EC2 được admin chủ động maintaince gì đó (ví dụ để update, upgrade) dẫn tới<br>việc “unhelth”, thì ELB sẽ tạm ignore case này, trong khoảng thời gian này</li><li>Is disabled by default</li><li>Default, wait 300 seconds</li></ul><h2 id="7-SNI-and-ELB"><a href="#7-SNI-and-ELB" class="headerlink" title="7. SNI and ELB"></a>7. SNI and ELB</h2><ul><li>Server Name Indication (SNI) là một phần mở rộng của giao thức mạng máy tính TLS . Nó cho phép một máy chủ có thể sử<br>dụng nhiều chứng chỉ SSL cho nhiều tên miền trên cùng một địa chỉ IP mạng WAN. Nó giống như việc sử dụng https cho<br>nhiều tên miền cùng sử dụng chung một địa chỉ IP để tiết kiệm</li><li>Elastic Load Balancing không hỗ trợ Server Name Indication (SNI) &#x3D;&gt; cần tìm giải pháp</li><li>X-Forwarded-For is supported with HTTP&#x2F;HTTPS listeners only</li><li>Proxy protocol is supported with TCP&#x2F;SSL listeners only</li></ul><h2 id="8-ELB-Pre-Warming"><a href="#8-ELB-Pre-Warming" class="headerlink" title="8. ELB-Pre-Warming"></a>8. ELB-Pre-Warming</h2><p>ELB Scaling:</p><ul><li>Thời gian để ELB phát hiện được việc tăng traffic là khoảng 1-7p</li><li>ELB không được thiết kế để queue requests</li><li>Trả về lỗi 503, nếu ELB không thể handle được request</li><li>Nếu traffic có thể tăng quá nhanh, hơn 50%, thì cần contact AWS để pre-warm</li><li>Khi ELB scales, nó sẽ update DNS record với danh sách IP mới</li><li>Để chắc chắn clients đang có sự gia tăng về capacity, ELB sẽ gửi TTL tới DNS Record mỗi 60s</li></ul><h1 id="Kinesis"><a href="#Kinesis" class="headerlink" title="Kinesis"></a>Kinesis</h1><ul><li>Kinesis Stream<ul><li>data stored for 24 hours by default</li><li>data stored in shards</li><li>data consumers (ec2 instances) turn shards into data to analyze</li><li>5 transactions per second for reads, maximum total rate of 2 MB&#x2F;second up to 1,000 records for writes</li></ul></li><li>Kinesis Firehose<ul><li>Automated</li><li>no dealing with shards</li></ul></li><li>Kinesis Analytics<ul><li>Way of analyzing data in Kinesis using SQL-like queries</li></ul></li></ul><h1 id="SQS"><a href="#SQS" class="headerlink" title="SQS"></a>SQS</h1><ul><li><p>messages are 256KB in size</p></li><li><p>kept 1 minute to 14 days, default 4 days</p></li><li><p>Visibility Timeout</p><ul><li>if job is not processed within timeout time, message becomes visible again</li><li>if message is processed within that time, message is deleted</li><li>maximum invisible time is 12 hours</li></ul></li><li><p>Standard Queues</p><ul><li>nearly-unlimited number of transactions per second</li><li>guarantee message is delivered at least once</li><li>more than one could be delivered out of order</li></ul></li><li><p>FIFO Queues</p><ul><li>messages sent and received in order they arrive</li><li>delivered once and remains available until consumer processes and deletes it</li><li>SQS FIFO, chỉ có 1 process được access</li></ul></li></ul><h1 id="SWF"><a href="#SWF" class="headerlink" title="SWF"></a>SWF</h1><ul><li>Task is assigned only once and is never duplicated</li><li>assigns tasks and monitors progress</li><li>workers&#x2F;deciders don’t track execution state, run independently, and scale quickly</li><li>parameters described in JSON</li><li>maximum workflow is 1 year, always measured in seconds</li></ul><h2 id="SWF-Domains"><a href="#SWF-Domains" class="headerlink" title="SWF Domains"></a>SWF Domains</h2><ul><li>workflow, activity types, workflow execution all scoped to a Domain</li><li>Domains isolate set of types, executions, and task lists from others in same account</li></ul><h2 id="SWF-Actors"><a href="#SWF-Actors" class="headerlink" title="SWF Actors"></a>SWF Actors</h2><ul><li><p>SWF Workflow Starters</p><ul><li>application to start&#x2F;initiate workflow</li><li>could be website or mobile app, for example</li></ul></li><li><p>SWF Decider</p><ul><li>program that controls coordination of tasks</li><li>task ordering, concurrency, scheduling according to application logic</li></ul></li><li><p>SWF Workers</p><ul><li>program&#x2F;person that interacts with SWF</li><li>gets task</li><li>process receives tasks</li><li>returns result</li></ul></li></ul><h1 id="Cloudwatch"><a href="#Cloudwatch" class="headerlink" title="Cloudwatch"></a>Cloudwatch</h1><ul><li>Có Cloudwatch Agent</li><li>Ko monitor được RAM (instance window monitor hạn chế hơn instance linux)</li><li>Standard Monitoring &#x3D; 5 minutes</li><li>Detailed Monitoring &#x3D; 1 Minute (mất nhiều fee hơn)</li></ul><h1 id="Auto-Scaling"><a href="#Auto-Scaling" class="headerlink" title="Auto Scaling"></a>Auto Scaling</h1><ul><li>Simple Scaling - Ví dụ khi CPU tới 1 ngưỡng nào đó thì scaling</li><li>Step Scaling - Ví dụ: khi có 2 instance, thì CPU tới ngưỡng X sẽ kickoff, nhưng khi 3 instance thì tới ngưỡng Y mới<br>kickoff</li><li>Target Tracking Scaling - so complex</li></ul><h1 id="Developer-Tools"><a href="#Developer-Tools" class="headerlink" title="Developer Tools"></a>Developer Tools</h1><ul><li>CodeStar - Project managing of code for developers</li><li>CodeCommit - Place to store code (source control), private git repository</li><li>CodeBuild - Compiles, tests code and build packages ready for deployment</li><li>CodeDeploy - Deployment services that will deploy applications to EC2, Lambda, on-premise</li><li>CodePipeline - Continuous Delivery to Model&#x2F;Visualize&#x2F;Automate steps for software release</li><li>X-Ray - Used to debug&#x2F;analyze serverless applications by showing traces</li><li>Cloud9 - IDE Environment to develop code inside AWS consol</li></ul><h1 id="Lon-xon"><a href="#Lon-xon" class="headerlink" title="Lộn xộn"></a>Lộn xộn</h1><ul><li><p>Elastic Beanstalk có thể tự động hóa deploy, tự tạo instance, ELB, VPC</p></li><li><p>Mặc định mỗi tài khoản AWS sẽ giới hạn 5IP Elastic</p></li><li><p>Provisioned IOPS SSD at least 4GB in size</p></li><li><p>AWS Import&#x2F;Export : không thể export from Glacier</p></li><li><p>Microsoft SQL: max 10GB per DB</p></li><li><p>Key pair are used only for EC2 and CloudFront</p></li><li><p>AWS Cloudformation sẽ rollback lại toàn bộ các service đã tạo, nếu có 1 service bị lỗi</p></li><li><p>Federated Storage Engine: ???</p></li><li><p>Oracle database: Oracle Data Pump</p></li><li><p>AWS STS - giống access key + secret key nhưng mà có time expired đi kèm</p></li><li><p>Với dịch vụ EC2, aws bắt đầu tính tiền khi EC2 được khởi tạo ở boot sequence, và kết thúc khi Instant shutdown</p></li><li><p>Có thể acces với EC2, sử dụng SOAP protocol</p></li><li><p>OpsWorks - Similar to elastic beanstalk, used to automate configuration of environments (convered in Sysops Admin<br>test)</p></li><li><p>Data Pipeline - Way of moving data between different AWS services</p></li><li><p>Glue - Used for ETL (extract, transform, load), glue is optimized to achieve this</p></li><li><p>WAF - Web Application Firewall (7-layer firewall), monitoring application layer</p></li><li><p>Shield - DDoS Mitigation</p></li><li><p>GameLift - Service to help develop game services in AWS</p></li><li><p>CDN - Edge Locations are not just read only, you can write to them, too</p></li><li><p>Muốn share snapshot của Redshift cluster sang 1 region khác, thì cần BẬT <code>enable cross-Region snapshots.</code></p></li><li><p>General Purpose (SSD) Storage - This storage type is optimized for I&#x2F;O-intensive transactional (OLTP) database<br>workloads</p></li><li><p>Provisioned IOPS SSD cũng support OLTP, performance cao hơn</p></li><li><p>HDD st1 hợp cho log-processing, nhưng throughput từ 250-500</p></li><li><p>AWS Trusted Advisor phân tích môi trường AWS của bạn và đưa ra khuyến nghị về phương pháp thực hành tốt nhất theo năm<br>hạng, 1 kiểu như thư ký ảo</p></li><li><p>không sử dụng Redshift như 1 OLTP Database, nó chỉ nên dùng như OLAP Database</p></li><li><p>Cloudwatch cannot remove EC2 instance from rotation but Route53 health check can do this</p></li><li><p>With Amazon Kinesis Data Analytics for SQL Applications, you can process and analyze streaming data using standard SQL</p></li><li><p>data in 8 KB chunks &#x3D;&#x3D;&gt; NoSQL DB</p></li><li><p>AWS Config - tool analyzes account resources and provides a detailed inventory of changes over time</p></li><li><p>Khi design subnet, nhớ chú ý yếu tố nhân 2, vì để đảm bảo HA, thì mỗi AZ nên có 1 subnet</p></li><li><p>Redshift có thể config độ ưu tiên cho mỗi query riêng biệt được?</p></li><li><p>EFS does not support security groups.</p></li><li><p>(Use an Amazon SNS topic to fan out the data to the SQS queue in addition to a Lambda function that records the data<br>to an S3 bucket.</p></li><li><p>Sử dụng AWS Organizations có thể set policy như IAM được, nhưng trong 1 số case, thao tác IAM tốn nhiều effort hơn</p></li><li><p>AWS Batch is not to be confused for AWS Backup.</p></li><li><p>AWS Batch plans, schedules, and executes your batch computing workloads using Amazon EC2 and Spot Instances and its<br>not used to take backups. AWS Backup can perform backups.</p></li><li><p>Nếu tạo Nat Gateway dùng chung cho 2 AZ, thì case AZ đang chạy Nat gateway down, sẽ làm cho AZ còn lại cũng không có<br>NatGateway dùng</p></li><li><p>Aurora allows its read replicas to be easily promoted to the master and typically only has 100ms of replication lag</p></li><li><p>AWS Premium support: Basic, Developer, Business, Enterprise</p></li><li><p>3 level support của AWS : Enterprise, Business, Developer (không có gói Free Tier)</p></li><li><p>RDS supports SOAP only through HTTPS (not HTTP)</p></li><li><p>Create the IAM roles with cross account access</p></li><li><p>“Domain” refer to in Amazon SWF : A Collection of related workflows</p></li><li><p>AWS CloudFormation : Json-formatted</p></li><li><p>Document aws ghi rằng khi EC2 stop, thì data trên ec2 instance cũng mất đi. (chưa kiểm duyệt lại được, vì test thì<br>thấy vẫn lưu bình thường???)</p></li><li><p>Amazon’s Redshift uses which block size for its columnar storage:  1024KB &#x2F; 1 MB</p></li><li><p>Với các app mobile cần authentication để truy cập vào db như DynamoDB &#x3D;&gt; Nên sử dụng Web Identity Federation, cụ thể<br>app sẽ call tới bên thứ 3 để xác thực user (Google, Facebook, Amazon…), để nhận token, sử dụng này để pass service<br>AWS STS. AWS STS sẽ cung cấp 1 temporary AWS access credential. Được quyền như IAM Role, accesss vào AWS Resource (<br>trong case này là DynamoDB)</p></li><li><p>Mỗi tài khoản AWS gồm:</p><ul><li>AWS Account ID: such as <code>123456789012</code>. Dùng trong case ARN</li><li>Canonical User ID : such as <code>79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be</code>. Root user và IAM<br>user sẽ cùng chung 1 <code>canonical user</code>. Được dùng trong case <code>cross-account access</code>.</li></ul></li><li><p>1 Public subnet trong 1 VPC được định nghĩa là trong bảng định tuyến của nó, có ít nhất 1 route IGW</p></li><li><p>Khi nào nên chọn Provisioned IOPS trên Standard RDS?</p><ul><li>Khi cần db OLTP</li></ul></li><li><p>Để lấy thông tin metadata của ec2, <code>curl http://169.254.169.254/latest/meta-data</code> (lưu ý 169.254.169.254 là địa chỉ IP<br>cố định, không phải thay đổi)</p></li><li><p>AWS Direct Connect: 1Gbps - 10Gbps</p></li><li><p>The maximum size of an Amazon EBS snapshot is 1TB</p></li><li><p>Provisioned IOPS sẽ được charge phí ngay cả khi không dùng, mỗi tháng</p></li><li><p>Storage size increments of at least 10%</p></li><li><p>Muốn bắn mail khi Instance được start&#x2F;terminate khi chạy Auto Scaling thì phải config (nó không tự gửi mail)</p></li><li><p>Cần phải shutdown EC2 trước khi tạo snapshot</p></li><li><p>Route 53 không thể tạo hosted zone for a top-level domain</p></li><li><p>Không thể tạo IAM account giống nhau để login vào các AWS Account khác nhau</p></li><li><p>RDS không hỗ trợ giảm size db, nhưng với Dynamo DB (NOSQL) thì có support cả tăng và giảm</p></li><li><p>SQS không support priority queue</p></li><li><p>Security group sẽ merge các policy conflict lại với nhau (cái nào to hơn xài cái đó, ví dụ có 2 cái, 1 cái chỉ cho<br>phép vào port 80 với IP xxx, và 1 cái cho phép vào port 80 với mọi IP, thì kết quả cuối sẽ là cho phép all)</p></li><li><p>AWS using SQS to store the message from mobile apps, and using AWS Mobile Push to send offers to mobile apps.</p></li><li><p>EC2 sẽ không bị terminate nếu tính năng “terminate protection” đang được enabled. Chế độ Auto Scaling, khi scale-in sẽ<br>không thể terminate được instance</p></li><li><p>AuTO Scaling support Manual scaling, và Dynamic Scaling</p></li><li><p>Trong một Region, thì việc user sử dụng AZ khác nhau không giảm được latency. AZ mục đích chính để fault toleration<br>hoặc HA</p></li><li><p>EBS Magnetic: 1 loại ổ đĩa HDD</p></li><li><p>Amazon Redshift: 1 service phục vụ data warehouse,lưu data theo dạng COLUMNAR</p></li><li><p>Cross-account: account sau khi đăng nhập, có thể switch sang role khác của 1 tài khoản khác, mà không cần phải<br>login&#x2F;logout đăng nhập tài khoản đó.</p></li><li><p>Danh sách các subscriber của SNS</p><ul><li>Lambda</li><li>SQS</li><li>HTTP&#x2F;S</li><li>Email</li><li>SMS</li></ul></li><li><p>CloudFront được dùng để distribution origin cho các service sau:</p><ul><li>S3</li><li>ELB</li><li>MediaPackage Origins</li><li>MediaStore Containers</li></ul></li><li><p>CloudFront support cho cả static và dynamic content trên global</p></li><li><p>AWS KMS : chỉ mã hóa với khóa đối xứng symetric key</p></li><li><p>Vì DynamoDB chỉ lưu được giới hạn data, nên có thể kết hợp với sử dụng S3 để lưu trữ, và DynamoDB sẽ lưu trữ vị trí<br>trỏ tới S3</p></li><li><p>DynamoDB không hỗ trợ TRANSACTION giữa S3 và DynamoDB</p></li><li><p>S3 giới hạn độ dài của object identifiers</p></li><li><p>Consolidated billing (Thanh toán tổng hợp): usecase 1 tài khoản master tạo ra organization và mời các tài khoản khác<br>invite vào organization này. Khi đó tài khoản master là Paying account, sẽ thực hiện thanh toán toàn bộ chi phí của<br>các tài khoản khác. Các tài khoản khác được invite, và không phải trả tiền là Linked Account</p></li><li><p>General Purpose SSD (gp2):<br>(Min: 1 GiB, Max: 16384 GiB)<br>IOPS 300 &#x2F; 3000</p></li><li><p>Provisioned IOPS SSD (io1)<br>(Min: 4 GiB, Max: 16384 GiB)<br>(Min: 100 IOPS, Max: 64000 IOPS)</p></li></ul><h1 id="FSx-vs-EFS"><a href="#FSx-vs-EFS" class="headerlink" title="FSx vs EFS"></a>FSx vs EFS</h1><ul><li>Có vẻ FSX được thiết kế chuyên cho window server</li><li>1 số case sau nên dùng FSX</li></ul><pre><code class="Home">Windows line-of-business applications.Web servers and content management systems built on Windows and deeply integrated with the Windows Server ecosystem.Windows app dev environments, notably Visual Studio.Media workflows.Windows data analytics, such as Power BI, the SQL Server data platform, Sisense or other third-party applications.</code></pre><ul><li>Fsx Có 2 loại:<ul><li>Cho window server. (dùng như kiểu share directory)</li><li>Cho Lustre</li></ul></li></ul><h1 id="Some-topic"><a href="#Some-topic" class="headerlink" title="Some topic"></a>Some topic</h1><ul><li><a href="https://aws.amazon.com/vi/premiumsupport/knowledge-center/migrate-nat-instance-gateway/">https://aws.amazon.com/vi/premiumsupport/knowledge-center/migrate-nat-instance-gateway/</a></li><li><a href="https://acloud.guru/forums/aws-csa-2019/discussion/-LbnjIbr3jdqQdRSRa7s/VPC%20Endpoint%20Interface%20vs%20Gateway">https://acloud.guru/forums/aws-csa-2019/discussion/-LbnjIbr3jdqQdRSRa7s/VPC%20Endpoint%20Interface%20vs%20Gateway</a></li><li><a href="https://blog.treasuredata.com/blog/2016/02/10/whats-the-difference-between-aws-redshift-aurora/">https://blog.treasuredata.com/blog/2016/02/10/whats-the-difference-between-aws-redshift-aurora/</a></li><li><a href="https://hevodata.com/blog/amazon-redshift-vs-aurora/">https://hevodata.com/blog/amazon-redshift-vs-aurora/</a></li><li><a href="https://dev.to/garyker/aws-classic-load-balancer-vs-application-load-balancer-12m0">https://dev.to/garyker/aws-classic-load-balancer-vs-application-load-balancer-12m0</a></li><li><a href="https://medium.com/awesome-cloud/aws-difference-between-ebs-and-instance-store-f030c4407387">https://medium.com/awesome-cloud/aws-difference-between-ebs-and-instance-store-f030c4407387</a></li><li><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-monitoring-using-cloudwatch.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-monitoring-using-cloudwatch.html</a></li><li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> note total </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Logback</title>
      <link href="/2020/06/Java/Logback/"/>
      <url>/2020/06/Java/Logback/</url>
      
        <content type="html"><![CDATA[<h1 id="Cau-hinh-Logback-xml-trong-Spring"><a href="#Cau-hinh-Logback-xml-trong-Spring" class="headerlink" title="Cấu hình Logback.xml trong Spring"></a>Cấu hình Logback.xml trong Spring</h1><h2 id="1-Template"><a href="#1-Template" class="headerlink" title="1. Template"></a>1. Template</h2><ul><li>1</li></ul><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt;    &lt;include resource=&quot;org/springframework/boot/logging/logback/defaults.xml&quot;/&gt;    &lt;include resource=&quot;org/springframework/boot/logging/logback/file-appender.xml&quot;/&gt;    &lt;include resource=&quot;org/springframework/boot/logging/logback/console-appender.xml&quot;/&gt;    &lt;timestamp key=&quot;today&quot; datePattern=&quot;yyyy-MM-dd&quot;/&gt;    &lt;property name=&quot;LOG_FILE&quot; value=&quot;my-application&quot;/&gt;    &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;        &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt;            &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %-5p $&#123;PID:- &#125; --- [%t] %c : %L: %m%n%wEx&lt;/pattern&gt;        &lt;/encoder&gt;        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt;            &lt;fileNamePattern&gt;$&#123;LOG_PATH&#125;/$&#123;LOG_FILE&#125;-$&#123;HOSTNAME&#125;-%d&#123;yyyy-MM-dd&#125;.%i.gz&lt;/fileNamePattern&gt;            &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt;            &lt;maxHistory&gt;30&lt;/maxHistory&gt;        &lt;/rollingPolicy&gt;    &lt;/appender&gt;    &lt;root level=&quot;INFO&quot;&gt;        &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;        &lt;appender-ref ref=&quot;FILE&quot;/&gt;    &lt;/root&gt;&lt;/configuration&gt;</code></pre><ul><li>2</li></ul><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt;    &lt;property name=&quot;defaultPattern&quot; value=&quot;%date [%thread] %highlight(%-5level) %cyan(%logger&#123;15&#125;) [%file : %line] %msg%n&quot; /&gt;    &lt;appender name=&quot;Sentry&quot; class=&quot;io.sentry.logback.SentryAppender&quot;&gt;        &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;            &lt;level&gt;WARN&lt;/level&gt;        &lt;/filter&gt;    &lt;/appender&gt;    &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;        &lt;encoder&gt;            &lt;pattern&gt;$&#123;defaultPattern&#125;&lt;/pattern&gt;        &lt;/encoder&gt;    &lt;/appender&gt;    &lt;root level=&quot;INFO&quot;&gt;        &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;        &lt;appender-ref ref=&quot;Sentry&quot;/&gt;    &lt;/root&gt;&lt;/configuration&gt;</code></pre><ul><li>3</li></ul><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt;    &lt;include resource=&quot;org/springframework/boot/logging/logback/base.xml&quot;/&gt;    &lt;jmxConfigurator/&gt;    &lt;springProperty name=&quot;SENTRY_LOG_LEVEL&quot; source=&quot;sentry.log.level&quot; defaultValue=&quot;OFF&quot;/&gt;    &lt;property name=&quot;defaultPattern&quot; value=&quot;%date %level [%thread] %logger&#123;10&#125; [%file : %line] %msg%n&quot; /&gt;    &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt;        &lt;file&gt;service.log&lt;/file&gt;        &lt;encoder&gt;            &lt;pattern&gt;$&#123;defaultPattern&#125;&lt;/pattern&gt;        &lt;/encoder&gt;    &lt;/appender&gt;    &lt;appender name=&quot;Sentry&quot; class=&quot;io.sentry.logback.SentryAppender&quot;&gt;        &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;            &lt;level&gt;$&#123;SENTRY_LOG_LEVEL&#125;&lt;/level&gt;        &lt;/filter&gt;    &lt;/appender&gt;    &lt;logger level=&quot;DEBUG&quot; name=&quot;service&quot; additivity=&quot;false&quot;&gt;        &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;    &lt;/logger&gt;    &lt;logger level=&quot;DEBUG&quot; name=&quot;exception&quot; additivity=&quot;false&quot;&gt;        &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;    &lt;/logger&gt;    &lt;root level=&quot;DEBUG&quot;&gt;        &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt;    &lt;/root&gt;&lt;/configuration&gt;</code></pre><ul><li><ol start="4"><li>Set biến theo springProfile</li></ol></li></ul><pre><code class="xml">&lt;springProfile name=&quot;staging&quot;&gt;        &lt;property name=&quot;LOG_ROOT&quot; value=&quot;/sapo-logs&quot; /&gt;    &lt;/springProfile&gt;    &lt;springProfile name=&quot;live&quot;&gt;        &lt;property name=&quot;LOG_ROOT&quot; value=&quot;/sapo-logs&quot; /&gt;    &lt;/springProfile&gt;    &lt;springProfile name=&quot;debug&quot;&gt;        &lt;property name=&quot;LOG_ROOT&quot; value=&quot;sapo-logs&quot; /&gt;    &lt;/springProfile&gt;    &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt;        &lt;file&gt;$&#123;LOG_ROOT&#125;/app-name/service.log&lt;/file&gt;        &lt;encoder&gt;            &lt;pattern&gt;&quot;%date&quot; %level [%thread] %logger&#123;10&#125; [%file : %line] %msg%n            &lt;/pattern&gt;        &lt;/encoder&gt;    &lt;/appender&gt;</code></pre><h2 id="2-Chu-thich"><a href="#2-Chu-thich" class="headerlink" title="2. Chú thích"></a>2. Chú thích</h2><pre><code class="java">    @Bean    public Logger logger() &#123;        return new Slf4jLogger(&quot;exception&quot;);    &#125;</code></pre><pre><code class="xml">&lt;logger name=&quot;com.lankydan.service.MyServiceImpl&quot; additivity=&quot;false&quot; level=&quot;debug&quot;&gt;  &lt;appender-ref ref=&quot;STDOUT&quot; /&gt;&lt;/logger&gt;</code></pre><ul><li>Có thể cấu hình trong file .yml hoặc .properties</li></ul><pre><code class="properties">logging.level.root=infologging.level.com.lankydan.service=errorlogging.path=logslogging.file=$&#123;logging.path&#125;/log.loglogging.pattern.file=%d&#123;dd-MM-yyyy HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125;.%M - %msg%nlogging.pattern.console=  </code></pre><ul><li>ref: <a href="http://logback.qos.ch/manual/layouts.html">http://logback.qos.ch/manual/layouts.html</a></li></ul><h2 id="3-AWS-CloudWatch-Logback-Apender"><a href="#3-AWS-CloudWatch-Logback-Apender" class="headerlink" title="3. AWS CloudWatch Logback Apender"></a>3. AWS CloudWatch Logback Apender</h2><ul><li>Maven</li></ul><pre><code class="xml">        &lt;dependency&gt;            &lt;groupId&gt;ca.pjer&lt;/groupId&gt;            &lt;artifactId&gt;logback-awslogs-appender&lt;/artifactId&gt;            &lt;version&gt;1.4.0&lt;/version&gt;        &lt;/dependency&gt;</code></pre><ul><li>Logback.xml<ul><li>Lưu ý cần set 2 env để credential AWS_ACCESS_KEY&#x3D;123456; AWS_SECRET_KEY&#x3D;123456;</li></ul></li></ul><pre><code class="xml">&lt;appender name=&quot;ASYNC_AWS_LOGS1&quot; class=&quot;ca.pjer.logback.AwsLogsAppender&quot;&gt;        &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;            &lt;level&gt;INFO&lt;/level&gt;        &lt;/filter&gt;        &lt;logStreamName&gt;topic.test1&lt;/logStreamName&gt;        &lt;logRegion&gt;ap-east-1&lt;/logRegion&gt;        &lt;logGroupName&gt;/sapo/web-pos-channel-v2&lt;/logGroupName&gt;        &lt;layout&gt;            &lt;pattern&gt;%date %level [%thread] %logger&#123;10&#125; [%file : %line] %msg%n&lt;/pattern&gt;        &lt;/layout&gt;    &lt;/appender&gt;</code></pre><p>Ref: <a href="https://github.com/pierredavidbelanger/logback-awslogs-appender">https://github.com/pierredavidbelanger/logback-awslogs-appender</a></p><h2 id="4-Loki-log-Appender"><a href="#4-Loki-log-Appender" class="headerlink" title="4. Loki log Appender"></a>4. Loki log Appender</h2><ul><li>maven</li></ul><pre><code class="xml">        &lt;dependency&gt;            &lt;groupId&gt;com.github.loki4j&lt;/groupId&gt;            &lt;artifactId&gt;loki-logback-appender&lt;/artifactId&gt;            &lt;version&gt;0.4.0&lt;/version&gt;        &lt;/dependency&gt;</code></pre><ul><li>logback.xml</li></ul><pre><code class="xml">    &lt;appender name=&quot;LOKI&quot; class=&quot;com.github.loki4j.logback.LokiJavaHttpAppender&quot;&gt;        &lt;url&gt;http://192.168.13.249:3100/loki/api/v1/push&lt;/url&gt;        &lt;batchSize&gt;100&lt;/batchSize&gt;        &lt;batchTimeoutMs&gt;10000&lt;/batchTimeoutMs&gt;        &lt;encoder class=&quot;com.github.loki4j.logback.JsonEncoder&quot;&gt;            &lt;label&gt;                &lt;pattern&gt;app=tung-test-can-remove-whenever,host=$&#123;HOSTNAME&#125;,level=%level&lt;/pattern&gt;            &lt;/label&gt;            &lt;message&gt;                &lt;pattern&gt;l=%level h=$&#123;HOSTNAME&#125; c=%logger&#123;20&#125; t=%thread | %msg %ex&lt;/pattern&gt;            &lt;/message&gt;            &lt;sortByTime&gt;true&lt;/sortByTime&gt;        &lt;/encoder&gt;    &lt;/appender&gt;</code></pre><p>Ref: <a href="https://github.com/tungtv202/loki-logback-appender">https://github.com/tungtv202/loki-logback-appender</a></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> log </tag>
            
            <tag> logback </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Implement Quartz</title>
      <link href="/2020/06/Java/Quartz/"/>
      <url>/2020/06/Java/Quartz/</url>
      
        <content type="html"><![CDATA[<h1 id="Tich-hop-Quartz-vao-Spring"><a href="#Tich-hop-Quartz-vao-Spring" class="headerlink" title="Tích hợp Quartz vào Spring"></a>Tích hợp Quartz vào Spring</h1><p><img src="https://miro.medium.com/max/1400/1*TC0WiXrDTkYsLRdT2Sk9mg.png" alt="https://miro.medium.com/max/1400/1*TC0WiXrDTkYsLRdT2Sk9mg.png"></p><h2 id="Use-case"><a href="#Use-case" class="headerlink" title="Use case"></a>Use case</h2><ul><li>Dùng trong case có nhiều schedule, mà lịch chạy schedule là dynamic</li><li>Ví dụ:<ul><li>Mỗi 1 endUser có 1 lịch schedule chạy 1 task vụ bất kỳ khác nhau.</li><li>Lên schedule gửi mail phỏng vấn ứng viên với calendar chuẩn bị trước</li></ul></li></ul><h2 id="Tich-hop"><a href="#Tich-hop" class="headerlink" title="Tích hợp"></a>Tích hợp</h2><h3 id="1-Config"><a href="#1-Config" class="headerlink" title="1. Config"></a>1. Config</h3><ul><li>Thư viện</li></ul><pre><code class="xml">&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><ul><li>config .yml</li></ul><pre><code class="yml">spring:  datasource-main:    driver-class-name: com.microsoft.sqlserver.jdbc.SQLServerDriver    jdbcUrl: jdbc:sqlserver://0.0.0.0:1433;databaseName=SchemaName    username: user    password: passs    leakDetectionThreshold: 0    connectionTimeout: 30000  quartz:    job-store-type: jdbc    properties:      org.quartz:        threadPool.threadCount: 2        jobStore:          isClustered: false          driverDelegateClass: org.quartz.impl.jdbcjobstore.MSSQLDelegate</code></pre><ul><li><p>Sử dụng jdbc để persistence các job, scheduler</p></li><li><p>Lưu ý trường hợp sử dụng MSSQL, cần khai báo <code>driverDelegateClass</code></p></li><li><p>Cần tạo các table phục vụ cho QUARTZ. Query create table ứng với mỗi<br>RDBMS: <a href="https://github.com/quartz-scheduler/quartz/tree/master/quartz-core/src/main/resources/org/quartz/impl/jdbcjobstore">https://github.com/quartz-scheduler/quartz/tree/master/quartz-core/src/main/resources/org/quartz/impl/jdbcjobstore</a></p></li><li><p>ScheduleBuilder</p><ul><li>Mỗi 1 job là 1 schedule</li><li>Có 4 loại:<ul><li>CalendarIntervalScheduleBuilder: calendar. (ví dụ set lịch gửi mail phỏng vấn ứng viên)</li><li>CronScheduleBuilder: cron job sử dụng cron expression. Ví dụ set lịch chạy backup định kỳ hàng tuần của mỗi<br>end user</li><li>SimpleScheduleBuilder: loại basic nhất, chỉ cần set “StartAt” &#x3D; thời gian muốn chạy job</li><li>DailyTimeIntervalScheduleBuilder</li></ul></li></ul></li></ul><h3 id="2-Su-dung"><a href="#2-Su-dung" class="headerlink" title="2. Sử dụng"></a>2. Sử dụng</h3><ul><li>2 phần cơ bản:<ul><li>Khai báo, tạo scheduler</li><li>Execute job</li></ul></li><li>a). Khai báo&#x2F; tạo scheduler</li></ul><pre><code class="java">@Autowiredprivate  Scheduler scheduler;//final var quartJobKey=UUID.randomUUID().toString();    JobDetail jobDetail=JobBuilder.newJob(BackupQuartzJob.class)    .withIdentity(quartJobKey,schedule.jobGroupName())    .withDescription(&quot;Scheduler backup : &quot;+userId)    .storeDurably()    .build();    Trigger trigger=TriggerBuilder.newTrigger()    .forJob(jobDetail)    .withIdentity(jobDetail.getKey().getName(),schedule.triggerGroupName())    .withDescription(&quot;Trigger backup : &quot;+userId)    .withSchedule(CronScheduleBuilder.cronSchedule(schedule.cronExpression()))    .build();    scheduler.scheduleJob(jobDetail,trigger);</code></pre><ul><li>jobDetail có thể <code>.setJobData()</code> là 1 <code>JobDataMap</code>. (cho bên Job Excute lôi ra dùng)</li><li><code>BackupQuartzJob.class</code> là class để execute job</li><li>b) Execute job</li></ul><pre><code class="java">@Componentpublic class BackupQuartzJob extends QuartzJobBean &#123;    @Override    protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123;        System.out.println(&quot;TUNG exe: &quot; + new Date());    &#125;&#125;</code></pre><ul><li>c) Delete job</li></ul><pre><code class="java">scheduler.deleteJob(new JobKey(storeConfig.getQuartzJobName(),storeConfig.getQuartzJobGroup()));</code></pre><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><ul><li><code>scheduler</code> default support sẵn Transactional</li><li>Nếu threadpool &#x3D; 2, và số job có <code>fire time</code> trùng nhau là 1000. Thì tại thời điểm <code>fire time</code> 2 schedule sẽ được<br>execute, sau đó 996 schedule còn lại sẽ bị delay xử lý dần dần, chứ không mất</li><li>RISK: nếu time execute &gt; circle time. (Ví dụ cron trigger 5s chạy 1 lần, và time execute là 8s). Và có N schedule có<br>time execute trùng nhau, thì có vẻ như có 1 local queue được sử dụng tạm thời, để N schedule đều được xử lý hết vòng.<br>Nhưng khi tới 1 threshold nào đó (chưa rõ cách tính threshold), thì N schedule sẽ bị reset time next trigger. Khi đó<br>quartz sẽ có thông báo lỗi kiểu “Có N schedule bị miss”<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/QuartzRisk.PNG" alt="Risk"></li><li>Mỗi 1 instance (các instance chạy cùng db), nếu không set name, thì SCHEDULE_NAME &#x3D; <code>quartzScheduler</code></li><li>Để set tên<pre><code>      org.quartz:      scheduler:        instanceName: instance002</code></pre></li><li>khi trigger job, job sẽ được chạy trên instance cùng tên với tên SCHEDULE_NAME trong trigger</li><li>trong case các instance cùng tên, khi instance đang trigger job bị shutdown, thì trigger đó sẽ không được chạy trên<br>instance còn lại. Chỉ trừ trường hợp restart lại 1 trong các instance. (Default là như vậy, có thể sẽ khác nếu config<br>khác)</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> quartz </tag>
            
            <tag> auto scheduler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rabbitmq - Mirror Queue + Durable Message</title>
      <link href="/2020/05/RabbitMQ/RabbitMQ_Mirror/"/>
      <url>/2020/05/RabbitMQ/RabbitMQ_Mirror/</url>
      
        <content type="html"><![CDATA[<h2 id="Important"><a href="#Important" class="headerlink" title="Important:"></a>Important:</h2><ul><li>Sử dụng Mnesia database</li><li>Durable Queues&#x2F;Exchanges:<ul><li>2 loại queue:<ul><li>durable: khi node bị lỗi, hoặc sự cố khởi động lại thì queue sẽ được “load” lại khi startup</li><li>non-durable: ngược lại vs durable. Restart là mất</li></ul></li></ul></li><li>Persistent Messages<ul><li>message phải được set là <code>persistent</code> + vs queue là durable, thì mới không lo message bị mất khi bị restart</li></ul></li><li>Evaluation<ul><li>Non-durable Queue + Non-Persistent message &#x3D; mất Queue + mất Message (sau khi Broker restart)</li><li>Durable queue + Non-Persistent message &#x3D; Còn Queue + mất Message</li><li>Durable queue + Persistent message &#x3D; Còn Queue + Còn Message</li><li>Mirrored queue + Persistent message &#x3D; Còn Queue + Còn Message</li></ul></li><li>Rabbitmq không khi message vào disk ngay khi nhận được. Mà nó sẽ save vào disk theo lịch định kỳ (mỗi chu kỳ vài trăm<br>ms). Nếu có cấu hình <code>mirror queue</code>, thì tất cả các node mirror ghi vào disk xong, thì publisher mới confirm ACK là<br>write message done.</li><li>Thiết kế cluster rabbitmq, thì client có thể connect tới bất kỳ broker nào, Nhưng sau đó các message đọc ghi sẽ được<br>điều hướng về broker có queue master. (giống cơ chế của Kafka)</li></ul><h2 id="Mirrored-Queues"><a href="#Mirrored-Queues" class="headerlink" title="Mirrored Queues"></a>Mirrored Queues</h2><ul><li>Tiền thân là Replicated queue</li><li>Node master sẽ nhận tất cả các request đọc&#x2F; ghi. Các node mirror sẽ nhận tất cả message từ node master và ghi vào<br>disk.</li><li><code>Lưu ý: các node mirror không có nhiệm vụ giảm tải &quot;read&quot;/&quot;write&quot; cho node master. Nó chỉ mirror message để phụ vụ việc HA. =&gt; Từ phiên bản 3.8 trở đi, Rabbit MQ support QUORUM QUEUE để giải quyết vấn đề này.</code></li><li>Lưu ý: Ví dụ có 3 broker. Queue A có thể master trên broker 1. Nhưng Queue B có thể master trên broker 2</li><li>Khi node mirror gặp sự cố, cluster sẽ chọn node mirror <code>oldest</code> để làm mirror. Và broadcast lại cho cluster.</li></ul><h3 id="ha-mode"><a href="#ha-mode" class="headerlink" title="ha-mode"></a>ha-mode</h3><ul><li>Ví dụ cách chỉ định:<ul><li>ha-mode: all</li><li>ha-mode: exactly, ha-params: 2 (one master and one mirror)</li><li>ha-mode: nodes, ha-params: rabbit@node1, rabbit@node2</li></ul></li><li>Lưu ý: số broker trong cluster &gt;&#x3D; số node được khai báo ở <code>ha-mode</code>. Ví dụ có 3 broker, nhưng Queue A chỉ khai<br>báo <code>ha-mode: exactly, ha-params: 2</code> thì queue đó chỉ có 1 master, 1 mirror. (broker thứ 3 còn lại không làm gì).</li></ul><h3 id="Synchronization-ha-sync-mode"><a href="#Synchronization-ha-sync-mode" class="headerlink" title="Synchronization - ha-sync-mode"></a>Synchronization - ha-sync-mode</h3><ul><li>Có 2 mode để sync data giữa các broker:<ul><li><code>ha-sync-mode = automatic</code> : khi có 1 broker fail, bị loại ra khỏi cluster, sau đó online trở lại broker. Thì data<br>sẽ được sync từ master lại “từ đầu”. (queue + message trong queue)</li><li><code>ha-sync-mode = manual</code>: sau khi broker fail quay trở lại cluster. thì chỉ sync queue + không sync các message<br>cũ. (chỉ sync các message mới, từ lúc broker fail online trở lại)</li></ul></li><li>Mode manual gặp vấn đề khi broker fail comeback. Tuy nhiên ưu điểm của manual<br>là <code>trong khoảng thời gian broker online gia nhập cluster lại </code>  queue ở master vẫn hoạt động bình thường. Còn với<br>mode <code>automatic</code> gặp vấn đề là sẽ bị mất 1 khoảng thời gian để sync data message. Và khoảng thời gian này sẽ làm block<br>việc đọc ghi queue.</li></ul><h3 id="Network-Partitions"><a href="#Network-Partitions" class="headerlink" title="Network Partitions"></a>Network Partitions</h3><ul><li>Vấn đề gặp phải khi network bị partitions giữa các node. Khiến cho các node không kết nối được tới node khác, nên tự<br>set chính mình lên làm master. Dẫn tới có &gt;1 master trong 1 cluster. Và khi sự cố network được giải quyết, thì không<br>biết chọn node nào làm master, node nào làm mirror. Cái này keyword là “split-brain”</li><li>Để giải quyết vấn đề này cần setup <code>Pause Minority</code>. Khi bị <code>split-brain</code> thì bên phía nào ít node hơn. Sẽ tự pause<br>chính mình. Từ chối nhận bất kỳ message nào từ publisher. Để publisher sau đó tự tìm tới node master bên phía có nhiều<br>node hơn.<ul><li>Ví dụ: có 3 node. Khi gặp sự cố network partition. Sẽ có 2 bên. 1 bên là 1 node master. 1 bên là 1 node master và<br>1 node mirror. bên phía chỉ có 1 node master sẽ tự pause chính mình. từ chối các message tới. Để publisher tìm tới<br>node master bên phía có 2 node.</li></ul></li></ul><h3 id="Tong-ket"><a href="#Tong-ket" class="headerlink" title="Tổng kết"></a>Tổng kết</h3><ul><li>Nếu nghiêng về hướng thiết kế HA (chấp nhận rủi ro mất message, hoặc không đảm bảo tính nhất quán):<ul><li>ha-promote-on-failure&#x3D;always</li><li>ha-sync-mode&#x3D;manual</li><li>cluster_partition_handling&#x3D;ignore or auto-heal</li><li>Persistent messages</li><li>Cần đảm bảo client có kết nối tới cluster khi có node down</li></ul></li><li>Nếu nghiêng về hướng thiết kế đảm bảo data (không đảm bảo HA trong 1 thời gian ngắn)<ul><li>use Publisher Confirms and Manual Acknowledgements on the consumer side</li><li>ha-promote-on-failure&#x3D;when-synced (nếu publisher có thể retry sau, và bộ nhớ thoải mái)</li><li>ha-sync-mode&#x3D;automatic (chấp nhận khi có node restart thì quá trình sync lại data sẽ làm block cả cluster trong<br>khoảng thời gian sync)</li><li>Pause Minority mode</li><li>Persistent messages</li></ul></li><li>Khi thiết kế HA, nên có 1 LB, để khi accesses vào node lỗi, thì sẽ được lb qua node khác.</li></ul><h2 id="Quorum-Queue"><a href="#Quorum-Queue" class="headerlink" title="Quorum Queue"></a>Quorum Queue</h2><ul><li>client sử dụng mirror queue có thể sử dụng với quorum queue (có khả năng tương thích ngược)</li><li>khi sử dụng quorum queue, message luôn là durable (không cần phải khai báo như với Mirror Queue)</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/rabbitmq/QuorumVsMirrorQueue.PNG" alt="Quorum vs Mirror"></p><ul><li>Poison Message Handling: message gửi cho consumer với số lần quá ngưỡng cho phép<ul><li>x-delivery-count  : thông tin ở header msg</li><li>delivery-limit : sử dụng attribute này để setup config giới hạn</li></ul></li><li>Quorum queue dữ msg mãi mãi trên disk (khác với mirror là sau khi consumer ack thì sẽ delete ?)</li><li>WAL - write-ahead-log</li><li>khi node fail, xong quay lại, thì nó chỉ đồng bộ các message mới. mà không phải sync lại từ đầu. Và quá trình sync các<br>message mới này không bị blocking (ưu việt hơn so với Mirror)</li><li>MEMORY USAGE - ALL MESSAGES IN-MEMORY ALL THE TIME</li><li>Nếu broker bị lỗi gì đó làm mất dữ liệu, thì toàn bộ msg trên broker đó sẽ mất vĩnh viễn. Khi broker đó online trở<br>lại, thì không thể sync lại data từ leader từ đầu.</li></ul>]]></content>
      
      
      <categories>
          
          <category> rabbitmq </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ha </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> mirror queue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Jhipster</title>
      <link href="/2020/04/Java/Jhipster/"/>
      <url>/2020/04/Java/Jhipster/</url>
      
        <content type="html"><![CDATA[<h1 id="JHipster"><a href="#JHipster" class="headerlink" title="JHipster"></a>JHipster</h1><h2 id="1-Muc-dich"><a href="#1-Muc-dich" class="headerlink" title="1. Mục đích"></a>1. Mục đích</h2><p>Tạo nhanh project java:</p><ul><li>build sẵn CRUD: service + repository + controller + entity</li><li>Tự động generator database &#x3D; jdl file</li><li>cung cấp giao diện admin CRUD</li></ul><p>&#x2F;&#x2F; Có nét giống với Django Admin Module trong Python</p><h2 id="2-Install"><a href="#2-Install" class="headerlink" title="2. Install"></a>2. Install</h2><ul><li>Java</li><li>Nodejs (nodejs + npm)<ul><li>Lưu ý về version</li><li>Khi tạo jhipster project, khi thực hiện command <code>mvnw</code>, có thể jhipster sẽ install lại nodejs + npm với version<br>khác.</li></ul></li><li>Install jhipster<pre><code>mkdir myapplicationcd myapplication/jhipster# select and install</code></pre></li><li><code>npm install</code></li></ul><p><a href="https://www.jhipster.tech/installation/">https://www.jhipster.tech/installation/</a></p><h2 id="3-Note"><a href="#3-Note" class="headerlink" title="3. Note"></a>3. Note</h2><ul><li>Khi cài đặt jhipster, có thể chọn microservice, hoặc monothinic</li><li>Support reactjs + angularjs</li><li>Support deployment &#x3D; docker</li><li>Support deployment &#x3D; kiểu truyền thống, chạy file .jar với profile</li><li>Lưu ý, trong 1 số case, có thể phải tạo proxy, gateway để chạy frontend (reactjs&#x2F;angularjs) riêng với backend (java)</li><li>Tạo file jdl tại: <a href="https://start.jhipster.tech/jdl-studio/">https://start.jhipster.tech/jdl-studio/</a></li><li>Run file jdl &#x3D; command <code>jhipster import-jdl file.jdl</code></li><li>Có thể custom serviceImpl để khi install file <code>jdl</code> các code sẽ được tạo ra theo format đã config trước đó (chưa thử)</li><li>Chạy frontend &#x3D; <code>npm start</code>. (code frontend auto hotswap)</li><li>Sử dụng liquibase để detect change database. (<code>src/main/resources/config/liquibase/master.xml</code>)</li><li>Dữ liệu mẫu để fake, là file .csv tại: <code>src/main/resources/config/liquibase/fake-data</code></li><li>Cách khai báo file <code>jdl</code> quan hệ 1 - nhiều:</li></ul><pre><code>    entity Blog &#123;    name String required minlength(3),    handle String required minlength(2)    &#125;    entity Entry &#123;    title String required,    content TextBlob required,    date Instant required    &#125;    entity Tag &#123;    name String required minlength(2)    &#125;    relationship ManyToOne &#123;    Blog&#123;user(login)&#125; to User,    Entry&#123;blog(name)&#125; to Blog    &#125;    relationship ManyToMany &#123;    Entry&#123;tag(name)&#125; to Tag&#123;entry&#125;    &#125;    paginate Entry, Tag with infinite-scroll</code></pre><p>  <a href="https://github.com/mraible/jhipster6-demo">https://github.com/mraible/jhipster6-demo</a></p><ul><li>Customize Repository  (Sử dụng trong trường hợp JPA không default sẵn)</li></ul><pre><code class="java">    @Query(&quot;select p from Product p where p.status =:status and p.fFirstId =:f1Id order by p.createdDate desc&quot;)    Page&lt;Product&gt; findAllByStatusAndFFirstIdOrderByCreatedDateDesc(Pageable pageable, @Param(&quot;status&quot;) String status, @Param(&quot;f1Id&quot;) Long f1Id);    //    @Modifying(clearAutomatically = true)    @Query(&quot;update Product p set p.fFirstId =:f1Id where p.fSecondId =:f2Id&quot;)    void updateF2(@Param(&quot;f1Id&quot;) Long f1Id, @Param(&quot;f2Id&quot;) Long f2Id);</code></pre><ul><li>Customize DAOImpl</li></ul><pre><code class="java">@Override    public List&lt;F2summaryDto&gt; getF2SummaryList() &#123;        String sql = &quot;select &quot; +            &quot;       f2.id as f2Id,\n&quot; +            &quot;       f2.name as f2Name,\n&quot; +            &quot;       f2.url  as f2Url,\n&quot; +            &quot;       case\n&quot; +            &quot;           when p_count.counter is null then 0\n&quot; +            &quot;           else p_count.counter\n&quot; +            &quot;           end as counter,\n&quot; +            &quot;       f1.id as f1Id,\n&quot; +            &quot;       f1.name as f1Name,\n&quot; +            &quot;       f1.url  as f1Url\n&quot; +            &quot;from fsecond f2\n&quot; +            &quot;         left join (select count(*) as counter, f_second_id from product &quot; +            &quot;   where status = &#39;new&#39; group by f_second_id) as p_count\n&quot; +            &quot;                   on p_count.f_second_id = f2.id\n&quot; +            &quot;         left join ffirst f1 on f2.ffirst_id = f1.id &quot;;        List&lt;Object[]&gt; queryResult = entityManager.createNativeQuery(sql).getResultList();        List&lt;F2summaryDto&gt; result = new ArrayList&lt;&gt;();        queryResult.stream().forEach((record) -&gt; &#123;            Long f2Id = ((BigInteger) record[0]).longValue();            String f2Name = (String) record[1];            String f2Url = (String) record[2];            int counter = ((BigInteger) record[3]).intValue();            Long f1Id = ((BigInteger) record[4]).longValue();            String f1Name = (String) record[5];            String f1Url = (String) record[6];            result.add(new F2summaryDto(f2Id, f2Name, f2Url, counter, f1Id, f1Name, f1Url));        &#125;);        return result;    &#125;</code></pre><ul><li>Trong 1 số trường hợp không muốn chạy app qua proxy (ví dụ reactjs, proxy 9000, 9060). Muốn chạy trực tiếp trên port<br>java, cần config security</li></ul><pre><code class="text">    // config/SecurityConfiguration.java    Sửa format `default-src &#39;self&#39;` thành `default-src *`</code></pre><ul><li>Thêm script&#x2F;css common tại: <code>src/main/webapp/index.html</code></li><li>Config accept Javascript trong reactjs .ts file <code>.eslintrc.json</code></li></ul><pre><code class="json">    &quot;rules&quot;: &#123;        &quot;no-console&quot;: 0    &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> jhipster </tag>
            
            <tag> auto tool </tag>
            
            <tag> gen code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stories - Query SQL on S3</title>
      <link href="/2020/04/Stories/BackupRestoreAthenaS3/"/>
      <url>/2020/04/Stories/BackupRestoreAthenaS3/</url>
      
        <content type="html"><![CDATA[<h1 id="Diary-of-Finding-a-“SQL-Query”-Solution-on-Object-Storage"><a href="#Diary-of-Finding-a-“SQL-Query”-Solution-on-Object-Storage" class="headerlink" title="Diary of Finding a “SQL Query” Solution on Object Storage"></a>Diary of Finding a “SQL Query” Solution on Object Storage</h1><p>I am building a feature that allows users to backup&#x2F;restore their data. The data they need to backup is from RDBMS SQL. The solution is to query SQL by userId, then export the results to a data file (such as .csv) and upload it to S3. There will be a database logging the location of the object file on S3, or the path will be created according to some formula. When restoration is needed, the data will be downloaded to execute the business logic.</p><h2 id="Challenges-and-Solution-Search"><a href="#Challenges-and-Solution-Search" class="headerlink" title="Challenges and Solution Search"></a>Challenges and Solution Search</h2><p>The only thing that excites me about this problem is finding a tool that can “understand” the data I store on S3.</p><p>Initially, I thought of AWS Athena because I had read somewhere that Athena allows direct SQL queries on S3. I expected there to be a cool mechanism between Athena and S3, and my application would call Athena’s APIs without handling File IO. This would allow me to avoid downloading the CSV file from S3 and parsing it to get a list of objects; instead, I wanted to stream the data via API.</p><h2 id="Evaluating-AWS-Athena"><a href="#Evaluating-AWS-Athena" class="headerlink" title="Evaluating AWS Athena"></a>Evaluating AWS Athena</h2><p>Unfortunately, after researching AWS Athena, I found it unsuitable for my case. Athena is great for analyzing large data files but not for my purpose.</p><p>Some limitations of Athena include:</p><ul><li>Athena allows a maximum of 100 databases per account.</li><li>Athena limits the number of concurrent queries (up to 5 queries).</li></ul><p>With my initial intention, this failed because it is impossible to create a separate database for each CSV file.</p><h2 id="Exploring-More-Solutions"><a href="#Exploring-More-Solutions" class="headerlink" title="Exploring More Solutions"></a>Exploring More Solutions</h2><p>I found many keywords related to Apache Drill, but it is not serverless, so it was eliminated.</p><p>During my research, I discovered that an engine is needed to query object storage like PrestoDB, AVRO, Parquet… This is a significant issue that big companies like Facebook, Google, and AWS are all using some core to address. Perhaps in the future, if I have the chance, I will revisit and explore it further.</p><h2 id="Final-Solution-S3-Select"><a href="#Final-Solution-S3-Select" class="headerlink" title="Final Solution - S3 Select"></a>Final Solution - S3 Select</h2><p>Finally, I found S3 Select, which is quite suitable for my needs. However, S3 Select cannot query data at a specified offset.</p><p>For example, my CSV file has 1000 rows, and I want to retrieve 500 rows per API request. S3 Select does not support specifying the offset for the query.</p><p>To solve this problem, I can customize it by adding a NumberSequence column when creating the CSV file. When querying, I will add a condition like <code>WHERE 501 &lt; NumberSequence &lt; 1000</code>.</p><h2 id="Limitations-of-S3-Select"><a href="#Limitations-of-S3-Select" class="headerlink" title="Limitations of S3 Select"></a>Limitations of S3 Select</h2><p>S3 Select has limitations on the input and output size. If I meticulously calculate the number of columns and the maximum size of each column, I might determine the maximum size for each row.</p><p>However, this is not absolutely certain, so I will handle the logic in the code, with each query retrieving N rows quickly.</p>]]></content>
      
      
      <categories>
          
          <category> stories </category>
          
      </categories>
      
      
        <tags>
            
            <tag> s3 </tag>
            
            <tag> query sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PASETO - Token base authentication</title>
      <link href="/2020/04/Other/Paseto/"/>
      <url>/2020/04/Other/Paseto/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Use-Cases"><a href="#1-Use-Cases" class="headerlink" title="1. Use Cases"></a>1. Use Cases</h1><h2 id="1-1-Use-Case-1"><a href="#1-1-Use-Case-1" class="headerlink" title="1.1. Use Case 1"></a>1.1. Use Case 1</h2><p>You are building a system with two applications:</p><ul><li><strong>Web application:</strong> Allows users to pay for file downloads.</li><li><strong>Download service application:</strong> Provides the file download service via a link containing a token from the Web Application.</li></ul><h3 id="Desired-Scenario"><a href="#Desired-Scenario" class="headerlink" title="Desired Scenario:"></a>Desired Scenario:</h3><ul><li>The user makes a payment on the website.</li><li>The website verifies the payment and generates a download link (with a token), returning it to the user.</li><li>The user uses the download link to download the file.</li></ul><p><strong>Problem:</strong> How can the Download service validate the download request URL without querying the database?</p><h2 id="1-2-Use-Case-2"><a href="#1-2-Use-Case-2" class="headerlink" title="1.2. Use Case 2"></a>1.2. Use Case 2</h2><p>You are building two systems:</p><ul><li><strong>Authorization service:</strong> Manages user login and permissions.</li><li><strong>Website:</strong> Allows user login.</li></ul><h3 id="Desired-Scenario-1"><a href="#Desired-Scenario-1" class="headerlink" title="Desired Scenario:"></a>Desired Scenario:</h3><ul><li>The user accesses the website and logs in.</li><li>The website redirects the user to the authorization service.</li><li>The user fills in the login form at the authorization service.</li><li>The authorization service verifies the credentials and redirects the user back to the website with a token.</li><li>The website receives the token, validates it, and creates a user session.</li></ul><p><strong>Problem:</strong> How can the website ensure the token is valid and issued by the authorization service, preventing middle-man attacks?</p><h1 id="2-PASETO"><a href="#2-PASETO" class="headerlink" title="2. PASETO"></a>2. PASETO</h1><ul><li>PASETO (Platform-Agnostic SEcurity TOkens) is a protocol for token-based authentication.</li><li>It is a stateless token, meaning it can validate itself without needing additional storage or queries.</li><li>PASETO has two modes: LOCAL (for use case 1) and PUBLIC (for use case 2).</li></ul><h2 id="2-1-LOCAL-Mode"><a href="#2-1-LOCAL-Mode" class="headerlink" title="2.1 LOCAL Mode"></a>2.1 LOCAL Mode</h2><p>Token format: <code>v1.local.payload.optional_footer</code></p><h3 id="Example"><a href="#Example" class="headerlink" title="Example:"></a>Example:</h3><pre><code>v1.local.CuizxAzVIz5bCqAjsZpXXV5mk_WWGHbVxmdF81DORwyYcMLvzoUHUmS_VKvJ1hn5zXyoMkygkEYLM2LM00uBI3G9gXC5VrZCUM.BLZo1q9IDIncAZTxYkE1NUTMz</code></pre><ul><li><strong>v1:</strong> PASETO version.</li><li><strong>local:</strong> LOCAL mode.</li><li><strong>payload:</strong> A JSON object, encrypted.</li><li><strong>optional_footer:</strong> Contains metadata, not encrypted.</li></ul><h3 id="How-it-works"><a href="#How-it-works" class="headerlink" title="How it works:"></a>How it works:</h3><p>LOCAL mode uses symmetric encryption, meaning the same key is used for both encryption and decryption (e.g., AES algorithm).</p><h3 id="JSON-Payload-Fields"><a href="#JSON-Payload-Fields" class="headerlink" title="JSON Payload Fields:"></a>JSON Payload Fields:</h3><pre><code class="text">+-----+------------+--------+-------------------------------------+| Key |    Name    |  Type  |               Example               |+-----+------------+--------+-------------------------------------+| iss |   Issuer   | string |       &#123;&quot;iss&quot;:&quot;paragonie.com&quot;&#125;       || sub |  Subject   | string |            &#123;&quot;sub&quot;:&quot;test&quot;&#125;           || aud |  Audience  | string |       &#123;&quot;aud&quot;:&quot;pie-hosted.com&quot;&#125;      || exp | Expiration | DtTime | &#123;&quot;exp&quot;:&quot;2039-01-01T00:00:00+00:00&quot;&#125; || nbf | Not Before | DtTime | &#123;&quot;nbf&quot;:&quot;2038-04-01T00:00:00+00:00&quot;&#125; || iat | Issued At  | DtTime | &#123;&quot;iat&quot;:&quot;2038-03-17T00:00:00+00:00&quot;&#125; || jti |  Token ID  | string |  &#123;&quot;jti&quot;:&quot;87IFSGFgPNtQNNuw0AtuLttP&quot;&#125; || kid |   Key-ID   | string |    &#123;&quot;kid&quot;:&quot;stored-in-the-footer&quot;&#125;   |+-----+------------+--------+-------------------------------------+</code></pre><ul><li>Typically, the <code>exp</code> and <code>iat</code> fields are used to check the token’s validity period.</li></ul><h3 id="Example-for-solving-use-case-1"><a href="#Example-for-solving-use-case-1" class="headerlink" title="Example for solving use case 1:"></a>Example for solving use case 1:</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/paseto/baitoan1.gif" alt="Baitoan1"></p><h2 id="2-2-PUBLIC-Mode"><a href="#2-2-PUBLIC-Mode" class="headerlink" title="2.2 PUBLIC Mode"></a>2.2 PUBLIC Mode</h2><p>Token format: <code>v1.public.payload.optional_footer</code></p><h3 id="How-it-works-1"><a href="#How-it-works-1" class="headerlink" title="How it works:"></a>How it works:</h3><p>PUBLIC mode uses asymmetric encryption, meaning there is a pair of keys:<br>a private key for encryption and a public key for decryption.</p><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/paseto/format.JPG" alt="Format"></p><h3 id="Example-for-solving-use-case-2"><a href="#Example-for-solving-use-case-2" class="headerlink" title="Example for solving use case 2:"></a>Example for solving use case 2:</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/paseto/baitoan2.gif" alt="Baitoan2"></p><h1 id="3-Comparison-between-PASETO-and-JWT"><a href="#3-Comparison-between-PASETO-and-JWT" class="headerlink" title="3. Comparison between PASETO and JWT"></a>3. Comparison between PASETO and JWT</h1><h2 id="3-1-Similarities"><a href="#3-1-Similarities" class="headerlink" title="3.1 Similarities"></a>3.1 Similarities</h2><ul><li>Both are protocols for token-based authentication.</li><li>Payload is a JSON object.</li><li>Both are stateless tokens.</li><li>Both include an “expire time” field in the payload to check the token’s validity period.</li></ul><h2 id="3-2-Differences"><a href="#3-2-Differences" class="headerlink" title="3.2 Differences"></a>3.2 Differences</h2><table><thead><tr><th>Difference</th><th>JWT</th><th>PASETO</th></tr></thead><tbody><tr><td>Self-validation method</td><td>Decode payload + header with base64, then hash with the secret key</td><td>Decrypt payload with shareKey or publicKey</td></tr><tr><td>Number of modes</td><td>Only 1 mode</td><td>Two modes: local and public, chosen based on use case</td></tr></tbody></table><h2 id="3-3-Weaknesses-of-JWT"><a href="#3-3-Weaknesses-of-JWT" class="headerlink" title="3.3 Weaknesses of JWT"></a>3.3 Weaknesses of JWT</h2><ul><li>An attacker can modify the <code>alg</code> field in the header to change the hashing algorithm to a weaker one, increasing vulnerability if the server does not check <code>alg</code>.</li></ul><h1 id="4-References"><a href="#4-References" class="headerlink" title="4. References"></a>4. References</h1><h2 id="4-1-Algorithms"><a href="#4-1-Algorithms" class="headerlink" title="4.1 Algorithms"></a>4.1 Algorithms</h2><ul><li><strong>v1.local:</strong> AES-256-CTR + HMAC-SHA384.</li><li><strong>v1.public:</strong> 2048-bit RSA.</li><li><strong>v2.local:</strong> XChaCha20-Poly1305.</li><li><strong>v2.public:</strong> Ed25519.</li></ul><h2 id="4-2-Related-Links"><a href="#4-2-Related-Links" class="headerlink" title="4.2 Related Links"></a>4.2 Related Links</h2><ul><li><a href="https://developer.okta.com/blog/2019/10/17/a-thorough-introduction-to-paseto">Original article</a></li><li><a href="https://paseto.io/">PASETO homepage</a></li><li><a href="https://docs.google.com/presentation/d/1Rn4xQWB0NCKvy7_lcyowZGz0QPvbAskGjNdYLmuQMhY">NoWayJoseCPV2018</a></li></ul><p><strong>Note:</strong> When experimenting with <code>v2</code>, you may need to install <code>sodium</code> for OS support for the algorithms used in PASETO.</p>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> token </tag>
            
            <tag> paseto </tag>
            
            <tag> authen </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Install &amp; Config Note</title>
      <link href="/2020/04/Install_Guide/Install_Config_Note/"/>
      <url>/2020/04/Install_Guide/Install_Config_Note/</url>
      
        <content type="html"><![CDATA[<h1 id="Install-config-note"><a href="#Install-config-note" class="headerlink" title="Install &amp; config note"></a>Install &amp; config note</h1><h2 id="Selenium-ChromeDriver"><a href="#Selenium-ChromeDriver" class="headerlink" title="Selenium ChromeDriver"></a>Selenium ChromeDriver</h2><p><a href="https://tecadmin.net/setup-selenium-chromedriver-on-ubuntu/">https://tecadmin.net/setup-selenium-chromedriver-on-ubuntu/</a></p><h2 id="OpenVPN"><a href="#OpenVPN" class="headerlink" title="OpenVPN"></a>OpenVPN</h2><p><a href="https://support.hidemyass.com/hc/en-us/articles/202721546-OpenVPN-via-terminal-using-openvpn-binary-the-manual-way-">https://support.hidemyass.com/hc/en-us/articles/202721546-OpenVPN-via-terminal-using-openvpn-binary-the-manual-way-</a></p><ul><li><a href="https://computingforgeeks.com/easy-way-to-install-and-configure-openvpn-server-on-ubuntu-18-04-ubuntu-16-04/">Easy Way to Install and Configure OpenVPN Server on Ubuntu 18.04 &#x2F; Ubuntu 16.04</a></li></ul><h2 id="Install-PostgreSQL-12-on-linux"><a href="#Install-PostgreSQL-12-on-linux" class="headerlink" title="Install PostgreSQL 12 on linux"></a>Install PostgreSQL 12 on linux</h2><p><a href="https://computingforgeeks.com/install-postgresql-12-on-ubuntu/">https://computingforgeeks.com/install-postgresql-12-on-ubuntu/</a></p><h2 id="Aria2"><a href="#Aria2" class="headerlink" title="Aria2"></a>Aria2</h2><ul><li><code>aria2</code>: tool download</li><li><code>webui-aria2</code>: cung cấp giao diện webui cho aria2</li></ul><ol><li>Install</li></ol><p>ref: <a href="https://hub.docker.com/r/timonier/webui-aria2">https://hub.docker.com/r/timonier/webui-aria2</a></p><ul><li>install <code>aria2</code></li></ul><pre><code class="bash"># Define installation folderexport INSTALL_DIRECTORY=/usr/bin# Use local installationsudo bin/installer install# Use remote installationcurl --location &quot;https://gitlab.com/timonier/aria2/raw/master/bin/installer&quot; | sudo sh -s -- install# See all aria2c optionsaria2c --help</code></pre><ul><li>run aria2</li></ul><pre><code class="bash"># 1. plainaria2c --dir /home/ubuntu/torrents --enable-rpc --rpc-listen-all# 2. dùng nohupnohup  aria2c --dir /home/ubuntu/torrents --enable-rpc --rpc-listen-all &gt;&gt; /tmp/aria2c.log 2&gt;&amp;1&amp;# chỉnh &quot;/home/ubuntu/torrents&quot; thành đường dẫn mà file sau khi download sẽ được lưu vào# lưu ý sau khi run, thì 1 container docker mới sẽ được chạy</code></pre><ul><li>install <code>webui-aria2</code></li></ul><pre><code class="bash">docker run -d -p 9999:80 timonier/webui-aria2</code></pre><h2 id="Node-Npm"><a href="#Node-Npm" class="headerlink" title="Node, Npm"></a>Node, Npm</h2><pre><code class="bash">$ sudo apt-get install npm(...apt installation of npm was successful...)$ npm -v3.5.2$ command -v npm/usr/bin/npm$ sudo npm install -g npm(...npm installation of npm was successful...so far, so good)$ type npmnpm is hashed (/usr/bin/npm)hash -d npm$ npm -v6.4.1$ command -v npm/usr/local/bin/npm</code></pre><h2 id="Postgresql"><a href="#Postgresql" class="headerlink" title="Postgresql"></a>Postgresql</h2><pre><code class="bash"> docker run --name postgres-crawler1688 \    -e POSTGRES_PASSWORD=crawler1688a@ \    -v /home/ubuntu/docker/postgres_data:/var/lib/postgresql/data  \    -p 5432:5432 \    -d postgres</code></pre><h3 id="pgadmin4-webui-cho-postgresql"><a href="#pgadmin4-webui-cho-postgresql" class="headerlink" title="pgadmin4 (webui cho postgresql)"></a>pgadmin4 (webui cho postgresql)</h3><pre><code class="bash">docker run -p 8083:80 \    -e &#39;PGADMIN_DEFAULT_EMAIL=admin&#39; \    -e &#39;PGADMIN_DEFAULT_PASSWORD=password@&#39; \    -d dpage/pgadmin4</code></pre><h3 id="install-psql-client"><a href="#install-psql-client" class="headerlink" title="install psql client"></a>install psql client</h3><pre><code class="bash">wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main&quot; |sudo tee  /etc/apt/sources.list.d/pgdg.listsudo apt updatesudo apt-get install postgresql-clientpg_dump -h crawler1688-s2.tungexplorer.me -U postgres -d crawler1688  --exclude-table=exclude_id_seq &gt; backup_crawler1688_`date +%Y_%m_%d`.sql</code></pre><h2 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h2><ul><li>prometheus_example.yml</li></ul><pre><code class="yaml"># my global configglobal:  scrape_interval:     1s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 5s # Evaluate rules every 15 seconds. The default is every 1 minute.  # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting:  alertmanagers:    - static_configs:        - targets:          # - alertmanager:9093# Load rules once and periodically evaluate them according to the global &#39;evaluation_interval&#39;.rule_files:# - &quot;first_rules.yml&quot;# - &quot;second_rules.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it&#39;s Prometheus itself.scrape_configs:  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.  - job_name: &#39;prometheus&#39;    # metrics_path defaults to &#39;/metrics&#39;    # scheme defaults to &#39;http&#39;.    static_configs:      - targets: [&#39;localhost:9090&#39;]</code></pre><h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><ul><li>Download package</li></ul><pre><code class="bash">wget https://download.java.net/openjdk/jdk11/ri/openjdk-11+28_linux-x64_bin.tar.gzwget https://download.oracle.com/otn/java/jdk/8u241-b07/1f5b5a70bf22433b84d0e960903adac8/jdk-8u241-linux-x64.tar.gz?AuthParam=1586232610_d9b5e1b404c1d80ee03b9ad36c391ed6</code></pre><ul><li>Extract</li></ul><pre><code class="bash">tar zxvf openjdk-11.0.2_linux-x64_bin.tar.gz</code></pre><pre><code class="bash">sudo mv jdk-11* /usr/local/</code></pre><ul><li>Set environment variables</li></ul><pre><code class="bash">sudo nano /etc/profile.d/jdk.sh</code></pre><ul><li>Add</li></ul><pre><code class="bash">export JAVA_HOME=/usr/local/jdk-11.0.2export PATH=$PATH:$JAVA_HOME/bin</code></pre><ul><li>Can set JAVA_HOME, PATH env in:</li></ul><pre><code>/etc/environment</code></pre><p>or</p><pre><code>~/.basrhc</code></pre><p>or new file in <code>/etc/profile.d/</code></p><pre><code>/etc/profile.d/jdk.sh</code></pre><p>Source env when startup OS</p><pre><code>source $file</code></pre><h3 id="Common-errors"><a href="#Common-errors" class="headerlink" title="Common errors"></a>Common errors</h3><ul><li>Should append PATH (not replace)</li></ul><p>&#x2F;&#x2F; Sưu tầm: <a href="https://computingforgeeks.com/how-to-install-java-11-on-ubuntu-18-04-16-04-debian-9">https://computingforgeeks.com/how-to-install-java-11-on-ubuntu-18-04-16-04-debian-9</a></p><h3 id="Intellij"><a href="#Intellij" class="headerlink" title="Intellij"></a>Intellij</h3><pre><code>- File &gt; Project Structure &gt; Platform Settings &gt; SDKs   - Click button (+) `Add new sdk` &gt; Download JDK         - Chọn Vendor và Version &gt; Click Download</code></pre><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><ul><li>docker-compose_install_broker_zookeeper.yml</li></ul><pre><code class="yaml">version: &#39;2&#39;services:  zookeeper:    image: confluentinc/cp-zookeeper:5.4.0    hostname: zookeeper    container_name: zookeeper    ports:      - &quot;2181:2181&quot;    environment:      ZOOKEEPER_CLIENT_PORT: 2181      ZOOKEEPER_TICK_TIME: 2000  broker:    image: confluentinc/cp-server:5.4.0    hostname: broker    container_name: broker    depends_on:      - zookeeper    ports:      - &quot;9092:9092&quot;    environment:      KAFKA_HEAP_OPTS: -Xmx256M -Xms256M      KAFKA_BROKER_ID: 1      KAFKA_ZOOKEEPER_CONNECT: &#39;zookeeper:2181&#39;      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://tungexplorer.me:9092      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1      CONFLUENT_METRICS_ENABLE: &#39;true&#39;      CONFLUENT_SUPPORT_CUSTOMER_ID: &#39;anonymous&#39;</code></pre><h2 id="docker-compose-install-kafdrop-yml"><a href="#docker-compose-install-kafdrop-yml" class="headerlink" title="docker-compose_install_kafdrop.yml"></a>docker-compose_install_kafdrop.yml</h2><pre><code class="yaml">version: &quot;2&quot;services:  kafdrop:    image: obsidiandynamics/kafdrop    restart: &quot;no&quot;    ports:      - &quot;9009:9000&quot;    environment:      KAFKA_BROKERCONNECT: &quot;tungexplorer.me:9092&quot; </code></pre><h2 id="Install-Node-js-http-server"><a href="#Install-Node-js-http-server" class="headerlink" title="Install Node js http server"></a>Install Node js http server</h2><pre><code class="bash">docker run --name file-server -p 8082:8080 -v /home/ubuntu/torrents:/torrents -w /torrents -t cannin/nodejs-http-server</code></pre><h3 id="Install-qbittorrent"><a href="#Install-qbittorrent" class="headerlink" title="Install qbittorrent"></a>Install qbittorrent</h3><pre><code class="bash">docker run -d -v /home/ubuntu/torrents:/downloads -p 9998:8080 --name torrent linuxserver/qbittorrent# account: admin/adminadmin</code></pre>]]></content>
      
      
      <categories>
          
          <category> install_guide </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Code Template - Google Translate</title>
      <link href="/2020/03/CodeTemplate/GoogleTranslateExmple/"/>
      <url>/2020/03/CodeTemplate/GoogleTranslateExmple/</url>
      
        <content type="html"><![CDATA[<pre><code class="java">package com.mservice;import com.google.cloud.translate.Detection;import com.google.cloud.translate.Translate;import com.google.cloud.translate.TranslateOptions;import com.google.cloud.translate.Translation;import java.util.ArrayList;import java.util.List;public class TestGoogleTranslate &#123;    /*          &lt;dependency&gt;            &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;            &lt;artifactId&gt;google-cloud-translate&lt;/artifactId&gt;            &lt;version&gt;1.94.4&lt;/version&gt;        &lt;/dependency&gt;    */    public static final String KEY = &quot;gooogle_api_key&quot;;    public static final String TEXT = &quot;秋冬重磅货意大利软糯加厚大衣女中长款外套连帽过膝开襟开衫女 \n 开襟开衫女&quot;;    public static final String TEXT2 = &quot;hello&quot;;    public static void main(String[] args) &#123;        Translate translate = TranslateOptions.newBuilder().setApiKey(KEY).build().getService();        List&lt;String&gt; temp = new ArrayList&lt;&gt;();        temp.add(&quot;秋冬重磅货意大利软糯加厚大衣女中长款外套连帽过膝开襟开衫女&quot;);        temp.add(&quot;开襟开衫女&quot;);        List&lt;Translation&gt; temp2 = translate.translate(temp, Translate.TranslateOption.sourceLanguage(&quot;zh-CN&quot;),                Translate.TranslateOption.targetLanguage(&quot;vi&quot;));        Detection detection = translate.detect(TEXT);        String detectedLanguage = detection.getLanguage();        Translation translation = translate.translate(TEXT, Translate.TranslateOption.sourceLanguage(&quot;zh-CN&quot;),                Translate.TranslateOption.targetLanguage(&quot;vi&quot;));        System.out.println(translation.getTranslatedText());    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> code_template </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code template </tag>
            
            <tag> Google Translate </tag>
            
            <tag> translate </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CICD - Flow Example</title>
      <link href="/2020/02/Ops/CICD_FLow_Example/"/>
      <url>/2020/02/Ops/CICD_FLow_Example/</url>
      
        <content type="html"><![CDATA[<h2 id="Example-1-Jenkins"><a href="#Example-1-Jenkins" class="headerlink" title="Example 1 - Jenkins"></a>Example 1 - Jenkins</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/cicd/Diagram_Example_1.png" alt="Example 1"></p><ul><li>Có 2 repository, 1 repo chứa source code của developer đẩy lên, 1 repo chứa file application (ex: .jar, .dll…, file<br>này được build từ source code)</li><li>Trong mỗi repository tạo file <code>Jenkinsfile</code> chứa script <code>pipelines</code>. (giúp hạn chế việc viết script ở Jenkins -&gt;<br>developer chủ động hơn trong việc chỉnh sửa file Jenkins)</li><li>Ở mỗi Git repository (ví dụ gitlab, github…) có feature <code>intergrations</code> đẩy <code>hooks/ notification</code></li><li>Jenkins khi nhận được <code>hooks</code> cần detect được <code>branch/label</code> nào</li></ul>]]></content>
      
      
      <categories>
          
          <category> cicd_ops </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cicd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>E-Wallet Note</title>
      <link href="/2020/02/Other/EWallet/"/>
      <url>/2020/02/Other/EWallet/</url>
      
        <content type="html"><![CDATA[<h1 id="Note-ve-tich-hop-vi-dien-tu"><a href="#Note-ve-tich-hop-vi-dien-tu" class="headerlink" title="Note về tích hợp ví điện tử"></a>Note về tích hợp ví điện tử</h1><h2 id="vi-MOMO"><a href="#vi-MOMO" class="headerlink" title="ví MOMO"></a>ví MOMO</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/payment/momo-payment-flow.jpg" alt="Payment"></p><ul><li><p>Tích hợp trên nền tảng website</p></li><li><p>Có nét tương đồng với Ví Bảo Kim (có thể các ví điện tử sẽ chung flow)</p></li><li><p>Key credential</p><ul><li><code>Partner Code</code>: Thông tin để định danh tài khoản doanh nghiệp.</li><li><code>Access Key</code>: Cấp quyền truy cập vào hệ thống MoMo.</li><li><code>Secret Key</code>: Dùng để tạo chữ ký điện tử signature.</li><li><code>Public Key</code>: Sử dụng để tạo mã hoá dữ liệu bằng thuật toán RSA.</li></ul></li><li><p>Ý nghĩa của <code>Secret key</code>: khi nhận data từ <code>momo</code>, server tích hợp sẽ sử dụng nó để làm khóa cho hàm băm. (các thuật<br>toán băm như md5, sha1, sha256…). đối tượng băm là <code>payload</code>. Sau khi có kết quả băm, sẽ compare với giá trị <code>hash</code><br>được gửi kèm trong payload. Nếu bằng nhau thì xác nhận là đúng momo gửi data.</p><pre><code class="java">String signResponse = Encoder.signHmacSHA256(responserawData, getSecretKey());if (signResponse.equals(captureMoMoResponse.getSignature())) &#123;    return oke;&#125; else &#123;    throw new IllegalArgumentException(&quot;Wrong signature&quot;);&#125;</code></pre></li><li><p><code>hash</code> là 1 chiều, chỉ encode, không thể decode</p></li><li><p><code>Public key</code> - được sử dụng trong thuật toán mã hóa RSA (async- encrypt là 1 khóa, decrypt là 1 khóa khác, ai có<br>public key cũng có thể khóa, nhưng chỉ có momo có <code>private key</code> mới có thể giải mã, ví dụ giải thuật AES). Server tích<br>hợp không được cấp <code>Private key</code>. Server tích hợp sẽ không gửi <code>payload</code> “thô” cho momo. Mà gửi payload đã<br>được <code>encrypt</code> với <code>publickey</code> cho momo.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/payment/momo-encrypt-1.JPG" alt="Encprypt1"></p></li><li><p>Momo gửi IPN (instant payment notification &#x2F; 1 kiểu kiểu như webhook) cho merchant server thì dùng Secret key</p></li><li><p>Merchant server gửi request cho momo thì dùng Public key</p></li><li><p>Trong <code>Create payment request</code> sẽ luôn có 2 trường</p><ul><li><code>returnUrl</code>: Một URL của đối tác. URL này được sử dụng để chuyển trang (redirect) từ MoMo về trang mua hàng của<br>đối tác sau khi khách hàng thanh toán.</li><li><code>notifyUrl</code>: địa chỉ nhận IPN</li></ul></li><li><p><code>NotifyURL</code> sinh ra để tránh trường hợp attacker fake url request tới momo. Merchant server sẽ sử dụng <code>notifyurl</code> để<br>nhận data từ momo báo về kết quả giao dịch, verify chúng, trước khi xác nhận cho buyer giao dịch thành công.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> e-wallet </tag>
            
            <tag> momo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>My Bookmarks</title>
      <link href="/2020/02/Other/Bookmarks/"/>
      <url>/2020/02/Other/Bookmarks/</url>
      
        <content type="html"><![CDATA[<h1 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h1><ul><li><a href="https://microservices.io/patterns/data/transactional-outbox.html">Transactional outbox</a></li><li><a href="https://www.threatstack.com/blog/101-aws-security-tips-quotes-part-3-best-practices-for-using-security-groups-in-aws">Best Practices for Using Security Groups in AWS</a></li><li><a href="https://jayendrapatil.com/aws-ec2-placement-groups/">EC2 placement group</a></li><li><a href="https://www.logicbig.com/tutorials/misc/jackson.html">Jackson JSON</a></li><li><a href="https://www.sumologic.com/insight/s3-cost-optimization/?fbclid=IwAR2MSTHmJH9eWQlgLm5bmA8vfuXxBZIz8_-0C3xje65D3GYR6ApgBb1wsCU">AWS S3 Cost Optimization</a></li><li><a href="https://viblo.asia/p/ansible-fundamentals-ad-hoc-commands-bJzKmk16l9N">Ansible Fundamentals - Ad-hoc Commands</a></li><li><a href="https://techtalk.vn/kpt-keep-problem-try-nhung-dieu-can-biet.html">KPT (KEEP – PROBLEM – TRY)</a></li><li><a href="https://serverless-training.com/articles/save-money-by-replacing-api-gateway-with-application-load-balancer/">Saving Money By Replacing API Gateway With Application Load Balancer’s Lambda Integration</a></li><li><a href="https://paragonie.com/files/talks/NoWayJoseCPV2018.pdf">NoWayJoseCPV2018</a></li><li><a href="https://svlada.com/jwt-token-authentication-with-spring-boot/?fbclid=IwAR3OBFK9XX0qfEbjFt3EiF765yUeH8P4xRJyGYFlaY7Qa2hb-vBtreonHPo">JWT Authentication Tutorial - An example using Spring Boot</a></li><li><a href="https://jack-vanlightly.com/blog/2018/8/31/rabbitmq-vs-kafka-part-5-fault-tolerance-and-high-availability-with-rabbitmq">HA trong RabbitMQ</a></li><li><a href="https://www.youtube.com/watch?v=nwBBd9GrcqI">Connection pool</a></li><li><a href="https://pgtune.leopard.in.ua/?fbclid=IwAR2-s6Lqh_Q-8CI24InxwUwjbEFg4Go5E7LmImbi5t4HKNLVWfYITWNueCw#/">Postgresql config evalute</a></li><li><a href="https://blog.overops.com/jenkins-vs-travis-ci-vs-circle-ci-vs-teamcity-vs-codeship-vs-gitlab-ci-vs-bamboo/">Why should you use a tool for your CI&#x2F;CD workflow, and which one is the right tool for you?</a></li><li><a href="https://trinhhieu668.wordpress.com/2020/02/08/architechture-dataware-house-at-zitga/?fbclid=IwAR2lbEEBiWRWnyd0T4zD22_VLe_yPJVn4hPJNj0xaGlXF4ZD_Mmze2Hr9Sw">Architecture data warehouse at Zitga</a></li><li>at least once at most once exactly once</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Authentication Topic</title>
      <link href="/2020/02/Other/Authentication_Topic/"/>
      <url>/2020/02/Other/Authentication_Topic/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Basic-Authentication"><a href="#1-Basic-Authentication" class="headerlink" title="1. Basic Authentication"></a>1. Basic Authentication</h2><ul><li>The <code>Authorization</code> header for Basic Authentication looks like this: <code>Authorization: Basic YWJjOjEyMw==</code>. Here, <code>YWJjOjEyMw==</code> is the <code>base64</code> encoded string of <code>abc:123</code> (username &#x3D; abc, password &#x3D; 123).</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/authen_topic/basicAuthFlow.jpg" alt="BasicAuthen"></p><h2 id="2-Session-based-Authentication"><a href="#2-Session-based-Authentication" class="headerlink" title="2. Session-based Authentication"></a>2. Session-based Authentication</h2><ul><li>The Session ID will appear in subsequent HTTP requests within the Cookie header (e.g., <code>Cookie: SESSION_ID=abc</code>).</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/authen_topic/Session-based_Authentication.jpg" alt="SessionAuthen"></p><h2 id="3-Token-based-Authentication"><a href="#3-Token-based-Authentication" class="headerlink" title="3. Token-based Authentication"></a>3. Token-based Authentication</h2><ul><li>Tokens, often self-contained (like JWT), can be verified for correctness using encryption and decryption algorithms based on the token’s information and a secret key of the server. Therefore, the server does not need to store the token or query user information to verify the token.</li><li>The token will appear in subsequent HTTP requests within the Authorization header (e.g., <code>Authorization: Bearer abc</code>).</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/authen_topic/TokenBased.jpg" alt="TokenAuthen"></p><h2 id="4-Comparison"><a href="#4-Comparison" class="headerlink" title="4. Comparison"></a>4. Comparison</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/authen_topic/compare.JPG" alt="Compare"></p><h2 id="5-OAuth-2-0"><a href="#5-OAuth-2-0" class="headerlink" title="5. OAuth 2.0"></a>5. OAuth 2.0</h2><p><img src="https://shopify.dev/assets/api/oauth-code-grant-flow.png" alt="Oauth2.0"></p><ol><li>The merchant requests to install the app.</li><li>The app redirects to Shopify to load the OAuth grant screen and requests the required scopes.</li><li>Shopify displays a prompt to receive authorization and prompts the merchant to log in if required.</li><li>The merchant consents to the scopes and is redirected to the <code>redirect_uri</code>.</li><li>The app makes an access token request to Shopify including the <code>client_id</code>, <code>client_secret</code>, and code.</li><li>Shopify returns the access token and requested scopes.</li><li>The app uses the token to make requests to the Shopify API.</li><li>Shopify returns the requested data.</li></ol><p><a href="https://shopify.dev/tutorials/authenticate-with-oauth">Reference</a></p><h2 id="6-OIDC-openId-connect"><a href="#6-OIDC-openId-connect" class="headerlink" title="6. OIDC - openId connect"></a>6. OIDC - openId connect</h2><ul><li>OIDC &#x3D; extends OAuth 2.0.</li></ul><h3 id="ID-Token-vs-Access-Token"><a href="#ID-Token-vs-Access-Token" class="headerlink" title="ID Token vs Access Token"></a>ID Token vs Access Token</h3><ul><li><p><strong>ID Token</strong>:</p><ul><li>SHOULD: Be used to get information about the user, such as birthday, address, etc.</li><li>SHOULD: Be used by the app to get user info from the ID provider (e.g., Spring OAuth uses <code>id_token</code> to call API and get user info from the ID provider).</li><li>SHOULD NOT: Be used for authorizing.</li><li>SHOULD NOT: Be sent from client to app.</li></ul></li><li><p><strong>Access Token</strong>:</p><ul><li>SHOULD: Be used for authorizing.</li><li>SHOULD: Be sent from client to app and from app to ID provider.</li></ul></li></ul><h3 id="Authorization-Code-Flow-vs-PKCE"><a href="#Authorization-Code-Flow-vs-PKCE" class="headerlink" title="Authorization Code Flow vs PKCE"></a>Authorization Code Flow vs PKCE</h3><ul><li><p>PKCE is an extension of Authorization Code Flow.</p></li><li><p>Authorization Code Flow requires <code>clientId</code> and <code>clientSecret</code> (used in the step: App -&gt; Authorization Server).</p><ul><li>SHOULD NOT: Be used in Single Page Applications (SPAs) (e.g., React.js, AngularJS) because users can access <code>clientSecret</code> from the web browser.</li><li>SHOULD: Be used in traditional web apps (e.g., Thymeleaf with Spring Boot).</li></ul></li><li><p><strong>PKCE</strong>: More secure.</p><ul><li>The client app (e.g., React.js) needs to create a <code>code_verifier</code> (a random string). <code>HASH(code_verifier) = code_challenge</code>.</li><li>The client app sends the <code>code_challenge</code> to the Authorization Server.</li><li>The Authorization Server stores the <code>code_challenge</code> for later verification.</li><li>After the user authenticates, they are redirected back to the app with an authorization code. The app requests to exchange the code for tokens, sending the <code>code_verifier</code> instead of a fixed secret. The Authorization Server hashes the <code>code_verifier</code> and compares it to the stored hashed value.</li></ul></li></ul><p><img src="https://developer.okta.com/assets-jekyll/blog/okta-authjs-pkce/pkce-59cd81484ee5be4248d4f8efc986070d7d6ac20b8091da3b8377bf1e278a0b54.svg" alt="PKCE Flow"></p><h3 id="What-is-JWKS-URI"><a href="#What-is-JWKS-URI" class="headerlink" title="What is JWKS URI?"></a>What is JWKS URI?</h3><ul><li><strong>JWKS</strong>: JSON Web Key Set.</li><li><strong>JWKS URI</strong>: Endpoint to get the public key, used for token verification.</li><li>This endpoint often has the suffix <code>.well-known/jwks.json</code>.<ul><li>In Keycloak, it has the format <code>/auth/realms/realm1/protocol/openid-connect/certs</code>.</li><li>The format looks like this:</li></ul></li></ul><pre><code class="json">&#123;    &quot;keys&quot;: [        &#123;            &quot;kid&quot;: &quot;mpRkvfWFRXvrmbr_TuiSboWX8PJXIk9jDw-S98_9Yfw&quot;,            &quot;kty&quot;: &quot;RSA&quot;,            &quot;alg&quot;: &quot;RS256&quot;,            &quot;use&quot;: &quot;sig&quot;,            &quot;n&quot;: &quot;qyXuEh5ITO4xaHP2OilF-zi7B-ijvDNvY1AqKUQAqroKSHVTjR5G8jjYKh3vs_-eRc3oIve0l_GnM88L2DwmOFzDYLUTMbc37cb3sd6sZvHeohUMHDSblZHBWkGPUBcAz-7cP5C1ZU6Z9lGSOOSVjsxYMloUi-RrjrtMzC0cgdbCUDxycJLbxH6DR8_pf0_-P30cxwMl6DtDkS4bcHILiWkTaGts-Dw0VF1XU6Dl4MiTp9xPmfeGmoxHGSlDH_--DxY5qESVNRjZt3NcvOGHCwYmZNU0ocUfvJnpdLocaqPbGBYaxVOuFcia52GNlx3rjhpQDJjpiPYb4SMhp5RewQ&quot;,            &quot;e&quot;: &quot;AQAB&quot;,            &quot;x5c&quot;: [                &quot;MIICmzCCAYMCBgF9mvWlFzANBgkqhkiG9w0BAQsFADARMQ8wDQYDVQQDDAZyZWFsbTEwHhcNMjExMjA4MTY1MDI5WhcNMzExMjA4MTY1MjA5WjARMQ8wDQYDVQQDDAZyZWFsbTEwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCrJe4SHkhM7jFoc/Y6KUX7OLsH6KO8M29jUCopRACqugpIdVONHkbyONgqHe+z/55Fzegi97SX8aczzwvYPCY4XMNgtRMxtzftxvex3qxm8d6iFQwcNJuVkcFaQY9QFwDP7tw/kLVlTpn2UZI45JWOzFgyWhSL5GuOu0zMLRyB1sJQPHJwktvEfoNHz+l/T/4/fRzHAyXoO0ORLhtwcguJaRNoa2z4PDRUXVdToOXgyJOn3E+Z94aajEcZKUMf/74PFjmoRJU1GNm3c1y84YcLBiZk1TShxR+8mel0uhxqo9sYFhrFU64VyJrnYY2XHeuOGlAMmOmI9hvhIyGnlF7BAgMBAAEwDQYJKoZIhvcNAQELBQADggEBAA4osUItTVx4ZfZCE0pwrHXlxxioBRj/BB5eGhJjTR8ZK4B30hbyv5BJn04sRDOZmagEFn1xTUjpTyPHzH3thdqSDIqsWrxI1fwCY8l8KD/NeigP25CJn+Zn5YO/2+SfN7JP4eeE4PjhjVVFNTP+UBuMUaGEnbw4yhZZxAYuF4rUOPGJ1V7Fc22d8r8gBIMykIvipdGFDIypQj2Cearitqs5/6P+9WiLHlBiiwiNTr5FmVhk3HVcxKSk8pzEhl85RHUT60Hn/82how28vjSLmdY2n6ApkhauUoNUJvsHdhmJhptMIKLAGTSc4Jl+qOg+y7TcL5dvNqbdXy5bn3m2LC8=&quot;            ],            &quot;x5t&quot;: &quot;bD7dNY4UHbG95tFBievhD1WXcEU&quot;,            &quot;x5t#S256&quot;: &quot;wN9C1fwn8V6MGs0J3ymKAFa7GF7Kah9OsnXmopkJV58&quot;        &#125;,        &#123;            &quot;kid&quot;: &quot;FP2Ie6xKpFRDYXpaJanHWL0GlMLMNDc9St38x6TevbM&quot;,            &quot;kty&quot;: &quot;RSA&quot;,            &quot;alg&quot;: &quot;RS256&quot;,            &quot;use&quot;: &quot;enc&quot;,            &quot;n&quot;: &quot;l8PegYQQYtLmXyQ4ItwlCbUsK7TmjBgi1BtqUhPyyhea4OIflIulkgPOE2Jj4-vHCVbdvFFZkmLwQpu2nUm_cG9m-R2L6h9WGmBG0oIg4c-mm3XjLEbv0j86wAhqIXrh7Xbuyk2zwZZsHWjYqONOVMkN71cpWgmTbsjBjfEgdKdlOX3yIHVlILyQopH9gIsokTHbSxuYaZFh2JWkCQ_TyLgvvUs0VYtkLPFhw5oLkn4SpI6e2vqNsZSgiYAN1UdfxhNUGFKijPY7cK76WxTR18N3baD9jzUpmuJL1dvIJlXR9XwqAVSpbf-Uzu6-ajT7JGGK5kLHLQf4-T9jXLaMwQ&quot;,            &quot;e&quot;: &quot;AQAB&quot;,            &quot;x5c&quot;: [                &quot;MIICmzCCAYMCBgF9mvWlSDANBgkqhkiG9w0BAQsFADARMQ8wDQYDVQQDDAZyZWFsbTEwHhcNMjExMjA4MTY1MDI5WhcNMzExMjA4MTY1MjA5WjARMQ8wDQYDVQQDDAZyZWFsbTEwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCXw96BhBBi0uZfJDgi3CUJtSwrtOaMGCLUG2pSE/LKF5rg4h+Ui6WSA84TYmPj68cJVt28UVmSYvBCm7adSb9wb2b5HYvqH1YaYEbSgiDhz6abdeMsRu/SPzrACGoheuHtdu7KTbPBlmwdaNio405UyQ3vVylaCZNuyMGN8SB0p2U5ffIgdWUgvJCikf2AiyiRMdtLG5hpkWHYlaQJD9PIuC+9SzRVi2Qs8WHDmguSfhKkjp7a+o2xlKCJgA3VR1/GE1QYUqKM9jtwrvpbFNHXw3dtoP2PNSma4kvV28gmVdH1fCoBVKlt/5TO7r5qNPskYYrmQsctB/j5P2NctozBAgMBAAEwDQYJKoZIhvcNAQELBQADggEBAD9FUakmwBYASe0W7CMX31B4SxkxW3kpSiN87LZg9KszAEF4nUSIAtmO21s9RDND5cQh3sxrS+ONVx36BU2xYBzoBeijAYFqjAr4ZpwBR6KPHAEQpalKog9zmEgR1Pki648zedi80gkFCxm6TXI4DY760ThvGzjkbfYFd2YSf+i1RWTq8NAqzUx0UTPNO3cCkRYOQ7Hrb5EpSXIgXjm3fG2/HyyB2Jvwb9yU6ffvg0FrUqiytChFxLeqhLoUEBNdiw0CroyeKaiYaNkZTwD9EwkUq6OXkqW5XHceM30oT4G2CXGNR9Al2oQi5kdYG6W+th0acZzeh+raDawrWGZpYic=&quot;            ],            &quot;x5t&quot;: &quot;V0xH-oSQYY8Xm_tro1n9bIZjcBU&quot;,            &quot;x5t#S256&quot;: &quot;VGwU3h3_Z5d2LdEsW1FRrmiCDJDm85pLnyReExQ9nY8&quot;        &#125;    ]&#125;</code></pre><ul><li>Pay attention to the <code>kid</code> (Key ID) property, which helps identify the exact public key for any token. The <code>kid</code> is contained in the header of a JWT token.</li></ul><h3 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h3><ul><li><code>state</code>, <code>session_state</code>: Used for CSRF protection.</li><li>Spring debug class: <code>org.springframework.security.oauth2.client.oidc.authentication.OidcAuthorizationCodeAuthenticationProvider</code>.</li><li>In <code>credential</code> mode: <code>client_id + client_secret</code> is equivalent to <code>username + password</code>. This is implicitly <code>Basic Auth</code>.</li></ul><h3 id="Why-does-the-OAuth-server-return-an-authorization-code-instead-of-an-access-token-in-the-first-step"><a href="#Why-does-the-OAuth-server-return-an-authorization-code-instead-of-an-access-token-in-the-first-step" class="headerlink" title="Why does the OAuth server return an authorization code instead of an access token in the first step?"></a>Why does the OAuth server return an authorization code instead of an access token in the first step?</h3><ul><li><a href="https://stackoverflow.com/questions/13387698/why-is-there-an-authorization-code-flow-in-oauth2-when-implicit-flow-works-s">Stack Overflow Discussion</a></li><li><a href="https://www.quora.com/Why-does-OAuth-server-return-a-authorization-code-instead-of-access-token-in-the-first-step">Quora Discussion</a></li><li><a href="https://blog.httpwatch.com/2011/03/01/6-things-you-should-know-about-fragment-urls/">HTTPWatch Blog</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication </tag>
            
            <tag> basic </tag>
            
            <tag> session </tag>
            
            <tag> token </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Install Guide - SNPM + Prometheus + Grafana</title>
      <link href="/2020/02/Install_Guide/Install_SNMP_Prometheus_Grafana/"/>
      <url>/2020/02/Install_Guide/Install_SNMP_Prometheus_Grafana/</url>
      
        <content type="html"><![CDATA[<h1 id="Monitor-traffic-network-vs-combo-SNMP-Prometheus-Grafana"><a href="#Monitor-traffic-network-vs-combo-SNMP-Prometheus-Grafana" class="headerlink" title="Monitor traffic network vs combo: SNMP + Prometheus + Grafana"></a>Monitor traffic network vs combo: SNMP + Prometheus + Grafana</h1><ul><li>Monitor traffic network<ul><li>SNMP: giao thức monitor network</li><li>Prometheus: collect metric data</li><li>Grafana: web ui, giao diện đồ họa, dashboard theo dõi</li></ul></li></ul><h1 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h1><h2 id="1-SNMP"><a href="#1-SNMP" class="headerlink" title="1. SNMP"></a>1. SNMP</h2><ul><li>install <code>apt snmp</code> tool</li></ul><pre><code class="bash">sudo apt-get updatesudo apt-get install snmp snmp-mibs-downloader</code></pre><ul><li>Lưu ý:<ul><li><code>snmp</code> mặc định, chỉ có thể monitor resource: ram, cpu.</li><li><code>snmp-mibs-downloader</code> hỗ trợ download “IF-MIB”, dùng để monitor interface, traffic network</li></ul></li><li>install <code>apt snmpd</code> (1 số OS, không cài sẽ không config, start được snmp)</li></ul><pre><code class="bash">sudo apt-get install snmpd</code></pre><ul><li>edit file config</li></ul><pre><code class="bash">sudo nano /etc/snmp/snmp.conf# comment on #mibs :</code></pre><pre><code class="bash"># 1sudo nano /etc/snmp/snmpd.conf# 2#  Listen for connections from the local system only#agentAddress  udp:127.0.0.1:161#  Listen for connections on all interfaces (both IPv4 *and* IPv6)agentAddress udp:161,udp6:[::1]:161# 3. replace configrocommunity public 172.26.6.172# để restrict IP có thể monitor# 4. restart snmpdsudo service snmpd restart# 5. checksnmpwalk -v2c -c public 172.26.6.172 </code></pre><h2 id="2-Prometheus"><a href="#2-Prometheus" class="headerlink" title="2. Prometheus"></a>2. Prometheus</h2><h3 id="2-1-Prometheus-export"><a href="#2-1-Prometheus-export" class="headerlink" title="2.1 Prometheus export"></a>2.1 Prometheus export</h3><ul><li>Là client thu nhập 1 loại metric riêng biệt</li><li>&#x3D;&gt; <code>snmp_exporter</code></li><li>Install <code>snmp_exporter</code><ul><li>Ref <a href="https://github.com/prometheus/snmp_exporter">link</a></li></ul></li></ul><pre><code class="bash">#1 wget https://github.com/prometheus/snmp_exporter/releases/download/v0.16.1/snmp_exporter-0.16.1.linux-amd64.tar.gz#2 tar -zxf snmp_exporter-0.16.1.linux-amd64.tar.gz# 3./snmp_exporter# 4. validate result via endpoint examplehttp://172.26.6.172:9116/snmp?module=if_mib&amp;target=172.26.6.172</code></pre><h3 id="2-2-Prometheus-server"><a href="#2-2-Prometheus-server" class="headerlink" title="2.2 Prometheus server"></a>2.2 Prometheus server</h3><ul><li>Install via docker</li></ul><pre><code class="bash">docker run -d \    -p 9190:9090 \    -v /home/ubuntu/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \    prom/prometheus</code></pre><ul><li>Trong đó file <code>prometheus.yml</code></li></ul><pre><code class="yml">global:  scrape_interval:     1sscrape_configs:  - job_name: &#39;snmp&#39;    static_configs:      - targets:        - 172.26.6.172  # SNMP device.    metrics_path: /snmp    params:      module: [if_mib]    relabel_configs:      - source_labels: [__address__]        target_label: __param_target      - source_labels: [__param_target]        target_label: instance      - target_label: __address__        replacement: 172.26.6.172:9116 </code></pre><p><a href="http://ls1.tungexplorer.me:9190/targets">http://ls1.tungexplorer.me:9190/targets</a><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/prometheus/snmp_export_prometheus.png" alt="http://ls1.tungexplorer.me:9190/targets"></p><ul><li>promQuery</li></ul><pre><code>rate(ifHCOutOctets[1m])</code></pre><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/prometheus/query_gettraffic.png" alt="query get traffic"></p><h2 id="3-Grafana"><a href="#3-Grafana" class="headerlink" title="3. Grafana"></a>3. Grafana</h2><ul><li>web ui dashboard, graphic monitor</li><li>install via docker</li></ul><pre><code class="bash">docker run -d -p 3000:3000 grafana/grafana## admin/admin</code></pre><ul><li>config<ul><li>Add prometheus endpoint</li><li>Create dashboard with query <code>rate(ifHCOutOctets[1m])</code> .</li></ul></li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/grafana/snmp_garafana.png" alt="Grafana dashboard"></p>]]></content>
      
      
      <categories>
          
          <category> install_guide </category>
          
      </categories>
      
      
        <tags>
            
            <tag> script </tag>
            
            <tag> install </tag>
            
            <tag> snmp </tag>
            
            <tag> prometheus </tag>
            
            <tag> grafana </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stories - Get Link Fshare</title>
      <link href="/2020/02/Stories/48h_inhouse_getLink_project/"/>
      <url>/2020/02/Stories/48h_inhouse_getLink_project/</url>
      
        <content type="html"><![CDATA[<h1 id="Nhat-ky-lam-he-thong-getLink-vip-Fshare-trong-48h"><a href="#Nhat-ky-lam-he-thong-getLink-vip-Fshare-trong-48h" class="headerlink" title="Nhật ký làm hệ thống getLink vip Fshare trong 48h"></a>Nhật ký làm hệ thống getLink vip Fshare trong 48h</h1><p>Nhật ký ngày tháng năm…<br>Vào một ngày đẹp trời, mình thấy các website cung cấp dịch vụ <code>getlink VIP fshare</code> đồng loạt thông báo lỗi, không thể<br>getlink VIP download <code>maxbandwith</code>. Và mình cũng thấy rất nhiều bài viết mới được tạo trong 1 group (chuyên chia sẻ về<br>IT), về việc sharing Fcode (fshare code) mà họ mới mua, và không dùng hết. Các <code>fcode</code> thì có hạn về số lượng, và thời<br>gian sử dụng.<br>Điều đó làm mình lóe lên suy nghĩ về việc tự làm 1 website getLink VIP. Và <code>Fshare</code> là dịch vụ mình sẽ thử nghiệm.</p><p>&#x2F;&#x2F; (Nếu có điều kiện, bạn hãy trả fee cho fshare để có thể sử dụng dịch vụ tốt hơn)</p><h2 id="1-Y-tuong-thiet-ke-ban-dau"><a href="#1-Y-tuong-thiet-ke-ban-dau" class="headerlink" title="1. Ý tưởng thiết kế ban đầu"></a>1. Ý tưởng thiết kế ban đầu</h2><p>Fshare là 1 hệ thống cung cấp dịch vụ lưu trữ file nổi tiếng. Và để download file với tốc độ max băng thông, thì bạn cần<br>có tài khoản VIP. Và để có tài khoản VIP thì bạn sẽ phải trả phí. Và theo <code>policy</code> của fshare thì 1 tài khoản VIP sẽ<br>không bị giới hạn lượt tải, và băng thông tải.<br>&#x3D;&gt; Mình sẽ làm 1 website mà client sẽ điền form chứa URL tới file cần download. Sau đó server backend sẽ sử dụng tài<br>khoản VIP của mình, request fshare để lấy link download, rồi trả về cho client.</p><ul><li><strong>Trở ngại 1…</strong></li></ul><p>Không thể làm như vậy được, vì fshare có cơ chế là chỉ địa chỉ IP nào request link download, thì chỉ địa chỉ IP đó mới<br>có thể download được. Trong url download mà fshare trả về, đã được <code>signed</code> (link download có truyền thêm các parameter<br>để validate), mình “đoán” một trong các param để signed, có <code>IP address</code> của client.<br>&#x3D;&gt; Nếu việc request link không diễn ra ở backend service, thì mình sẽ làm nó ở frontend.</p><ul><li><strong>Trở ngại 2…</strong></li></ul><p>Việc để client (frontend) tự động trực tiếp request link với fshare, làm mình lo lắng về việc bảo mật tài khoản VIP của<br>mình. Mình không muốn client có được bất cứ thông tin gì nhạy cảm về tài khoản VIP. Hơn nữa, mình cũng đã thử nghiệm,<br>khá phức tạp để <code>CORS</code> tới fshare (sử dụng <code>ajax</code> call chéo tới 1 domain khác domain đang chạy). Mình đã debug<br>việc <code>cors</code> fshare bằng cách f12 của web browser. Và mình thấy fshare vẫn chấp nhận <code>cors</code>, nhưng nó quá phức tạp, khó<br>control.<br>&#x3D;&gt; Với 2 lý do trên, mình quyết định bỏ cách này. Và quay lại với cách đầu tiên. Get link ở phía backend server. Tuy<br>nhiên, sau khi link download được tạo ra, trả cho client. Mình sẽ cung cấp luôn 1 đường proxy, có nhiệm vụ forward<br>traffic download từ client tới server backend của mình, sau đó mới tới fshare.</p><ul><li><strong>Trở ngại 3…</strong></li></ul><p>Cách này cũng không hoàn hảo, vì việc forward traffic sẽ rất dễ gây <code>bottleneck</code> (nghẽn cổ chai) tại server của mình.<br>Tuy nhiên server mà mình đang có, theo quảng cáo là bandwith lên tới 10Gbps. Mình không chắc chắn về performance khi số<br>lượng client sử dụng lên nhiều, nhưng đó sẽ là 1 bài toán ở lĩnh vực khác. Nên mình vẫn quyết định triển theo hướng này.</p><h2 id="2-Lua-chon-tech-stack-vong-1"><a href="#2-Lua-chon-tech-stack-vong-1" class="headerlink" title="2. Lựa chọn tech stack vòng 1"></a>2. Lựa chọn tech stack vòng 1</h2><ul><li>Java vs Spring Framework (Springboot, Spring webservice…): java là ngôn ngữ mình dùng nhiều nhất, và Spring là 1<br>framework nổi tiếng. Nên mình dùng nó làm phía backend.</li><li>Frontend: html + css + jquery + ajax. Mình sẽ dùng ajax để call api tới server backend.</li></ul><h2 id="3-Di-tim-cach-ket-noi-voi-fshare"><a href="#3-Di-tim-cach-ket-noi-voi-fshare" class="headerlink" title="3. Đi tìm cách kết nối với fshare"></a>3. Đi tìm cách kết nối với fshare</h2><p>Mình đã google, nhưng không thấy fshare có 1 document nào nói về việc <code>public</code> API cho các developer.<br>Mình lại bắt đầu suy nghĩ lại về việc debug f12 web browser <code>fshare.vn</code> để tìm format cho httpClient.<br>Thật may mắn, sau đó mình search trên <code>github</code> thì lại thấy có repo về getLink fshare. Đó là repo<br>này: <a href="https://github.com/tudoanh/get_fshare">https://github.com/tudoanh/get_fshare</a>   (dùng python)<br>Để chắc chắn API trong repo đúng, mình dùng phần mềm <code>PostMan</code> để test lại. Và kết quả các API vẫn tốt.<br>Cơ bản có 2 API:</p><ol><li>Sử dụng <code>user_email</code> + <code>password</code> trên fshare, để lấy <code>session_id</code> và <code>token</code><ul><li>Request example:</li></ul></li></ol><pre><code class="bash">    curl --location --request POST &#39;https://api.fshare.vn/api/user/login&#39; \    --header &#39;Content-Type: application/json&#39; \    --data-raw &#39;&#123;                &quot;user_email&quot;: &quot;usename@gmail.com&quot;,                &quot;password&quot;: &quot;passWord&quot;,                &quot;app_key&quot;: &quot;L2S7R6ZMagggC5wWkQhX2+aDi467PPuftWUMRFSn&quot;            &#125;    &#39;</code></pre><ul><li>Response example:</li></ul><pre><code class="bash">    &#123;    &quot;code&quot;: 200,    &quot;msg&quot;: &quot;Login successfully!&quot;,    &quot;token&quot;: &quot;483f5bb8ab3812070f95b2f52d0ff5645a4f4&quot;,    &quot;session_id&quot;: &quot;f35jj7d0q5v91dt6b4r3vns&quot;    &#125;</code></pre><ol start="2"><li>Sử dụng <code>token</code> để request link download<ul><li>Request example:</li></ul></li></ol><pre><code class="bash">    curl --location --request POST &#39;https://api.fshare.vn/api/session/download&#39; \    --header &#39;Content-Type: application/json&#39; \    --data-raw &#39;&#123;                &quot;token&quot;: &quot;483f5bb8ab3812070f95b2f52d0ff5645a4f4&quot;,                &quot;url&quot;: &quot;https://www.fshare.vn/file/VG8998AQNPB&quot;    &#125;&#39;</code></pre><pre><code>- Response example:</code></pre><pre><code class="bash">    &#123;        &quot;location&quot;: &quot;http://download022.fshare.vn/dl/g1igLGM7IScOwdkwo3iwyPmJPDJk1roRViiVDbNszLZwk4k2xb-6TogmIK9Rjxq4y+Ggl2V0Z/ipz0g78hhb.wmv&quot;    &#125;</code></pre><p>&#x2F;&#x2F; Các example trên mình export từ Postman. Mình đã dùng <code>Postman</code> thì thấy luôn ok. Nhưng không hiểu sao khi<br>dùng <code>curl</code> lại lỗi. Khá khó hiểu. Hơn nữa, lại chỉ thấy sử dụng <code>token</code> chứ không thấy ở đâu sử dụng <code>session_id</code>. Về<br>sau khi mình convert sang code Java, thì request thứ 2 luôn báo lỗi là “chưa đăng nhập”. Sau đó mình phát hiện ra, trong<br>request thứ 2, mình phải truyền thêm <code>header</code> chứa <code>Cookie</code>, trong đó giá trị <code>Cookie</code> có <code>session_id</code> thì mới pass.</p><h2 id="4-Lua-chon-tech-stack-vong-2"><a href="#4-Lua-chon-tech-stack-vong-2" class="headerlink" title="4. Lựa chọn tech stack vòng 2"></a>4. Lựa chọn tech stack vòng 2</h2><h3 id="4-1-Lua-chon"><a href="#4-1-Lua-chon" class="headerlink" title="4.1 Lựa chọn"></a>4.1 Lựa chọn</h3><p>Việc dựng project <code>spring boot</code> có backend và frontend trong 1 repo khá nhanh.</p><ul><li><p>Combo Spring framework</p><ul><li>Để call tới API của fshare, cần 1 httpClient. Ban đầu mình định sử dụng <code>FeignClient</code>, code của nó khá ngắn. Tuy<br>nhiên mình đã gặp khó khăn trong việc debug lỗi với fshare. Nên mình quyết định quay lại sử dụng <code>RestTemplate</code><br>của Spring.</li><li>Ngay khi code <code>application</code> được run, mình cần phải lấy giá trị của <code>session_id</code>, <code>token</code> và lưu vào biến <code>static</code><br>. Mình sử dụng <code>CommandLineRunner</code>.</li><li>Mình phát hiện ra <code>session_id</code> + <code>token</code> chỉ sử dụng được trong 1 khoảng thời gian nhất định, có lẽ đây là chính<br>sách của fshare. Vì vậy mình sử dụng thêm <code>Spring Scheduled</code> để hẹn giờ call API <code>rotation/ refresh</code> lại giá trị<br>token + session_id mới.</li><li>Việc client gửi form request lấy link download, và việc server request fshare, nên được chạy bất đồng bộ. Và vì<br>bất đồng bộ, nên mình sử dụng <code>Spring Websocket</code> (spring-boot-starter-websocket) để làm 1 kênh gửi kết quả về cho<br>client, sau khi Server run task lấy link download xong. Việc bất đồng bộ này cũng góp phần làm cho website của<br>mình trở nên thân thiện hơn.</li><li>Mình sử dụng <code>ThreadPoolTaskExecutor/TaskExecutor</code> của Spring, để tạo task cho mỗi request get link. (chạy multi<br>thread)</li></ul></li><li><p>Phía client, mình sử dụng <code>sockjs-client</code> + <code>stomp-websocket</code> để subscribe kênh socket. (subcribe để khi Server chạy<br>task getlink xong, sẽ show kết quả ra phía frontend, mà không phải refresh page). Với mỗi phiên request client, mình<br>sẽ sinh ra 1 topic socket mới. Mục đích để ID. Mình khá lo lắng về việc này sẽ dẫn tới performance của hệ thống giảm<br>xuống nhiều.</p></li></ul><h3 id="4-2-Quick-code"><a href="#4-2-Quick-code" class="headerlink" title="4.2 Quick code"></a>4.2 Quick code</h3><ul><li>Spring RestTemplate</li></ul><pre><code class="java"> public void setToken() &#123;        LoginRequest loginRequest = new LoginRequest();        loginRequest.setUser_email(userMail);        loginRequest.setPassword(password);        loginRequest.setApp_key(appKey);        RestTemplate restTemplate = new RestTemplate();        LoginResponse loginResponse = restTemplate.postForObject(Const.FSHARE_ENDPOINT_LOGIN, loginRequest, LoginResponse.class);        assert loginResponse != null;        Const.FSHARE_SESSION_ID = loginResponse.getSession_id();        Const.FSHARE_TOKEN = loginResponse.getToken();    &#125;</code></pre><ul><li>CommandLineRunner</li></ul><pre><code class="java">@Componentpublic class GetToken implements CommandLineRunner &#123;    @Autowired    private FshareService fshareService;    @Override    public void run(String... strings) &#123;        fshareService.setToken();    &#125;&#125;</code></pre><ul><li>Spring Scheduled</li></ul><pre><code class="java">@Componentpublic class RefreshToken &#123;    @Autowired    private FshareService fshareService;    @Scheduled(fixedRate = 1000 * 60 * 60 * 2)    public void run() &#123;        fshareService.setToken();    &#125;&#125;</code></pre><ul><li>Spring Websocket</li></ul><pre><code class="java">@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketConfig extends AbstractWebSocketMessageBrokerConfigurer &#123;    @Override    public void registerStompEndpoints(StompEndpointRegistry stompEndpointRegistry) &#123;        stompEndpointRegistry.addEndpoint(&quot;/websocket-receive-link&quot;)                .withSockJS();    &#125;    @Override    public void configureMessageBroker(MessageBrokerRegistry registry) &#123;        registry.enableSimpleBroker(&quot;/topic&quot;);        registry.setApplicationDestinationPrefixes(&quot;/app&quot;);    &#125;&#125;</code></pre><ul><li>ThreadPoolTaskExecutor</li></ul><pre><code class="java">    @Bean    @Primary    TaskExecutor taskExecutor() &#123;        ThreadPoolTaskExecutor t = new ThreadPoolTaskExecutor();        t.setCorePoolSize(10);        t.setMaxPoolSize(100);        t.setQueueCapacity(500);        return t;    &#125;        //    @Autowired    private TaskExecutor task;    //    task.execute(new TaskGetLink(requestLink, template, fshareService));</code></pre><ul><li>sockjs-client + stomp-websocket</li></ul><pre><code class="js">function initSocket() &#123;    var socket = new SockJS(&#39;/websocket-receive-link&#39;);    stompClient = Stomp.over(socket);    stompClient.connect(&#123;&quot;X-Token&quot;: &quot;tokenvalue&quot;&#125;, onConnected, onError);    function onConnected() &#123;        stompClient.subscribe(&quot;/topic/&quot; + requestId, onMessageReceived);    &#125;    function onMessageReceived(payload) &#123;        // todo some thing    &#125;    function onError(error) &#123;         // todo some thing    &#125;&#125;</code></pre><h2 id="5-Tro-ngai-4"><a href="#5-Tro-ngai-4" class="headerlink" title="5. Trở ngại 4"></a>5. Trở ngại 4</h2><p>Việc code application về cơ bản đã hoàn thiện kha khá. Mình bắt đầu tìm solution cho việc <code>proxy traffic</code> mà mình đã<br>nhắc bên trên. Traffic download sẽ từ client rồi forward qua server của mình, sau đó mới tới fshare. Mình đã google rất<br>nhiều về cách này, nhưng khá là rối với skill hiện tại.<br>Trong lúc trầm tư suy ngẫm, mình sực nhớ ra <code>NGINX</code>, 1 thằng mà mình từng dùng, có thể làm được điều này, 1 điều mà mình<br>lại quên béng mất.<br>&#x3D;&gt; Ý tưởng:<br>Nôm na thì linkdownload của fshare trả về đại loại format sau</p><pre><code>http://download022.fshare.vn/dl/g1igLGM7IScOwdkwo3iwyPmJPDJk1roRViimGIL8t7VDbNxb-6TogmIK9Rjxq4y+Ggl2V0Z/ipz0g78hhb.wmv</code></pre><p>Trong đó <code>download022.fshare.vn</code> chính là server fshare. Bây giờ mình sẽ biến nó thành <code>download022.tungexplorer.me</code> -<br>là địa chỉ server của mình. Và link downoad cho client là:</p><pre><code>http://download022.tungexplorer.me/dl/g1igLGM7IScOwdkwo3iwyPmJPDJk1roRViimGIL8t7VDbNxb-6TogmIK9Rjxq4y+Ggl2V0Z/ipz0g78hhb.wmv</code></pre><ul><li>Việc replace chuỗi String này trong java chỉ 1 nốt nhạc.</li><li>Cấu hình <code>Nginx</code> tham khảo:</li></ul><pre><code class="bash">server &#123;  listen 80;  server_name download03333.tungexplorer.me;  access_log off;  location / &#123;    proxy_pass http://download022.fshare.vn;    proxy_read_timeout 300;    proxy_connect_timeout 300;    proxy_redirect     off;    proxy_set_header   X-Forwarded-Proto $scheme;    proxy_set_header   Host              $http_host;    proxy_set_header   X-Real-IP         $remote_addr;  &#125;&#125;</code></pre><h2 id="6-Tro-ngai-5"><a href="#6-Tro-ngai-5" class="headerlink" title="6. Trở ngại 5"></a>6. Trở ngại 5</h2><p>Mình phát hiện ra <code>endpoint</code> mà fshare cung cấp, không chỉ có <code>download022.fshare.vn</code>, mà nó loadbalancer qua nhiều<br>endpoint khác nữa. Ví dụ 023, 024, 011… Mình không thể config 1 cách bị động cho từng endpoint như vậy được.<br>&#x3D;&gt; Config với <code>regex</code> để dynamic việc route proxy. Việc này khiến mình mất khá nhiều thời gian, vì mình không quá thành<br>thục về việc này. Nó làm mình mất 3-4 tiếng để thử nghiệm regex, và google debug.<br>Cuối cùng format hoạt động tốt:</p><pre><code class="bash">server &#123;  listen 80;  server_name   ~^(www\.)?[^.]+.tungexplorer.me$;  access_log off;  if ($host ~* ^(www\.)?([^.]+).tungexplorer.me$) &#123;    set $subdomain $2;  &#125;  resolver 8.8.8.8 valid=10s;  location / &#123;    proxy_pass http://$subdomain.fshare.vn;    proxy_read_timeout 300;    proxy_connect_timeout 300;    proxy_redirect     off;    proxy_set_header   X-Forwarded-Proto $scheme;    proxy_set_header   Host              $http_host;    proxy_set_header   X-Real-IP         $remote_addr;  &#125;&#125;</code></pre><h2 id="7-Lua-chon-tech-stack-vong-3"><a href="#7-Lua-chon-tech-stack-vong-3" class="headerlink" title="7. Lựa chọn tech stack vòng 3"></a>7. Lựa chọn tech stack vòng 3</h2><p>Về cơ bản hệ thống đã hoạt động gần như ý mình muốn. Tuy nhiên mình muốn làm 2 việc nữa:</p><ol><li>Control limit bandwith cho mỗi lượt download. Tuy server mình có đường truyền tới 10Gbps, tuy nhiên mình muốn mỗi<br>lượt download, chỉ có tốc độ giới hạn là XXX <code>Mbps</code>. &#x3D;&gt; NGINX Plus cung cấp tính năng này, tuy nhiên giá Nginx Plus<br>khá đắt. $25000 &#x2F; năm. &#x3D;&gt; tới thời điểm hiện tại mình vẫn bỏ ngỏ bài toán này.</li><li>Mình cần 1 hệ thống monitor webserver của mình, mình muốn theo dõi là hiện tại đang có bao nhiêu lượt download, tổng<br>traffic trên các cổng mạng đang là bao nhiêu, tài nguyên cpu, ram có đang full load không. Sau khi cân nhắc mình<br>quyết định sử dụng combo: <code>Prometheus + Grafana + Prometheus Exporter</code><ul><li>Prometheus server: lấy metric từ các <code>client/ device</code> về để theo dõi.</li><li>Prometheus Exporter : client của prometheus, thu thập metric trên “client” đang chạy, để gửi về<br>cho <code>prometheus server</code></li><li>Grafana: kết nối với prometheus, sau đó đồ thị hóa việc show metric cho admin, thông qua webUI.</li></ul></li></ol><p>Hình ảnh chụp <code>1 Dashboard monitor traffic của Grafana</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/grafana/snmp_garafana.png" alt="Grafana"></p><p>Note: việc sử dụng combo <code>Prometheus + Grafana + Prometheus Exporter</code> mới đầu mình đánh giá có vẻ khá thừa thãi, thấy<br>khá “phức tạp hóa” vấn đề. Tuy nhiên, hệ thống này mình sẽ còn phát triển thêm nữa. Nên mình vẫn quyết định lựa chọn<br>dùng chúng.</p><h2 id="8-Full-SourceCode"><a href="#8-Full-SourceCode" class="headerlink" title="8. Full SourceCode"></a>8. Full SourceCode</h2><ul><li>Mình public source application tại đây: <a href="https://github.com/tungtv202/getlink_fshare">https://github.com/tungtv202/getlink_fshare</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> stories </category>
          
      </categories>
      
      
        <tags>
            
            <tag> stories </tag>
            
            <tag> fshare </tag>
            
            <tag> get link </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker - Template File</title>
      <link href="/2020/02/Docker/Docker_TemplateFile/"/>
      <url>/2020/02/Docker/Docker_TemplateFile/</url>
      
        <content type="html"><![CDATA[<h1 id="For-Springboot"><a href="#For-Springboot" class="headerlink" title="For Springboot"></a>For <code>Springboot</code></h1><h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><ul><li>Template 1</li></ul><pre><code class="Dockerfile">FROM openjdk:8-jreRUN echo &quot;Europe/Berlin&quot; &gt; /etc/timezone &amp;&amp; dpkg-reconfigure -f noninteractive tzdataRUN mkdir /dataWORKDIR /dataADD myapp.jar /data/myapp.jarENV springprofiles=&quot;&quot; \    MAXRAMIFNOLIMIT=4096ENTRYPOINT MAXRAM=$(expr `cat /sys/fs/cgroup/memory/memory.limit_in_bytes` / 1024 / 1024) &amp;&amp; \           MAXRAM=$(($MAXRAM&gt;$MAXRAMIFNOLIMIT?$MAXRAMIFNOLIMIT:$MAXRAM))m &amp;&amp; \           echo &quot;MaxRam: $MAXRAM&quot; &amp;&amp; \           java -XX:MaxRAM=$MAXRAM -Djava.security.egd=file:/dev/./urandom -jar -Dspring.profiles.active=&quot;$springprofiles&quot; myapp.jar#when &quot;-XX:+UseCGroupMemoryLimitForHeap&quot; isn&#39;t experimental anymore, you can use the following#ENTRYPOINT java -XX:+UseCGroupMemoryLimitForHeap -Djava.security.egd=file:/dev/./urandom -jar -Dspring.profiles.active=&quot;$springprofiles&quot; myapp.jar# To prevent delays caused by the random number generator, use /dev/./urandom instead of /dev/random   EXPOSE 8080</code></pre><ul><li>Template 2<ul><li>Dockerfile</li></ul></li></ul><pre><code class="Dockerfile">FROM openjdk:8-jdk-alpineENV TZ=Asia/Ho_Chi_MinhENV JAVA_OPTS=&quot;-Xmx128M -Xms128M&quot;ADD run.sh run.shRUN sh -c &#39;chmod +x /run.sh&#39;ADD target/lib libADD target/applications.jar applications.jarCMD [&quot;/run.sh&quot;]</code></pre><ul><li>run.sh</li></ul><pre><code class="bash">#!/bin/shexport HOST_NAME=`hostname`case $SPRING_PROFILES_ACTIVE in    local)        exec java -Xmx128M -Djava.security.egd=file:/dev/./urandom -jar applications.jar    ;;    staging)        exec java -Xmx1G -Djava.security.egd=file:/dev/./urandom -jar applications.jar    ;;    live)        exec java -Xmx4G -Djava.security.egd=file:/dev/./urandom -jar applications.jar    ;;esac</code></pre><ul><li>Build .jar</li></ul><pre><code class="bash">docker build -t springio/gs-spring-boot-docker .</code></pre><h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><ul><li><a href="https://spring.io/guides/gs/spring-boot-docker/">https://spring.io/guides/gs/spring-boot-docker/</a></li><li>What exactly does <code>-Djava.security.egd=file:/dev/./urandom</code> do when containerizing a Spring Boot application?<ul><li>The purpose of that security property is to speed up tomcat startup. By default the library used to generate<br>random number in JVM on Unix systems relies on &#x2F;dev&#x2F;random. On docker containers there isn’t enough entropy to<br>support &#x2F;dev&#x2F;random. See Not enough entropy to support &#x2F;dev&#x2F;random in docker containers running in boot2docker.<br>The random number generator is used for session ID generation. Changing it to &#x2F;dev&#x2F;urandom will make the startup<br>process faster.<br>-&gt; <a href="https://stackoverflow.com/questions/58853372/what-exactly-does-djava-security-egd-file-dev-urandom-do-when-containerizi">Link</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> template </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Code Application expose Metric for Prometheus</title>
      <link href="/2020/02/Java/Java_Code_Prometheus_Metric/"/>
      <url>/2020/02/Java/Java_Code_Prometheus_Metric/</url>
      
        <content type="html"><![CDATA[<h1 id="Code-Application-expose-Metric-for-Prometheus"><a href="#Code-Application-expose-Metric-for-Prometheus" class="headerlink" title="Code Application expose Metric for Prometheus"></a>Code Application expose Metric for Prometheus</h1><h2 id="1-Solution"><a href="#1-Solution" class="headerlink" title="1. Solution"></a>1. Solution</h2><p>2 cách</p><ul><li>Dùng <code>micromiter</code> -&gt; <a href="https://github.com/tungtv202/micromiter-prometheus">SourceCode</a></li><li>Dùng <code>prometheus-client</code> -&gt; <a href="https://github.com/tungtv202/prometheus-client-sdk">SourceCode</a></li></ul><h2 id="2-Note"><a href="#2-Note" class="headerlink" title="2. Note"></a>2. Note</h2><h3 id="2-1-Prometheus-Metric-Type"><a href="#2-1-Prometheus-Metric-Type" class="headerlink" title="2.1 Prometheus Metric Type"></a>2.1 Prometheus Metric Type</h3><p>Có 4 type</p><ul><li>Counter<ul><li>When<ul><li>you want to record a value that only goes up</li><li>you want to be able to later query how fast the value is increasing (i.e. it’s rate)</li></ul></li><li>Use Case<ul><li>request count</li><li>tasks completed</li><li>error count</li></ul></li></ul></li><li>Gauges<ul><li>When<ul><li>you want to record a value that can go up or down</li><li>you don’t need to query its rate</li></ul></li><li>Use Case<ul><li>memory usage</li><li>queue size</li><li>number of requests in progress</li></ul></li></ul></li><li>Histograms<ul><li>When<ul><li>you want to take many measurements of a value, to later calculate averages or percentiles</li><li>you’re not bothered about the exact values, but are happy with an approximation</li><li>you know what the range of values will be up front, so can use the default bucket definitions or define your<br>own</li></ul></li><li>Use Case<ul><li>request duration</li><li>response size</li></ul></li></ul></li><li>Summaries<ul><li>When<ul><li>you want to take many measurements of a value, to later calculate averages or percentiles</li><li>you’re not bothered about the exact values, but are happy with an approximation</li><li>you don’t know what the range of values will be up front, so cannot use histograms</li></ul></li><li>Use Case<ul><li>request duration</li><li>response size</li></ul></li></ul></li></ul><h3 id="2-2-Note-for-code"><a href="#2-2-Note-for-code" class="headerlink" title="2.2 Note for code"></a>2.2 Note for code</h3><ul><li><p>Đổi port endpoint metric, độc lập với port webservice logic. Nếu không có khai báo này thì sẽ chạy chung port 8080</p><pre><code class="properties">server.port=8080management.server.port=8090</code></pre></li><li><p>All metrics are exposed at <code>/actuator/prometheus</code></p></li><li><p>Khai báo MetricFilter</p><ul><li>Cách 1: tạo bean <code>MeterFilter</code></li></ul><pre><code class="java">    @Bean    public MeterFilter meterFilter() </code></pre><ul><li>Cách 2: khai báo trong file <code>application.properties</code></li></ul><pre><code class="properties">management.metrics.enable.jvm=false</code></pre></li><li><p>Lưu ý việc cập nhật metric được chạy ở 1 process khác với endpoint metric. (tức endpoint metric sẽ trả về kết quả đã<br>được tính toán ở 1 process khác trước đó ).<br>Ex <code>BatchJobMetric</code> class file</p></li><li><p>Chú ý <code>MeterRegistry</code></p></li></ul><pre><code class="java">        @Override        public void trackTimerMetrics(String metricName, String... tags) &#123;            Timer timer = meterRegistry.timer(metricName, tags);            sampleStore.get().stop(timer);        &#125;        @Override        public void trackCounterMetrics(String metricName, double value, String... tags) &#123;            meterRegistry.counter(metricName, tags).increment(value);        &#125;</code></pre><ul><li>Với kiểu metric <code>Gauges</code>, cần chú ý tới việc khai báo ref giá trị metric. Code example</li></ul><pre><code class="java">public class MailLagMetricJob implements CommandLineRunner &#123;    private static int DELAY_QUERY_SECOND = 1;    private AtomicInteger totalMailIsSendingMetric = new AtomicInteger();    private AtomicInteger totalMailNotSentMetric = new AtomicInteger();    @Autowired    private MailOutboxDao _mailOutboxDao;    @Autowired    private MeterRegistry meterRegistry;    @Override    public void run(String... args) throws Exception &#123;        while (true) &#123;            totalMailIsSendingMetric.set(_mailOutboxDao.totalMailIsSending());            totalMailNotSentMetric.set(_mailOutboxDao.totalMailNotSent());            meterRegistry.gauge(&quot;mail.is-sending&quot;, totalMailIsSendingMetric);            meterRegistry.gauge(&quot;mail.is-not-sent&quot;, totalMailNotSentMetric);            Thread.sleep(DELAY_QUERY_SECOND * 1000);        &#125;    &#125;&#125;</code></pre><h2 id="3-Ref"><a href="#3-Ref" class="headerlink" title="3. Ref"></a>3. Ref</h2><ul><li><a href="https://www.youtube.com/watch?v=nJMRmhbY5hY">Youtube về metric type + demo dùng prometheus-client</a></li><li><a href="https://tomgregory.com/the-four-types-of-prometheus-metrics/">Blog về metric type + demo dùng prometheus-client &#x3D; </a></li><li><a href="https://blog.pvincent.io/2017/12/prometheus-blog-series-part-2-metric-types/">Hình ảnh về các prometheus-type, nhìn ảnh trực quan</a></li><li><a href="https://medium.com/@mejariamol/spring-boot-app-monitoring-micrometer-prometheus-registry-590723a9ae0a">@Medium code demo dùng micromiter</a></li><li><a href="https://micrometer.io/docs/registry/prometheus#_counters">Trang chủ micromiter về prometheus</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> java </tag>
            
            <tag> metric </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Note Total</title>
      <link href="/2020/02/Java/Java_Total/"/>
      <url>/2020/02/Java/Java_Total/</url>
      
        <content type="html"><![CDATA[<h2 id="Collection-Interface"><a href="#Collection-Interface" class="headerlink" title="Collection Interface"></a>Collection Interface</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/CollectionInterface.PNG" alt="CollectionInterface"></p><h2 id="SerialVersionUID"><a href="#SerialVersionUID" class="headerlink" title="SerialVersionUID"></a>SerialVersionUID</h2><ul><li><p>It will help define the order data of objects when serializing to bytes stream. We can only deserialize objects when<br>SerialVersionUID of a class is equal to <code>SerialVersionUID</code> of instance that storage</p></li><li><p>What happens if we don’t define <code>SerialVersionUID</code>?</p><ul><li>Mechanism of serializable will auto-creates the <code>SerialVersionUID</code> when runtime, that based on properties of this<br>class. Suppose we did not define it and storage object. Then we modify some properties (add&#x2F;remove…). We will<br>get the <code>InvalidClassException</code> when trying to deserialize the object.</li></ul></li></ul><h2 id="Double-Brace"><a href="#Double-Brace" class="headerlink" title="Double Brace"></a>Double Brace</h2><ul><li>initialization syntax <code>(&#123;&#123; ... &#125;&#125;) </code></li><li>potentially creating a memory leak<br><a href="https://stackoverflow.com/questions/1958636/what-is-double-brace-initialization-in-java">https://stackoverflow.com/questions/1958636/what-is-double-brace-initialization-in-java</a></li></ul><h2 id="KafkaListener-chi-dinh-vi-tri-offset-partition"><a href="#KafkaListener-chi-dinh-vi-tri-offset-partition" class="headerlink" title="KafkaListener - chỉ định vị trí offset + partition"></a>KafkaListener - chỉ định vị trí offset + partition</h2><pre><code class="java"> @KafkaListener(            topics = &quot;abc.ProductLogs111&quot;,            groupId = &quot;tmp-remove-whenever-001&quot;,            concurrency = &quot;1&quot;,            topicPartitions = @TopicPartition(topic = &quot;abc.ProductLogs&quot;,                    partitionOffsets = &#123;                            @PartitionOffset(partition = &quot;2&quot;, initialOffset = &quot;2049&quot;),                            @PartitionOffset(partition = &quot;0&quot;, initialOffset = &quot;2325&quot;),                            @PartitionOffset(partition = &quot;1&quot;, initialOffset = &quot;2049&quot;),                    &#125;)    )    public void listen(ConsumerRecord&lt;String, String&gt; record) throws InvocationTargetException, NoSuchMethodException, InstantiationException, IllegalAccessException, IOException &#123;        try &#123;            System.out.println(&quot;topic: &quot; + record.topic());            System.out.println(&quot;partition: &quot; + record.partition());            System.out.println(&quot;offset: &quot; + record.offset());            System.out.println(&quot;value: &quot; +record.value());            System.out.println(&quot;timeStamp: &quot; +record.timestamp());        &#125; catch (Exception e) &#123;            //logger.error(&quot;[Topic] &quot; + record.topic() + &quot; [Offset] &quot; + record.offset() + &quot; [Partition] &quot; + record.partition() + &quot; [Exception] &quot;, e);            logger.error(&quot;Kafka consumer failed: &quot;, e);            Sentry.capture(e);            throw e;        &#125;    &#125;</code></pre><ul><li>WARNING: <code>initialOffset</code> is the absolute offset that exits in Kafka. We will get an exception when try to<br>set <code>initialOffset</code> as a random number, that smaller some offset, that we want</li></ul><h2 id="Lazy"><a href="#Lazy" class="headerlink" title="@Lazy"></a>@Lazy</h2><p>&#x2F;&#x2F; TODO</p><h2 id="HashMap"><a href="#HashMap" class="headerlink" title="HashMap"></a>HashMap</h2><p>is not limited. That’s the maximum number of buckets. Each bucket uses a form of linked list which has no limitation<br>except memory. So in theory a HashMap can hold an unlimited number of elements. In practice, you won’t even get to 2^30<br>because you will have run out of memory long before that.</p><h2 id="MappedSuperclass"><a href="#MappedSuperclass" class="headerlink" title="MappedSuperclass"></a>MappedSuperclass</h2><p>Can not using both <code>@MappedSuperclass</code> and <code>@Entity</code> in same time.</p><h2 id="CommandLineRunner"><a href="#CommandLineRunner" class="headerlink" title="CommandLineRunner"></a>CommandLineRunner</h2><pre><code class="java">@Component@Order(Ordered.HIGHEST_PRECEDENCE)public class Test implements CommandLineRunner &#123;    private static Logger logger = LoggerFactory.getLogger(Test.class);    @Autowired    private TestClient testClient;    @Override    public void run(String... args) &#123;        var result = testClient.getById(1);    &#125;&#125;</code></pre><h2 id="NumberUtils"><a href="#NumberUtils" class="headerlink" title="NumberUtils"></a>NumberUtils</h2><pre><code class="java">public class NumberUtils &#123;    public static boolean isNotBlank(final Number number) &#123;        return !(number == null ||                (                        number instanceof Integer ? number.intValue() == 0 :                                number instanceof Long ? number.longValue() == 0 :                                        number instanceof Double ? number.doubleValue() == 0 :                                                number instanceof Short ? number.shortValue() == 0 :                                                        number.floatValue() == 0                ));    &#125;    public static boolean isBlank(final Number number) &#123;        return !isNotBlank(number);    &#125;    public static boolean isEqual(int v1, Integer v2) &#123;        if (v2 == null) return false;        return v1 == v2;    &#125;&#125;</code></pre><h2 id="CompressHelper"><a href="#CompressHelper" class="headerlink" title="CompressHelper"></a>CompressHelper</h2><pre><code class="java">import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.InputStream;import java.util.zip.GZIPInputStream;import java.util.zip.GZIPOutputStream;public class CompressHelper &#123;    public static byte[] gzipCompress(byte[] input) &#123;        try &#123;            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();            InputStream inputStream = new ByteArrayInputStream(input);            GZIPOutputStream gzipOS = new GZIPOutputStream(outputStream);            byte[] buffer = new byte[1024];            int len;            while ((len = inputStream.read(buffer)) != -1) &#123;                gzipOS.write(buffer, 0, len);            &#125;            gzipOS.close();            inputStream.close();            outputStream.close();            return outputStream.toByteArray();        &#125; catch (Exception ex) &#123;            throw new RuntimeException(ex);        &#125;    &#125;    public static byte[] gzipDecompress(byte[] input) &#123;        try &#123;            GZIPInputStream gis = new GZIPInputStream(new ByteArrayInputStream(input));            ByteArrayOutputStream fos = new ByteArrayOutputStream();            byte[] buffer = new byte[1024];            int len;            while ((len = gis.read(buffer)) != -1) &#123;                fos.write(buffer, 0, len);            &#125;            //close resources            fos.close();            gis.close();            return fos.toByteArray();        &#125; catch (Exception e) &#123;            throw new RuntimeException(e);        &#125;    &#125;&#125;</code></pre><h2 id="Spring-valid-enum"><a href="#Spring-valid-enum" class="headerlink" title="Spring valid enum"></a>Spring valid enum</h2><pre><code class="java">@Target(&#123;ElementType.METHOD, ElementType.FIELD, ElementType.ANNOTATION_TYPE, ElementType.CONSTRUCTOR, ElementType.PARAMETER, ElementType.TYPE_USE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Constraint(validatedBy = DoTypeValidator.class)public @interface DoTypeValid &#123;    DoSomethingType[] anyOf();    String message() default &quot;must be any of &#123;anyOf&#125;&quot;;    Class&lt;?&gt;[] groups() default &#123;&#125;;    Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125;</code></pre><pre><code class="java">public class DoTypeValidator implements ConstraintValidator&lt;DoTypeValid, DoSomethingType&gt; &#123;    private DoSomethingType[] doSomethingTypes;    @Override    public void initialize(DoTypeValid constraintAnnotation) &#123;        this.doSomethingTypes = constraintAnnotation.anyOf();    &#125;    @Override    public boolean isValid(DoSomethingType doSomethingType, ConstraintValidatorContext constraintValidatorContext) &#123;        return doSomethingType == null || Arrays.asList(doSomethingTypes).contains(doSomethingType);    &#125;&#125;</code></pre><p>Using:</p><pre><code class="java">    @DoTypeValid(anyOf = &#123;DoSomethingType.re_sync&#125;)    @NotNull    private DoSomethingType doType;</code></pre><h2 id="MapConverter-for-hibernate-java-hashmap-db-string"><a href="#MapConverter-for-hibernate-java-hashmap-db-string" class="headerlink" title="MapConverter - for hibernate, java hashmap, db string"></a>MapConverter - for hibernate, java hashmap, db string</h2><pre><code class="java">@Converterpublic class MapConverter implements AttributeConverter&lt;Map&lt;String, String&gt;, String&gt; &#123;    @Override    public String convertToDatabaseColumn(Map&lt;String, String&gt; attribute) &#123;        return Util.convertMapToString(attribute);    &#125;    @Override    public Map&lt;String, String&gt; convertToEntityAttribute(String dbData) &#123;        return Util.convertStringToMap(dbData);    &#125;&#125;</code></pre><p>Using in Entity class</p><pre><code class="java">    @Convert(converter = MapConverter.class)    private Map&lt;String, String&gt; metaData;</code></pre><h2 id="Valid-String-in-List"><a href="#Valid-String-in-List" class="headerlink" title="Valid - String in List"></a>Valid - String in List</h2><pre><code class="java">@Documented@Constraint(validatedBy = StringInListValidator.class)@Target(&#123;ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Order(value = Ordered.LOWEST_PRECEDENCE)public @interface StringInList &#123;    String[] array() default &#123;&#125;;    boolean allowBlank() default false;    String message() default &quot;invalid&quot;;    Class&lt;?&gt;[] groups() default &#123;&#125;;    Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125;</code></pre><pre><code class="java">public class StringInListValidator implements        ConstraintValidator&lt;StringInList, String&gt; &#123;    private StringInList stringInList;    @Override    public void initialize(StringInList stringInList) &#123;        this.stringInList = stringInList;    &#125;    @Override    public boolean isValid(String value, ConstraintValidatorContext context) &#123;        boolean isValid = false;        if (stringInList.allowBlank()) &#123;            if (StringUtils.isEmpty(value)) &#123;                isValid = true;            &#125;        &#125;        if (!isValid) &#123;            isValid = ArrayUtils.contains(stringInList.array(), value);        &#125;        if (!isValid) &#123;            context.disableDefaultConstraintViolation();            String message = String.format(&quot;is not in %s&quot;,                    Arrays.asList(stringInList.array()));            context.buildConstraintViolationWithTemplate(message)                    .addConstraintViolation();        &#125;        return isValid;    &#125;&#125;</code></pre><p>Using</p><pre><code class="java">    @StringInList(array = &#123;&quot;product&quot;, &quot;order&quot;&#125;, allowBlank = true)    private String errorType;</code></pre><h2 id="Optional-stream"><a href="#Optional-stream" class="headerlink" title="Optional.stream()"></a>Optional.stream()</h2><ol><li>Bad</li></ol><pre><code class="java">public BigDecimal getOrderPrice(Long orderId) &#123;    List&lt;OrderLine&gt; lines = orderRepository.findByOrderId(orderId);    BigDecimal price = BigDecimal.ZERO;           for (OrderLine line : lines) &#123;        price = price.add(line.getPrice());       &#125;    return price;&#125;</code></pre><ol start="2"><li>Bad</li></ol><pre><code class="java">public BigDecimal getOrderPrice(Long orderId) &#123;    List&lt;OrderLine&gt; lines = orderRepository.findByOrderId(orderId);    return lines.stream()                .map(OrderLine::getPrice)                .reduce(BigDecimal.ZERO, BigDecimal::add);&#125;</code></pre><p>and …</p><pre><code class="java">public BigDecimal getOrderPrice(Long orderId) &#123;    if (orderId == null) &#123;        throw new IllegalArgumentException(&quot;Order ID cannot be null&quot;);    &#125;    List&lt;OrderLine&gt; lines = orderRepository.findByOrderId(orderId);    return lines.stream()                .map(OrderLine::getPrice)                .reduce(BigDecimal.ZERO, BigDecimal::add);&#125;</code></pre><ol start="3"><li>bad</li></ol><pre><code class="java">public BigDecimal getOrderPrice(Long orderId) &#123;    return Optional.ofNullable(orderId)                                        .map(orderRepository::findByOrderId)                               .flatMap(lines -&gt; &#123;                                                    BigDecimal sum = lines.stream()                        .map(OrderLine::getPrice)                        .reduce(BigDecimal.ZERO, BigDecimal::add);                return Optional.of(sum);                                       &#125;).orElse(BigDecimal.ZERO);                            &#125;</code></pre><ol start="4"><li>perfect</li></ol><pre><code class="java">public BigDecimal getOrderPrice(Long orderId) &#123;    return Optional.ofNullable(orderId)            .stream()            .map(orderRepository::findByOrderId)            .flatMap(Collection::stream)            .map(OrderLine::getPrice)            .reduce(BigDecimal.ZERO, BigDecimal::add);&#125;</code></pre><h2 id="check-isPrivateIP"><a href="#check-isPrivateIP" class="headerlink" title="check isPrivateIP"></a>check isPrivateIP</h2><pre><code class="java">public static boolean isPrivateIP(HttpServletRequest request) &#123;        return isPrivateIP(getRemoteIP(request));    &#125;    public static boolean isPrivateIP(String ip) &#123;        InetAddress address;        try &#123;            address = InetAddress.getByName(ip);        &#125; catch (UnknownHostException exception) &#123;            return false;        &#125;        return address.isSiteLocalAddress() || address.isAnyLocalAddress() || address.isLinkLocalAddress()                || address.isLoopbackAddress() || address.isMulticastAddress();    &#125;    public static String getRemoteIP(HttpServletRequest request) &#123;        String ip = request.getHeader(&quot;X-Forwarded-For&quot;);        if (null != ip &amp;&amp; !&quot;&quot;.equals(ip.trim()) &amp;&amp; !&quot;unknown&quot;.equalsIgnoreCase(ip)) &#123;            // get first ip from proxy ip            int index = ip.indexOf(&#39;,&#39;);            if (index != -1) &#123;                return ip.substring(0, index);            &#125; else &#123;                return ip;            &#125;        &#125;        ip = request.getHeader(&quot;X-Real-IP&quot;);        if (null != ip &amp;&amp; !&quot;&quot;.equals(ip.trim()) &amp;&amp; !&quot;unknown&quot;.equalsIgnoreCase(ip)) &#123;            return ip;        &#125;        return request.getRemoteAddr();    &#125;</code></pre><h2 id="Pro-tip"><a href="#Pro-tip" class="headerlink" title="Pro tip"></a>Pro tip</h2><ul><li>The statement return null; makes the lambda into a <code>Callable</code> instead of a <code>Runnable</code>, so that you don’t have to catch<br>checked exceptions</li></ul><pre><code class="java">exec.submit(() -&gt; &#123;    process(something);    return null;&#125;);</code></pre><h2 id="Load-properties-from-file"><a href="#Load-properties-from-file" class="headerlink" title="Load properties from file"></a>Load properties from file</h2><pre><code class="java">import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;public class ExtraProperties &#123;    public static final String OVERRIDE_PROPERTY = &quot;extra.props&quot;;    public static final String DEFAULT_PATH = &quot;conf/jvm.properties&quot;;    public static void initialize() &#123;        String propsPath = System.getProperty(OVERRIDE_PROPERTY, DEFAULT_PATH);        try (FileInputStream in = new FileInputStream(propsPath)) &#123;            System.getProperties().load(in);        &#125; catch (FileNotFoundException e) &#123;            if (!DEFAULT_PATH.equals(propsPath)) &#123;                JamesServerMain.LOGGER.warn(&quot;Could not find extra system properties file &#123;&#125;&quot;, propsPath);            &#125;        &#125; catch (IOException e) &#123;            JamesServerMain.LOGGER.warn(                    &quot;Failed to load extra system properties from file &#123;&#125; : &#123;&#125;&quot;, propsPath, e.getMessage());        &#125;    &#125;&#125;</code></pre><pre><code class="java"> public static void main(String[] args) throws Exception &#123;        ExtraProperties.initialize();    &#125;</code></pre><ul><li>Command: <code>java -jar -Dmy.property=/home/tungtv/workplace/jvm123.properties</code></li><li>Sample file <code>jvm123.properties</code></li></ul><pre><code># my.property=whateverLOAD_ABC=true</code></pre><h2 id="String-equals-Vs-contentEquals"><a href="#String-equals-Vs-contentEquals" class="headerlink" title="String equals() Vs contentEquals()"></a>String equals() Vs contentEquals()</h2><pre><code class="java">String actualString = &quot;baeldung&quot;;CharSequence identicalStringBufferInstance = new StringBuffer(&quot;baeldung&quot;);assertFalse(actualString.equals(identicalStringBufferInstance));assertTrue(actualString.contentEquals(identicalStringBufferInstance));</code></pre><h2 id="Concurrent-thread-will-not-care-to-static-method"><a href="#Concurrent-thread-will-not-care-to-static-method" class="headerlink" title="Concurrent thread will not care to static method"></a>Concurrent thread will not care to <code>static</code> method</h2><pre><code class="java">public class ThreadStaticMethod &#123;    @SneakyThrows    public static void staticMethod() &#123;        System.out.println(Thread.currentThread().getName() + &quot; - &quot; + new Date());        TimeUnit.SECONDS.sleep(5);    &#125;    public static void main(String[] args) &#123;        for (int i = 1; i &lt; 10; i++) &#123;            new Thread(ThreadStaticMethod::staticMethod).start();        &#125;    &#125;&#125;</code></pre><p>output:</p><pre><code>Thread-2 - Tue Jan 11 21:52:51 ICT 2022Thread-0 - Tue Jan 11 21:52:51 ICT 2022Thread-3 - Tue Jan 11 21:52:51 ICT 2022Thread-1 - Tue Jan 11 21:52:51 ICT 2022</code></pre><h2 id="AutoCloseable-try-with-resource"><a href="#AutoCloseable-try-with-resource" class="headerlink" title="AutoCloseable  - try-with-resource"></a>AutoCloseable  - try-with-resource</h2><pre><code class="java">@Slf4jpublic class AutoCloseableDemo &#123;    @AllArgsConstructor    static class CustomCloseable implements AutoCloseable &#123;        @Getter        @Setter        private String value;        @Override        public void close() throws Exception &#123;            System.out.println(&quot;Closed : &quot; + value);;        &#125;    &#125;    public static void main(String[] args) throws Exception &#123;        try (CustomCloseable test = new CustomCloseable(&quot;Tung&quot;)) &#123;            System.out.println(test.getValue());        &#125;    &#125;&#125;</code></pre><pre><code>TungClosed : Tung</code></pre><h2 id="Why-we-need-Thread-currentThread-interrupt"><a href="#Why-we-need-Thread-currentThread-interrupt" class="headerlink" title="Why we need Thread.currentThread().interrupt();"></a>Why we need <code>Thread.currentThread().interrupt();</code></h2><pre><code class="java">            try &#123;               // some thing            &#125; catch (InterruptedException e) &#123;                Thread.currentThread().interrupt();            &#125;</code></pre><ul><li>it helps parent can handler InterruptedException. (In this exception has a flag, when we try catch it, that flag has been update, so the <code>Thread.currentThread().interrupt();</code> help recovery the state of flag)</li></ul><h2 id="Dequeue-vs-Stack"><a href="#Dequeue-vs-Stack" class="headerlink" title="Dequeue vs Stack"></a>Dequeue vs Stack</h2><ul><li>Stack is threadSafe, DeQueue is not</li></ul><h2 id="PhantomReference"><a href="#PhantomReference" class="headerlink" title="PhantomReference"></a>PhantomReference</h2><p><a href="https://openplanning.net/13697/java-phantomreference">https://openplanning.net/13697/java-phantomreference</a><br>Về cơ bản PhantomReference cung cấp cho bạn khả năng xác định chính xác khi nào đối tượng innerObject của nó bị xoá khỏi bộ nhớ. Phương thức phantomRef.isEnqueued() trả về true nghĩa là đối tượng innerObject đã bị xoá bỏ khỏi bộ nhớ. Khi đối tượng innerObject bị xoá khỏi bộ nhớ thì đối tượng phantomRef sẽ được đặt vào hàng đợi (queue).</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> java </tag>
            
            <tag> KafkaListener </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS - IAM</title>
      <link href="/2020/01/AWS/AWS_IAM/"/>
      <url>/2020/01/AWS/AWS_IAM/</url>
      
        <content type="html"><![CDATA[<ul><li>add user<br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Account.png" alt="AddUser"><br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Group1.png" alt="Group1"><br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Group2.png" alt="Group2"><br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Group3.png" alt="Group3"><br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Policy1.png" alt="Policy1"><br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Policy2.png" alt="Policy2"><br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Policy3.png" alt="Policy3"><br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Policy4.png" alt="Policy4"><br><img src="https://tungexplorer.s3-ap-southeast-1.amazonaws.com/aws/iam/Policy5.png" alt="Policy5"></li></ul>]]></content>
      
      
      <categories>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> iam </tag>
            
            <tag> policy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS - STS</title>
      <link href="/2020/01/AWS/AWS_LAB_STS/"/>
      <url>/2020/01/AWS/AWS_LAB_STS/</url>
      
        <content type="html"><![CDATA[<h1 id="AWS-Security-Token-Service"><a href="#AWS-Security-Token-Service" class="headerlink" title="AWS Security Token Service"></a>AWS <code>Security Token Service</code></h1><ul><li>Giống với việc sử dụng accesskey + secretKey (đã được gán role, permission…) để access vào resource. Nhưng khác là<br>accessKey,secretKey của STS thì có <code>expire time</code></li></ul><h2 id="1-Implementation"><a href="#1-Implementation" class="headerlink" title="1. Implementation"></a>1. Implementation</h2><h3 id="1-1-Step-Overview"><a href="#1-1-Step-Overview" class="headerlink" title="1.1 Step Overview"></a>1.1 Step Overview</h3><ul><li>Step 1. Create a Cross Account Role in IAM</li><li>Step 2. Attach a S3ReadOnly policy to that IAM Role (<code>S3ReadOnly</code> chỉ là ví dụ)</li><li>Step 3. Allow user to  <code>Assume Role</code> with STS</li><li>Step 4. Remove all other policy from user except <code>Assume Role</code></li></ul><h3 id="1-2-Create-IAM-Account"><a href="#1-2-Create-IAM-Account" class="headerlink" title="1.2 Create IAM Account"></a>1.2 Create IAM Account</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/sts/STS_CreateUser.JPG" alt="STS_CreateAccount"></p><ul><li>Tại client (client dùng awscli), kiểm tra với <code>aws s3 ls</code> để đảm bảo hiện tại client không thể access vào resource S3</li></ul><pre><code>An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied</code></pre><h3 id="1-3-Create-IAM-Role"><a href="#1-3-Create-IAM-Role" class="headerlink" title="1.3 Create IAM Role"></a>1.3 Create IAM Role</h3><p><a href="https://console.aws.amazon.com/iam/home?region=us-east-2#/roles$new?step=type&roleType=crossAccount">https://console.aws.amazon.com/iam/home?region=us-east-2#/roles$new?step=type&amp;roleType=crossAccount</a></p><ul><li>Role type: <code>Another AWS Account</code><ul><li>Input <code>Account ID*</code></li></ul></li><li>Attach permissions policies<ul><li>S3ReadOnly<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/sts/STS_CreateRole.JPG" alt="STS_CreateRole"></li></ul></li></ul><pre><code>arn:aws:iam::168146697673:role/sts_role_1</code></pre><h3 id="1-4-Set-Role-to-IAM-Account"><a href="#1-4-Set-Role-to-IAM-Account" class="headerlink" title="1.4 Set Role to IAM Account"></a>1.4 Set Role to IAM Account</h3><ul><li>Add permission</li></ul><pre><code class="json">&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;sts:AssumeRole&quot;,            &quot;Resource&quot;: &quot;arn:aws:iam::168146697673:role/sts_role_1&quot;        &#125;    ]&#125;</code></pre><h3 id="1-5-Generate-STS"><a href="#1-5-Generate-STS" class="headerlink" title="1.5 Generate STS"></a>1.5 Generate STS</h3><pre><code class="bash">aws sts assume-role --role-arn arn:aws:iam::168146697673:role/sts_role_1 --role-session-name tungexplorer</code></pre><ul><li>result example<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/sts/STS_GenerateSTS.JPG" alt="STS_Generate"></li><li>&#x2F;&#x2F; có thể truyền thêm param để set <code>duration time</code> cho sts sinh ra</li></ul><h3 id="1-6-Su-dung-STS-tai-client"><a href="#1-6-Su-dung-STS-tai-client" class="headerlink" title="1.6 Sử dụng STS tại client"></a>1.6 Sử dụng STS tại client</h3><ul><li>Ex: client dùng aws cli</li><li><code>nano .aws/credential</code></li></ul><pre><code class="bash">[sts_user_1]aws_access_key_id       = ASIASOJSS7XE6Y7LZWUJaws_secret_access_key   = rhWjBXKA+54vhliP7lYw96ISMJko6RdQK7c+wjjjaws_session_token       = FwoGZXIvYXdzEPL//////////wEaDK2sBRQ4KgL8M5xLECKwAaeA18ks+90pnrgFDGGriH9cN2GW/5hz3giGZZX/Kn3d9UkPz9N+Iz2Mtv35bI2mj13Ad5imOZZOEL1QwPsAIXMcDxKKiwoQW31u8sX6yfxWFbQS$</code></pre><ul><li>Test <code>sts</code></li></ul><pre><code class="bash">aws s3 ls --profile sts_user_1</code></pre><pre><code class="bash"># Example resultubuntu@tungexplorer:~$ aws s3 ls --profile sts_user_12020-01-25 13:12:52 config-bucket-1681466976732020-01-25 13:04:09 elasticbeanstalk-ap-east-1-1681466976732019-09-24 03:09:42 no-see-201909242019-09-24 02:14:22 ocr-test-201909242019-09-27 05:49:15 test-flog-log-20190927</code></pre><h2 id="2-Use-case"><a href="#2-Use-case" class="headerlink" title="2. Use case"></a>2. Use case</h2><ul><li><strong>Console Access with Broker</strong></li></ul><p><code>Use Case</code>: SSO to IAM console access using local directory service; no SAML</p><p><code>Flow</code>: User Requests browses to proxy; auth against directory which returns group membership; proxy gets list of roles<br>from groups via STS; user selects role; proxy calls STS:AssumeRole then returns auth package; proxy generates console<br>redirect.</p><p><code>Con</code>: IAM user required for proxy server</p><ul><li><strong>API Access with Broker</strong></li></ul><p><code>Use Case</code>: an app needs access to AWS resources via API; no SAML</p><p><code>Flow</code>: App requests session from proxy; auth against directory which returns entitlements; proxy requests session from<br>STS using STS:GetFederationToken; STS returns session; app calls AWS API using session</p><p><code>Cons</code>: The IAM user associated to the proxy must have GetFederationToken policy and all the premissions for all of the<br>users; and GetFederationToken does not support MFA</p><ul><li><strong>AssumeRole with SAML</strong></li></ul><p><code>Use Case</code>: SSO to IAM console without proxy server against AD or other SAML IdP</p><p><code>Background</code>: AD and it’s hosted version AWS Directory Service, LDAP and SAML can be integrated into IAM; generally you<br>map groups in the ID provider to IAM roles</p><p><code>Flow</code>: Auth with IdP which returns SAML token; with token in-hand the user is redirected to AWS sign-in endpoint for<br>SAML at <a href="https://signin.aws.amazon.com/saml">https://signin.aws.amazon.com/saml</a>; the endpoint calls AssumeRoleWithSAML then creates and passes the user a<br>console URL redirect; Roles must be configured to include the “saml:group”: “groupname”</p><p><code>Pro</code>: No dedicated proxy on the corporate side and the proxy requires no IAM user or permissions</p><p>ref: <a href="http://ric.mclaughlin.today/posts/aws-sts">http://ric.mclaughlin.today/posts/aws-sts</a></p>]]></content>
      
      
      <categories>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> sts </tag>
            
            <tag> security token service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Domain-Driven Design &amp; Event-Driven Architecture</title>
      <link href="/2020/01/Other/DDD_EventDrivenArchitecture/"/>
      <url>/2020/01/Other/DDD_EventDrivenArchitecture/</url>
      
        <content type="html"><![CDATA[<h1 id="Domain-Driven-Design-and-Event-Driven-Architecture"><a href="#Domain-Driven-Design-and-Event-Driven-Architecture" class="headerlink" title="Domain-Driven Design and Event-Driven Architecture"></a>Domain-Driven Design and Event-Driven Architecture</h1><ul><li>When placing orders during peak hours, customers care about getting the right products, in the right quantity, with a successful notification. The processing during peak hours is different from the processing of orders for operators later. This design ensures that the system runs fast and stable.</li><li>For example: talking about orders, there will be an order and order items. The database will have 2 tables, order and order item, but there will be only one Order object. When talking about an order, it must include both order and order items. There cannot be an order without items, and it doesn’t make sense to talk about order items without knowing the order they belong to.</li><li>If you need to retrieve 2 fields of information, use 2 separate queries (already global in the system), then aggregate them with code, rather than writing a new query just to get 2 fields for one specific need.</li><li><strong>Model</strong> is not a <strong>Table</strong>.<ul><li><strong>Table</strong> is designed for quick storage and retrieval.</li><li><strong>Model</strong> reflects the logical business properties of the system.</li></ul></li></ul><h2 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a>Aggregate</h2><ul><li>An aggregate is a group of data objects treated as a single unit within the system.<ul><li>Example: order and order items should be treated as consistent components of the aggregate order, identified in the system by the order id.</li></ul></li><li>Ensures a consistent view throughout the system.</li><li>When referring to an aggregate, it must be a complete, fully defined data object.</li><li>There should be no separate business logic for each component of an aggregate.</li><li>All logic from data access to service should revolve around aggregates.</li><li>Choose the “just right” scope for aggregates.</li><li>Too large an aggregate scope leads to poor performance.</li><li>Too small an aggregate scope leads to fragmented and hard-to-manage logic.</li><li>Ensure the components of an aggregate are always consistent.</li><li>Use unified data access patterns:<ul><li>Repository Pattern</li><li>ORM</li></ul></li><li>Structure API resources corresponding to aggregates.</li></ul><h2 id="Don’t-Repeat-Yourself"><a href="#Don’t-Repeat-Yourself" class="headerlink" title="Don’t Repeat Yourself:"></a>Don’t Repeat Yourself:</h2><ul><li>Don’t confuse Aggregate with DTO (DTO is for working with a specific data block and writing code for Data Access for that data layer).</li><li>Don’t design APIs based on display needs.</li><li>Don’t build data access for each function.</li></ul><h3 id="Architectural-Model"><a href="#Architectural-Model" class="headerlink" title="Architectural Model"></a>Architectural Model</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/ddd/MoHinhKienTruc.JPG" alt="MoHinhKienTruc"></p><h3 id="Infrastructure-Layer"><a href="#Infrastructure-Layer" class="headerlink" title="Infrastructure Layer"></a>Infrastructure Layer</h3><ul><li>Works with underlying components such as DB, Message Queue, File…</li><li>Most of the system logic is business logic.</li><li>Don’t let infrastructure logic slow down business development.</li><li>Must be highly reusable.</li><li>Must be consistent throughout the system structure.</li><li><strong>Patterns:</strong><ul><li>Repository Pattern</li><li>Observer Pattern</li><li>ORM Pattern</li></ul></li></ul><h3 id="Reusability-Level"><a href="#Reusability-Level" class="headerlink" title="Reusability Level"></a>Reusability Level</h3><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/java/ddd/layer.JPG" alt="Layer"></p><ul><li>Higher layers are used more frequently.</li><li>Lower layers must be highly reusable.</li><li>Avoid designing equal complexity across all layers.</li></ul><h3 id="Ensuring-Message-Order"><a href="#Ensuring-Message-Order" class="headerlink" title="Ensuring Message Order"></a>Ensuring Message Order</h3><ul><li>Ensure data write order.</li><li>Ensure event sending order.</li><li>Ensure event receiving order.</li></ul><h3 id="Solution-to-Ensure-Write-Order"><a href="#Solution-to-Ensure-Write-Order" class="headerlink" title="Solution to Ensure Write Order"></a>Solution to Ensure Write Order</h3><ul><li><strong>Solution 1:</strong> Lock by key range, use transaction mode lock serializable. Example: lock by order id.</li><li><strong>Solution 2:</strong> Use two resources:<br>R1: snapshot of the object<br>R2: event log<br>Lock the update command on R1 and append to R2.</li><li><strong>Solution 3:</strong> If storing events in a table, use a composite primary key of the event:<br>Aggregate Id – Version<br>Events with the same version will conflict when inserting.</li></ul><h3 id="Solution-to-Ensure-Sending-Order"><a href="#Solution-to-Ensure-Sending-Order" class="headerlink" title="Solution to Ensure Sending Order"></a>Solution to Ensure Sending Order</h3><ul><li>Don’t apply simultaneous DB write and event send because there’s no transaction.</li><li>Must log events before sending them.</li><li>Group events by the id of the aggregate that emits the event. Store events with a composite key: aggregateId – version.</li><li>Load events to be sent by aggregate id, and send sequentially by version.</li><li>Stop sending immediately if an error event is encountered.</li><li>Recover -&gt; Load by aggregateId and continue sending in version order.</li><li>Use two tables:<ul><li>Event table to store events.</li><li>Undispatched Event table to temporarily store events awaiting dispatch, deleted after sending.</li></ul></li></ul><h3 id="Ensuring-Message-Reception-Order"><a href="#Ensuring-Message-Reception-Order" class="headerlink" title="Ensuring Message Reception Order"></a>Ensuring Message Reception Order</h3><ul><li>Group messages by an identifier, such as aggregate id.</li><li>Messages in a group should be received by one thread at a time.</li><li>Use Kafka’s Partition or Windows Service Bus’s Session features to ensure message order when routing.</li></ul><hr><p><strong>References:</strong></p><ul><li><a href="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/other_file/DDD_EventDrivenArchitecture.pdf">DDD Event-Driven Architecture PDF</a></li><li><a href="https://youtu.be/glZs4QFfwbc">YouTube Video</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ddd </tag>
            
            <tag> domain driver design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS - LAB - ECS</title>
      <link href="/2020/01/AWS/AWS_LAB_ECS/"/>
      <url>/2020/01/AWS/AWS_LAB_ECS/</url>
      
        <content type="html"><![CDATA[<h1 id="Amazon-Elastic-Container-Service-ECS"><a href="#Amazon-Elastic-Container-Service-ECS" class="headerlink" title="Amazon Elastic Container Service (ECS)"></a>Amazon Elastic Container Service (ECS)</h1><ul><li><p>ECS manages Docker containers and services.</p></li><li><p>ECS is similar to Docker Swarm.</p></li><li><p>When creating ECS, you have two options: Fargate or EC2. The main difference is that Fargate does not provide instances, while EC2 does.</p><ul><li><p><strong>Fargate</strong>:</p><ul><li>Price based on task size.</li><li>Requires network mode awsvpc.</li><li>AWS-managed infrastructure, no Amazon EC2 instances to manage.</li></ul></li><li><p><strong>EC2</strong>:</p><ul><li>Price based on resource usage.</li><li>Multiple network modes available.</li><li>Self-managed infrastructure using Amazon EC2 instances.</li></ul></li></ul></li></ul><h2 id="What-is-the-difference-between-a-task-and-a-service-in-AWS-ECS"><a href="#What-is-the-difference-between-a-task-and-a-service-in-AWS-ECS" class="headerlink" title="What is the difference between a task and a service in AWS ECS?"></a>What is the difference between a task and a service in AWS ECS?</h2><ul><li><p><strong>Task Definition</strong>: A collection of one or more container configurations. Some tasks may need only one container, while other tasks may need two or more potentially linked containers running concurrently. The task definition allows you to specify which Docker image to use, which ports to expose, how much CPU and memory to allot, how to collect logs, and define environment variables.</p></li><li><p><strong>Task</strong>: Created when you run a task directly, which launches containers (defined in the task definition) until they are stopped or exit on their own, at which point they are not replaced automatically. Running tasks directly is ideal for short-running jobs, such as those performed via CRON.</p></li><li><p><strong>Service</strong>: Used to ensure that you always have a certain number of tasks running at all times. If a task’s container exits due to an error, or the underlying EC2 instance fails and is replaced, the ECS Service will replace the failed task. This is why we create clusters so that the service has plenty of resources in terms of CPU, memory, and network ports to use. It doesn’t really matter which instance tasks run on as long as they run. A service configuration references a task definition. A service is responsible for creating tasks.</p></li></ul><p>Services are typically used for long-running applications like web servers. For example, if I deployed my website powered by Node.JS in Oregon (us-west-2), I would want at least three tasks running across the three Availability Zones (AZ) for the sake of high availability; if one fails, I have another two, and the failed one will be replaced (self-healing). Creating a service is the way to do this. If I had six EC2 instances in my cluster, two per AZ, the service will automatically balance tasks across zones as best it can while also considering CPU, memory, and network resources.</p><p><em>UPDATE</em>:</p><pre><code class="text">I&#39;m not sure it helps to think of these things hierarchically.Another very important point is that a service can be configured to use a load balancer, so that as it creates the tasks—that is, it launches containers defined in the task definition—the service will automatically register the container&#39;s EC2 instance with the load balancer. Tasks cannot be configured to use a load balancer, only services can.</code></pre>]]></content>
      
      
      <categories>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ecs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS - LAB - ECR</title>
      <link href="/2020/01/AWS/AWS_LAB_ECR/"/>
      <url>/2020/01/AWS/AWS_LAB_ECR/</url>
      
        <content type="html"><![CDATA[<h1 id="Amazon-Elastic-Container-Registry"><a href="#Amazon-Elastic-Container-Registry" class="headerlink" title="Amazon Elastic Container Registry"></a>Amazon <code>Elastic Container Registry</code></h1><ul><li>là dịch vụ lưu trữ, quản lý các <code>docker container images</code> trên AWS</li><li>có thể hình dung nó giống như docker registry, nhưng chạy trên cloud</li></ul><p><a href="https://ap-east-1.console.aws.amazon.com/ecr/home?region=ap-east-1#">https://ap-east-1.console.aws.amazon.com/ecr/home?region=ap-east-1#</a></p><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/ecs/ECR_create.PNG" alt="ECR_create"></p><h2 id="1-Demo"><a href="#1-Demo" class="headerlink" title="1. Demo"></a>1. Demo</h2><h3 id="1-1-create-docker-images"><a href="#1-1-create-docker-images" class="headerlink" title="1.1 create docker images"></a>1.1 create docker images</h3><ul><li>Dockerfile</li></ul><pre><code class="bash">FROM ubuntu:16.04# Install dependenciesRUN apt-get updateRUN apt-get -y install apache2# Install apache and write &quot;Hello Tungexplorer&quot; RUN echo &quot;Hello Tungexplorer&quot; &gt; /var/www/html/index.html# Configure apacheRUN echo &#39;. /etc/apache2/envvars&#39; &gt; /root/run_apache.shRUN echo &#39;mkdir -p /var/run/apache2&#39; &gt;&gt; /root/run_apache.shRUN echo &#39;mkdir -p /var/lock/apache2&#39; &gt;&gt; /root/run_apache.shRUN echo &#39;/usr/sbin/apache2 -D FOREGROUND&#39; &gt;&gt; /root/run_apache.shRUN chmod 755 /root/run_apache.shEXPOSE 80CMD /root/run_apache.sh</code></pre><ul><li>build docker images from Dockerfile</li></ul><pre><code class="bash">docker build -t myapache .</code></pre><ul><li>check image build done</li></ul><pre><code class="bash">docker images</code></pre><ul><li>run container</li></ul><pre><code> docker run -d -p 80:80 myapache</code></pre><ul><li>Result:</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/ecs/ECR_build_docker_done.PNG" alt="Docker Build Done"></p><h3 id="1-2-Push-docker-images-to-ECR"><a href="#1-2-Push-docker-images-to-ECR" class="headerlink" title="1.2 Push docker images to ECR"></a>1.2 Push docker images to ECR</h3><ul><li>tạo EC2 role, và attact vào Instance, mục đích để khi chạy aws cli không phải configure</li></ul><pre><code>    &gt; IAM        &gt; Roles            &gt; Create Role                &gt; EC2                    &gt; Select &quot;AmazonEC2ContainerRegistryFullAccess&quot; </code></pre><p><a href="https://console.aws.amazon.com/iam/home?region=ap-east-1#/roles/full_ECR">https://console.aws.amazon.com/iam/home?region=ap-east-1#/roles/full_ECR</a><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/ecs/ECS_CreateRole.PNG" alt="ECS_CreateRole"></p><ul><li>attact Role vừa tạo vào EC2</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/ecs/ECS_Attact_Role.PNG" alt="ECS_Attact_Role"></p><ul><li>login</li></ul><pre><code>aws ecr get-login --no-include-email --region ap-east-1</code></pre><p>sau khi run command trên, kết quả trả về format như sau:</p><pre><code>docker login -u AWS -p eyJwYXlsb2FkIjoicndSNGY4bUJTY2xXZUdReHo5NWN0K3RTMzdGUkdIdEsyZnFsRys1a2RaMlFDL0JyNk5JNHpMLzNbTBUSXJuQUFhdUdlQT09IiwidmVyc2lvbiI6IjIiLCJ0eXBlIjoiREFUQV9LRVkiLCJleHBpcmF0aW9uIjoxNTc5OTc2MjEzfQ== https://168146697673.dkr.ecr.ap-east-1.amazonaws.com</code></pre><p>thực hiện run command trả về, để login vào ecr (login vào thì mới push images được)</p><ul><li>docker tag</li></ul><pre><code>docker tag myapache:latest 168146697673.dkr.ecr.ap-east-1.amazonaws.com/tungexplorer:latest</code></pre><ul><li>docker push lên ECR</li></ul><pre><code>docker push 168146697673.dkr.ecr.ap-east-1.amazonaws.com/tungexplorer</code></pre><pre><code class="bash"># result expubuntu@ip-172-31-21-90:~$ docker push 168146697673.dkr.ecr.ap-east-1.amazonaws.com/tungexplorerThe push refers to repository [168146697673.dkr.ecr.ap-east-1.amazonaws.com/tungexplorer]6b7f18c2e1d7: Pushed 93ebf59e7508: Pushed 53bb19ca1654: Pushed fb9d3d6977d1: Pushed bfaa8df62234: Pushed fd6a28796899: Pushed 5933364a1fec: Pushed 164f642a3cbc: Pushed fa1693d66d0b: Pushed 293b479c17a5: Pushed bd95983a8d99: Pushed 96eda0f553ba: Pushed latest: digest: sha256:90780667e1c4f42a8818d6b30260bf98078e6f7d693fd8245b2be4cab83bc216 size: 2816ubuntu@ip-172-31-21-90:~$ </code></pre><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/ecs/ECS_uploadImagesDone.PNG" alt="ECS_UplaodImagesDone"></p>]]></content>
      
      
      <categories>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ecr </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Install Guide - Mount an EBS volume to EC2 Linux</title>
      <link href="/2020/01/Install_Guide/Mount_EBS_to_EC2/"/>
      <url>/2020/01/Install_Guide/Mount_EBS_to_EC2/</url>
      
        <content type="html"><![CDATA[<h1 id="Mount-an-EBS-volume-to-EC2-Linux"><a href="#Mount-an-EBS-volume-to-EC2-Linux" class="headerlink" title="Mount an EBS volume to EC2 Linux"></a>Mount an EBS volume to EC2 Linux</h1><p>In this tutorial, we will teach you how to attach and mount an EBS volume to ec2 linux instances. Follow the steps given<br>below carefully for the setup.</p><p><strong>Step 1:</strong> Head over to EC2 –&gt; Volumes and create a new volume of your preferred size and type.</p><p><strong>Step 2:</strong> Select the created volume, right click and select the “attach volume” option.<br><strong>Step 3:</strong> Select the instance from the instance text box as shown below.<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/ebs/EBS_Attact.jpg" alt="attact"></p><p><strong>Step 4:</strong> Now, login to your ec2 instance and list the available disks using the following command.</p><pre><code>lsblk</code></pre><p>The above command will list the disk you attached to your instance.</p><p><strong>Step 5:</strong> Check if the volume has any data using the following command.</p><pre><code class="bash">sudo file -s /dev/xvdf</code></pre><p>If the above command output shows “&#x2F;dev&#x2F;xvdf: data”, it means your volume is empty.</p><p><strong>Step 6:</strong> Format the volume to ext4 filesystem using the following command.</p><pre><code class="bash">sudo mkfs -t ext4 /dev/xvdf</code></pre><p><strong>Step 7:</strong> Create a directory of your choice to mount our new ext4 volume. I am using the name “newvolume”</p><pre><code>sudo mkdir /newvolume</code></pre><p><strong>Step 8:</strong> Mount the volume to “newvolume” directory using the following command.</p><pre><code>sudo mount /dev/xvdf /newvolume/</code></pre><p><strong>Step 9:</strong> cd into newvolume directory and check the disk space for confirming the volume mount.</p><pre><code>cd /newvolume df -h .</code></pre><p>The above command would show the free space in the newvolume directory.</p><p>To unmount the volume, you have to use the following command.</p><pre><code>umount /dev/xvdf</code></pre><h3 id="EBS-Automount-on-Reboot"><a href="#EBS-Automount-on-Reboot" class="headerlink" title="EBS Automount on Reboot"></a>EBS Automount on Reboot</h3><p>By default on every reboot the EBS volumes other than root volume will get unmounted. To enable automount, you need to<br>make an entry in the &#x2F;etc&#x2F;fstab file.</p><ol><li>Back up the &#x2F;etc&#x2F;fstab file.</li></ol><pre><code>sudo cp /etc/fstab /etc/fstab.bak</code></pre><ol start="2"><li>Open &#x2F;etc&#x2F;fstab file and make an entry in the following format.</li></ol><pre><code>device_name mount_point file_system_type fs_mntops fs_freq fs_passno</code></pre><p>For example,</p><pre><code>/dev/xvdf       /newvolume   ext4    defaults,nofail        0       0</code></pre><ol start="3"><li>Execute the following command to check id the fstab file has any error.</li></ol><pre><code>sudo mount -a</code></pre><p>If the above command shows no error, it means your fstab entry is good.</p><p>Now, on every reboot the extra EBS volumes will get mounted automatically.</p><p>That’s how you mount and unmount EBS volumes in your ec2 instances. If you get any error during the setup, please feel<br>free to contact us in the comment section.</p><p>&#x2F;&#x2F; ref: <a href="https://devopscube.com/mount-ebs-volume-ec2-instance/">https://devopscube.com/mount-ebs-volume-ec2-instance/</a></p>]]></content>
      
      
      <categories>
          
          <category> install_guide </category>
          
      </categories>
      
      
        <tags>
            
            <tag> install </tag>
            
            <tag> EBS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux - Bash Script Collection</title>
      <link href="/2020/01/Linux/Linux_BashScript/"/>
      <url>/2020/01/Linux/Linux_BashScript/</url>
      
        <content type="html"><![CDATA[<h3 id="Run-file-after-restart-os"><a href="#Run-file-after-restart-os" class="headerlink" title="Run file after restart os"></a>Run file after restart os</h3><pre><code class="bash"># In the file you put in /etc/init.d/ you have to set it executable withchmod +x /etc/init.d/start_my_app# if this does not run you have to create a symlink to /etc/rc.d/ln -s /etc/init.d/start_my_app /etc/rc.d/# And don&#39;t forget to add on top of that file:#!/bin/sh</code></pre><h3 id="Nohup"><a href="#Nohup" class="headerlink" title="Nohup"></a>Nohup</h3><ul><li>Tạo file service bất kỳ trong <code>cd /etc/init.d/</code></li><li>Ex <code>nano myservice</code></li><li>key:</li></ul><pre><code>nohup java -jar -Dspring.profiles.active=test $PATH_TO_JAR  &gt;&gt; $LOG_DIR 2&gt;&amp;1&amp;</code></pre><pre><code class="bash">#!/bin/shSERVICE_NAME=myservicePATH_TO_JAR=/home/share/application.jarLOG_DIR =/home/share/log.txtPID_PATH_NAME=/tmp/application-pidcase $1 in   start)       echo &quot;Starting $SERVICE_NAME ...&quot;       if [ ! -f $PID_PATH_NAME ]; then           nohup java -jar -Dspring.profiles.active=test $PATH_TO_JAR  &gt;&gt; $LOG_DIR 2&gt;&amp;1&amp;           echo $! &gt; $PID_PATH_NAME           echo &quot;$SERVICE_NAME started ...&quot;       else           echo &quot;$SERVICE_NAME is already running ...&quot;       fi   ;;   stop)       if [ -f $PID_PATH_NAME ]; then           PID=$(cat $PID_PATH_NAME);           echo &quot;$SERVICE_NAME stoping ...&quot;           kill $PID;           echo &quot;$SERVICE_NAME stopped ...&quot;           rm $PID_PATH_NAME       else           echo &quot;$SERVICE_NAME is not running ...&quot;       fi   ;;   restart)       if [ -f $PID_PATH_NAME ]; then           PID=$(cat $PID_PATH_NAME);           echo &quot;$SERVICE_NAME stopping ...&quot;;           kill $PID;           echo &quot;$SERVICE_NAME stopped ...&quot;;           rm $PID_PATH_NAME           echo &quot;$SERVICE_NAME starting ...&quot;           nohup java -jar -Dspring.profiles.active=test $PATH_TO_JAR &gt;&gt; $LOG_DIR 2&gt;&amp;1&amp;           echo $! &gt; $PID_PATH_NAME           echo &quot;$SERVICE_NAME started ...&quot;       else           echo &quot;$SERVICE_NAME is not running ...&quot;       fi   ;;esac </code></pre><ul><li>SERVICE_NAME &#x3D;&gt; tên của service, đặt tùy thích, lúc xem monitor các process của ubuntu, tên cái service này nó sẽ hiển<br>thị</li><li>PATH_TO_JAR &#x3D;&gt; đường dẫn trỏ tới file .jar cần chạy</li><li>LOG_DIR &#x3D;&gt; đường dẫn tới chỗ để ghi log lại</li><li>PID_PATH_NAME &#x3D;&gt; đánh dấu cái pid (mã ID process cái service), mục đích để khi gõ command stop, nó sẽ sử dụng cái pid<br>đó để kill process.</li></ul><h3 id="Create-User-and-add-sshkey"><a href="#Create-User-and-add-sshkey" class="headerlink" title="Create User and add sshkey"></a>Create User and add sshkey</h3><h4 id="1-Server"><a href="#1-Server" class="headerlink" title="1. Server"></a>1. Server</h4><p>Create Home Directory + .ssh Directory</p><pre><code class="shell">mkdir -p /home/deploy/.ssh</code></pre><p>Create Authorized Keys File</p><pre><code class="shell">touch /home/deploy/.ssh/authorized_keys</code></pre><p>Create User + Set Home Directory</p><pre><code class="shell">useradd -d /home/deploy deploy</code></pre><p>Add User to sudo Group</p><pre><code class="shell">usermod -aG sudo deploy</code></pre><p>Set Permissions</p><pre><code class="shell">chown -R deploy:deploy /home/deploy/chown root:root /home/deploychmod 700 /home/deploy/.sshchmod 644 /home/deploy/.ssh/authorized_keys</code></pre><h4 id="2-Client"><a href="#2-Client" class="headerlink" title="2. Client"></a>2. Client</h4><p>For example, to generate an RSA key, I’d use:</p><pre><code class="shell">ssh-keygen -a 1000 -b 4096 -C &quot;&quot; -E sha256 -o -t rsa</code></pre><p>Get ssh key</p><pre><code class="shell">cat ~/.ssh/id_rsa.pub</code></pre><p>Paste sshkey to server</p><pre><code class="shell">/home/mynewuser/.ssh/authorized_keys</code></pre><h3 id="Get-Public-IP"><a href="#Get-Public-IP" class="headerlink" title="Get Public IP"></a>Get Public IP</h3><pre><code class="bash">dig +short myip.opendns.com @resolver1.opendns.com</code></pre><h3 id="kill-process-by-port"><a href="#kill-process-by-port" class="headerlink" title="kill process by port"></a>kill process by port</h3><pre><code class="bash">wget https://raw.github.com/abdennour/miscs.sh/master/killportkillport 3000</code></pre><h3 id="check-port-running"><a href="#check-port-running" class="headerlink" title="check port running"></a>check port running</h3><pre><code class="bash">sudo netstat -lnp</code></pre><h3 id="show-size-directory-“MB”"><a href="#show-size-directory-“MB”" class="headerlink" title="show size directory, “MB”"></a>show size directory, “MB”</h3><pre><code class="bash">alias ls=&quot;ls --block-size=M&quot;</code></pre><h3 id="Show-log-script"><a href="#Show-log-script" class="headerlink" title="Show log script"></a>Show log script</h3><pre><code class="bash">DATE=$(date +%F)cd /home/deploy/logs/webls | grep -v &#39;.gz&#39; &gt;tempFILE_LOG=$(grep $DATE temp)echo $FILE_LOGtailf $FILE_LOG</code></pre><h3 id="Set-icon-desktop-for-anyapp"><a href="#Set-icon-desktop-for-anyapp" class="headerlink" title="Set icon desktop for anyapp"></a>Set icon desktop for anyapp</h3><ul><li><code>cd /usr/share/applications</code></li><li><code>sudo gedit outline.desktop</code></li></ul><pre><code>[Desktop Entry]Name=OutlineComment=Outline VPNExec=&quot;/home/tungtv/Downloads/opt/Outline-Client.AppImage&quot; %UTerminal=falseType=ApplicationIcon=/home/tungtv/Pictures/icon/outline.pngStartupWMClass=OutlineCategories=Utility;</code></pre><ul><li>sudo updatedb</li></ul>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> script </tag>
            
            <tag> install </tag>
            
            <tag> bash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Spring web Socket + StompJS</title>
      <link href="/2020/01/Java/SpringBoot_WebSocket_StompJS/"/>
      <url>/2020/01/Java/SpringBoot_WebSocket_StompJS/</url>
      
        <content type="html"><![CDATA[<h1 id="Code-example-Spring-WebSocket"><a href="#Code-example-Spring-WebSocket" class="headerlink" title="Code example Spring WebSocket"></a>Code example Spring WebSocket</h1><h2 id="1-Target"><a href="#1-Target" class="headerlink" title="1. Target"></a>1. Target</h2><ul><li>Thử nghiệm websocket:<ul><li>Từ server gửi payload tới client thông qua websocket.</li><li>Từ client gửi payload tới topic</li></ul></li></ul><h2 id="2-Stack"><a href="#2-Stack" class="headerlink" title="2. Stack"></a>2. Stack</h2><ul><li>Spring boot</li><li>Sprint web socket</li><li>Sockjs.js</li><li>Stomp.js</li></ul><h2 id="3-Code"><a href="#3-Code" class="headerlink" title="3. Code"></a>3. Code</h2><h3 id="3-1-Configuration"><a href="#3-1-Configuration" class="headerlink" title="3.1 Configuration"></a>3.1 Configuration</h3><pre><code class="java">@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketConfig extends AbstractWebSocketMessageBrokerConfigurer &#123;    @Override    public void registerStompEndpoints(StompEndpointRegistry stompEndpointRegistry) &#123;        stompEndpointRegistry.addEndpoint(&quot;/websocket-example&quot;)            .withSockJS();    &#125;    @Override    public void configureMessageBroker(MessageBrokerRegistry registry) &#123;        registry.enableSimpleBroker(&quot;/topic&quot;);        registry.setApplicationDestinationPrefixes(&quot;/app&quot;);    &#125;&#125;</code></pre><ul><li><code>@EnableWebSocketMessageBroker</code> : enabled a broker-backed messaging over WebSocket using STOMP</li><li><code>registerStompEndpoints</code> : khai báo registerEndpoint, tất cả các socket client sẽ dùng url này để register, trước khi<br>subscribe</li><li><code>configureMessageBroker</code> …</li></ul><h3 id="3-2-Controller"><a href="#3-2-Controller" class="headerlink" title="3.2 @Controller"></a>3.2 @Controller</h3><pre><code class="java">@Controllerpublic class TestController &#123;    @Autowired    SimpMessagingTemplate template;    @RequestMapping(&quot;/test&quot;)    public void sendAdhocMessages(@RequestParam(&quot;payload&quot;) String payload) &#123;        template.convertAndSend(&quot;/topic/001&quot;, payload);    &#125;    @MessageMapping(&quot;/rm002&quot;)    @SendTo(&quot;/topic/001&quot;)    public String getUser(String payload) &#123;        return (&quot;From rm002 &quot; + payload);    &#125;&#125;</code></pre><ul><li><code> @RequestMapping(&quot;/test&quot;)</code> : dùng để test từ server gửi payload tới client, thông qua RequestMapping</li><li><code>@MessageMapping(&quot;/rm002&quot;)</code> : khi client gửi message tới “mapping” này, thì message sẽ được redirect tới <code>/topic/001</code>.<br>Tham khảo đoạn code js bên dưới để hiểu hơn.</li></ul><h3 id="3-3-Socket-Client"><a href="#3-3-Socket-Client" class="headerlink" title="3.3 Socket Client"></a>3.3 Socket Client</h3><pre><code class="html">&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;    &lt;title&gt;Hello WebSocket&lt;/title&gt;    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/sockjs-client/1.4.0/sockjs.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/stomp.js/2.3.3/stomp.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;test&lt;/body&gt;&lt;/html&gt;&lt;script&gt;    var SOCKET_URL_REGISTER = &#39;http://localhost:8091/websocket-example&#39;;    var SOCKET_TOPIC_SUBSCRIBE = &#39;/topic/001&#39;    var stompClient = null;    $(document).ready(function () &#123;        var socket = new SockJS(SOCKET_URL_REGISTER);        stompClient = Stomp.over(socket);        stompClient.connect(&#123;&quot;X-Token&quot;: &quot;123&quot;&#125;, onConnected, onError);    &#125;);    function onConnected() &#123;        stompClient.subscribe(SOCKET_TOPIC_SUBSCRIBE, onMessageReceived);    &#125;    function onMessageReceived(payload) &#123;        console.log(payload)    &#125;    function onError(error) &#123;        console.log(error);    &#125;    function sendMessage() &#123;        stompClient.send(&quot;/app/rm002&quot;, &#123;&#125;, &quot;payload test 123&quot;);    &#125;&lt;/script&gt;</code></pre><h2 id="4-Demo"><a href="#4-Demo" class="headerlink" title="4. Demo"></a>4. Demo</h2><ul><li>Source code: <a href="https://github.com/tungtv202/spring-boot-websocket-example">https://github.com/tungtv202/spring-boot-websocket-example</a></li><li>Sau khi build, truy cập vào url: <a href="http://localhost:8091/">http://localhost:8091/</a> , và F12 để theo dõi console<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/websocket/1.JPG" alt="1"></li><li>Để test case server gửi payload tới client đã subscribe, thực hiện truy cập<br>url <a href="http://localhost:8091/test?payload=123">http://localhost:8091/test?payload=123</a>  (tạo 1 cửa sổ web browser khác)</li><li>Để test case client send message tới topic, gõ <code>sendMessage()</code> tại console F12 của web browser</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> spring </tag>
            
            <tag> web socket </tag>
            
            <tag> stompJs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Javers Diff</title>
      <link href="/2020/01/Java/Javers_Diff/"/>
      <url>/2020/01/Java/Javers_Diff/</url>
      
        <content type="html"><![CDATA[<h1 id="Javers-Object-auditing-and-diff-framework-for-Java"><a href="#Javers-Object-auditing-and-diff-framework-for-Java" class="headerlink" title="Javers - Object auditing and diff framework for Java"></a>Javers - Object auditing and diff framework for Java</h1><p>Ref:</p><ul><li><a href="https://www.baeldung.com/javers">https://www.baeldung.com/javers</a></li><li><a href="https://javers.org/">https://javers.org/</a></li></ul><p>Source Code : <a href="https://github.com/tungtv202/javerdiff">https://github.com/tungtv202/javerdiff</a></p><h2 id="1-Maven"><a href="#1-Maven" class="headerlink" title="1. Maven"></a>1. Maven</h2><pre><code class="xml">&lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.javers&lt;/groupId&gt;            &lt;artifactId&gt;javers-core&lt;/artifactId&gt;            &lt;version&gt;3.1.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;version&gt;1.18.8&lt;/version&gt;            &lt;scope&gt;provided&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.assertj&lt;/groupId&gt;            &lt;artifactId&gt;assertj-core&lt;/artifactId&gt;            &lt;version&gt;3.13.2&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.13&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><h2 id="2-Code-demo"><a href="#2-Code-demo" class="headerlink" title="2. Code demo"></a>2. Code demo</h2><pre><code class="java">public class JaverDiffTest &#123;    @Test    public void givenPersonObject_whenApplyModificationOnIt_thenShouldDetectChange() &#123;        // given        Javers javers = JaversBuilder.javers().build();        Person person = new Person(1, &quot;Michael Program&quot;);        Person personAfterModification = new Person(1, &quot;Michael Java&quot;);        // when        Diff diff = javers.compare(person, personAfterModification);        System.out.println(javers.getJsonConverter().toJson(diff));        // then        ValueChange change = diff.getChangesByType(ValueChange.class).get(0);        assertThat(diff.getChanges()).hasSize(1);        assertThat(change.getPropertyName()).isEqualTo(&quot;name&quot;);        assertThat(change.getLeft()).isEqualTo(&quot;Michael Program&quot;);        assertThat(change.getRight()).isEqualTo(&quot;Michael Java&quot;);    &#125;    @Test    public void givenListOfPersons_whenCompare_ThenShouldDetectChanges() &#123;        // given        Javers javers = JaversBuilder.javers().build();        Person personThatWillBeRemoved = new Person(2, &quot;Thomas Link&quot;);        List&lt;Person&gt; oldList =                Lists.asList(new Person(1, &quot;Michael Program&quot;));        List&lt;Person&gt; newList =                Lists.asList(new Person(1, &quot;Michael Not Program&quot;), new Person(3, &quot;new object&quot;));        // when        Diff diff = javers.compareCollections(oldList, newList, Person.class);        // then        assertThat(diff.getChanges()).hasSize(3);        ValueChange valueChange = diff.getChangesByType(ValueChange.class).get(0);        assertThat(valueChange.getPropertyName()).isEqualTo(&quot;name&quot;);        assertThat(valueChange.getLeft()).isEqualTo(&quot;Michael Program&quot;);        assertThat(valueChange.getRight()).isEqualTo(&quot;Michael Not Program&quot;);        ObjectRemoved objectRemoved = diff.getChangesByType(ObjectRemoved.class).get(0);        assertThat(objectRemoved.getAffectedObject().get().equals(personThatWillBeRemoved))                .isTrue();        ListChange listChange = diff.getChangesByType(ListChange.class).get(0);        assertThat(listChange.getValueRemovedChanges().size()).isEqualTo(1);    &#125;    @Test    public void givenListOfPerson_whenPersonHasNewAddress_thenDetectThatChange() &#123;        // given        Javers javers = JaversBuilder.javers().build();        PersonWithAddress person =                new PersonWithAddress(1, &quot;Tom&quot;, Arrays.asList(new Address(&quot;England&quot;)));        PersonWithAddress personWithNewAddress =                new PersonWithAddress(1, &quot;Tom&quot;,                        Arrays.asList(new Address(&quot;England&quot;), new Address(&quot;USA&quot;)));        // when        Diff diff = javers.compare(person, personWithNewAddress);        List objectsByChangeType = diff.getObjectsByChangeType(NewObject.class);        // then        assertThat(objectsByChangeType).hasSize(1);        assertThat(objectsByChangeType.get(0).equals(new Address(&quot;USA&quot;)));    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> javers </tag>
            
            <tag> diff </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feign Client - Customize Feign By Bean</title>
      <link href="/2020/01/Java/FeignClient/FeignClient_CustomizeByBean/"/>
      <url>/2020/01/Java/FeignClient/FeignClient_CustomizeByBean/</url>
      
        <content type="html"><![CDATA[<h1 id="Customize-lai-FeignClient-bang-cach-tao-ra-cac-Bean-mac-dinh"><a href="#Customize-lai-FeignClient-bang-cach-tao-ra-cac-Bean-mac-dinh" class="headerlink" title="Customize lại FeignClient bằng cách tạo ra các Bean mặc định"></a>Customize lại FeignClient bằng cách tạo ra các Bean mặc định</h1><ul><li>Các class interface khai báo <code>@FeignClient</code> thì sẽ mặc định sử dụng các config customize này</li></ul><h2 id="1-Code"><a href="#1-Code" class="headerlink" title="1. Code"></a>1. Code</h2><ul><li>config</li></ul><pre><code class="java">@Configurationpublic class FeignClientConfig &#123;    private UserConfig userConfig;    private ObjectMapper main;    private ObjectMapper json;    private String applicationName;    @Autowired    public FeignClientConfig(@Autowired UserConfig userConfig, @Qualifier(&quot;json_main&quot;) ObjectMapper main, @Qualifier(&quot;json&quot;) ObjectMapper json, @Value(&quot;$&#123;spring.application.name&#125;&quot;) String applicationName) &#123;        this.json = json;        this.main = main;        this.userConfig = userConfig;        this.applicationName = applicationName;    &#125;    @Bean    @Primary    public RequestInterceptor feignRequestInterceptorDefault() &#123;        String plainCreds = String.format(&quot;%s:%s&quot;, userConfig.getName(), userConfig.getPassword());        byte[] plainCredsBytes = plainCreds.getBytes();        byte[] base64CredsBytes = Base64.encodeBase64(plainCredsBytes);        String base64Creds = new String(base64CredsBytes);        return template -&gt; &#123;            template.header(&quot;Authorization&quot;, String.format(&quot;Basic %s&quot;, base64Creds));            template.header(&quot;Content-Type&quot;, &quot;application/json;charset=utf-8&quot;);            template.header(&quot;Application-Name&quot;, applicationName);        &#125;;    &#125;    @Bean    @Primary    public Decoder feignDecoderDefault() &#123;        return new ResponseEntityDecoder(new FeignDecode(main, json));    &#125;    @Bean    @Primary    public Encoder feignEncoderDefault() &#123;        return new JacksonEncoder(main);    &#125;    @Bean    @Primary    public Contract contractDefault() &#123;        return new SpringMvcContract();    &#125;    @Bean    public Retryer retryer() &#123;        return Retryer.NEVER_RETRY;    &#125;    @Bean    @Profile(&#123;&quot;debug&quot;, &quot;dev2&quot;&#125;)    public Logger.Level feignLoggerLevel() &#123;        return Logger.Level.FULL;    &#125;    @Bean    @Profile(&#123;&quot;debug&quot;, &quot;dev2&quot;&#125;)    public Logger logger() &#123;        return new Slf4jLogger(&quot;service&quot;);    &#125;&#125;</code></pre><ul><li>FeignDecode class</li></ul><pre><code class="java">public class FeignDecode extends JacksonDecoder &#123;    private ObjectMapper main;    private ObjectMapper json;    public FeignDecode(ObjectMapper main, ObjectMapper json) &#123;        this.main = main;        this.json = json;    &#125;    @Override    public Object decode(Response response, Type type) throws IOException &#123;        if (type.equals(CountResult.class)) &#123;            return decode(response, type, json);        &#125;        return decode(response, type, main);    &#125;    private Object decode(Response response, Type type, ObjectMapper mapper) throws IOException &#123;        if (response.status() == 404) return Util.emptyValueOf(type);        if (response.body() == null) return null;        Reader reader = response.body().asReader();        if (!reader.markSupported()) &#123;            reader = new BufferedReader(reader, 1);        &#125;        try &#123;            // Read the first byte to see if we have any data            reader.mark(1);            if (reader.read() == -1) &#123;                return null; // Eagerly returning null avoids &quot;No content to map due to end-of-input&quot;            &#125;            reader.reset();            return mapper.readValue(reader, mapper.constructType(type));        &#125; catch (RuntimeJsonMappingException e) &#123;            if (e.getCause() != null &amp;&amp; e.getCause() instanceof IOException) &#123;                throw IOException.class.cast(e.getCause());            &#125;            throw e;        &#125;    &#125;&#125;</code></pre><h2 id="2-Handler-Error"><a href="#2-Handler-Error" class="headerlink" title="2. Handler Error"></a>2. Handler Error</h2><pre><code class="java">@Configurationpublic class BeanConfig &#123;    @Bean    public ErrorDecoder errorDecoder() &#123;        return new FeignErrorHanlder();    &#125;&#125;    @CommonsLog(topic = &quot;topic.feign_client&quot;)public class FeignErrorHanlder implements ErrorDecoder &#123;    @Autowired    private ObjectMapper objectMapper;    @Override    public Exception decode(String methodKey, Response response) &#123;        var request = response.request();        FeignException result = errorStatus(methodKey, response);        try &#123;            log.error(String.format(&quot;---%s---%s\n&quot; +                            &quot;---ResponseStatus=%s\n&quot; +                            &quot;---Reason=%s\n&quot; +                            &quot;---Response=%s\n&quot; +                            &quot;---ResponseHeader=%s\n&quot; +                            &quot;---RequestUrl=%s\n&quot; +                            &quot;---RequestBody=%s&quot;,                    request.httpMethod(),                    methodKey,                    response.status(),                    response.reason(),                    result.contentUTF8(),                    objectMapper.writeValueAsString(response.headers()),                    request.url(),                    request.body() == null ? &quot;null&quot; : new String(request.body(), StandardCharsets.UTF_8)));        &#125; catch (JsonProcessingException e) &#123;            e.printStackTrace();        &#125;        return result;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> feign </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> http client </tag>
            
            <tag> feign </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feign Client - Client builder example</title>
      <link href="/2020/01/Java/FeignClient/Feign_BuilderExample/"/>
      <url>/2020/01/Java/FeignClient/Feign_BuilderExample/</url>
      
        <content type="html"><![CDATA[<h1 id="Tu-build-FeignClient-khong-su-dung-annotation"><a href="#Tu-build-FeignClient-khong-su-dung-annotation" class="headerlink" title="Tự build FeignClient không sử dụng annotation"></a>Tự build FeignClient không sử dụng annotation</h1><ul><li>Được sử dụng trong case không muốn sử dụng chung các bean đã config sẵn cho FeignClient</li><li>Sử dụng cách này thì tại class interface client, không cần khai báo annotation <code>@FeignClient</code></li></ul><h2 id="1-Code-example"><a href="#1-Code-example" class="headerlink" title="1. Code example"></a>1. Code example</h2><ul><li>Config</li></ul><pre><code class="java">@Componentpublic class DemoFeignClientConfig &#123;    private static final List&lt;String&gt; profilesDev = Arrays.asList(&quot;local&quot;, &quot;dev&quot;, &quot;dev2&quot;);    private final ObjectMapper json;    private final Environment env;    @Value(&quot;$&#123;authen.key&#125;&quot;)    private String SEAuthorization;    @Value(&quot;$&#123;endpoint:http://192.168.0.42:8000&#125;&quot;)    private String endpoint;    @Autowired    public DemoFeignClientConfig(@Qualifier(&quot;json_main&quot;) ObjectMapper json, Environment env) &#123;        this.json = json;        this.env = env;    &#125;    @Bean    public YourClient yourclient() &#123;        return Feign.builder()                .encoder(new JacksonEncoder(json))                .decoder(new JacksonDecoder(json))                .logger(new Slf4jLogger(&quot;service&quot;))                .logLevel(feignLoggerLevel())                .requestInterceptor(template -&gt; &#123;                    template.header(&quot;Authorization&quot;, SEAuthorization);                    template.header(&quot;Content-Type&quot;, &quot;application/json&quot;);                    template.header(&quot;Application-Name&quot;, &quot;Your app name&quot;);                &#125;)                .contract(new SpringMvcContract())                .target(YourClient.class, endpoint);    &#125;    private Logger.Level feignLoggerLevel() &#123;        var profiles = env.getActiveProfiles();        if (profiles.length == 0) &#123;            return Logger.Level.NONE;        &#125; else &#123;            for (var profile : profiles) &#123;                if (profilesDev.contains(profile)) &#123;                    return Logger.Level.FULL;                &#125;            &#125;        &#125;        return Logger.Level.BASIC;    &#125;&#125;</code></pre><ul><li><code>YourClient</code> class</li></ul><pre><code class="java">public interface YourClient &#123;    @RequestMapping(value = &quot;/api/banks&quot;, method = GET)    BanksResponse getListBank();    @RequestMapping(value = &quot;/api/bank_branches&quot;, method = GET)    BankBranchesResponse getListBankBranch(@RequestParam(value = &quot;bankCode&quot;, required = false) String bankCode);&#125;</code></pre><ul><li>Sử dụng</li></ul><pre><code class="java">@Autowireprivate  YourClient yourclient;</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> feign </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> http client </tag>
            
            <tag> feign </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feign Client - Multiple Configuration</title>
      <link href="/2020/01/Java/FeignClient/Feign_MultipleConfig/"/>
      <url>/2020/01/Java/FeignClient/Feign_MultipleConfig/</url>
      
        <content type="html"><![CDATA[<h1 id="Multiple-Configurations-for-Feign-Clients"><a href="#Multiple-Configurations-for-Feign-Clients" class="headerlink" title="Multiple Configurations for Feign Clients"></a>Multiple Configurations for Feign Clients</h1><p>Mục đích khi có nhiều FeignClient, và mỗi client có 1 config riêng biệt.</p><ul><li>Tham khảo: <a href="https://medium.com/@shokri4971/multiple-configurations-for-feign-clients-aeaacb8f0edd">https://medium.com/@shokri4971/multiple-configurations-for-feign-clients-aeaacb8f0edd</a></li></ul><pre><code class="java">public class BarConfig &#123;    @Bean    public BarRequestInterceptor barRequestInterceptor() &#123;        return new BarRequestInterceptor();    &#125;&#125;public class FooConfig &#123;    @Bean    public FooRequestInterceptor fooRequestInterceptor() &#123;        return new FooRequestInterceptor();    &#125;&#125;</code></pre><h2 id="Khai-bao-RequestTemplate-cai-nay-quan-trong-nhat"><a href="#Khai-bao-RequestTemplate-cai-nay-quan-trong-nhat" class="headerlink" title="Khai báo RequestTemplate (cái này quan trọng nhất)"></a>Khai báo RequestTemplate (cái này quan trọng nhất)</h2><pre><code class="java">public class BarRequestInterceptor implements RequestInterceptor &#123;    private static final Logger LOGGER = LoggerFactory.getLogger(BarRequestInterceptor.class);    @Override    public void apply(RequestTemplate template) &#123;        template.header(&quot;authorization&quot;, &quot;auth-bar&quot;);        LOGGER.info(&quot;bar authentication applied&quot;);    &#125;&#125;public class FooRequestInterceptor implements RequestInterceptor &#123;    private static final Logger LOGGER = LoggerFactory.getLogger(FooRequestInterceptor.class);    @Override    public void apply(RequestTemplate template) &#123;        template.header(&quot;authorization&quot;, &quot;auth-foo&quot;);        LOGGER.info(&quot;foo authentication applied&quot;);    &#125;&#125;</code></pre><h2 id="Su-dung"><a href="#Su-dung" class="headerlink" title="Sử dụng"></a>Sử dụng</h2><pre><code class="java">@FeignClient(contextId = &quot;fooContextId&quot;, value = &quot;fooValue&quot;, url = &quot;http://foo-server.com/services&quot;, configuration = FooConfig.class)public interface FooFeignClient &#123;    @GetMapping(&quot;&#123;id&#125;/foo&quot;)    void getFoo(@PathVariable(&quot;id&quot;) Long id);&#125;</code></pre><pre><code class="java">@FeignClient(contextId = &quot;barContextId&quot;, value = &quot;barValue&quot;, url = &quot;http://bar-server.com/services&quot;, configuration = BarConfig.class)public interface BarFeignClient &#123;    @GetMapping(&quot;&#123;id&#125;/bar&quot;)    void getBar(@PathVariable(&quot;id&quot;) Long id);&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> feign </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> http client </tag>
            
            <tag> feign </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka - CLI Command</title>
      <link href="/2020/01/Kafka/Kafka_Command/"/>
      <url>/2020/01/Kafka/Kafka_Command/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Install"><a href="#1-Install" class="headerlink" title="1. Install"></a>1. Install</h2><h3 id="1-1-Thu-cong-Linux"><a href="#1-1-Thu-cong-Linux" class="headerlink" title="1.1 Thủ công Linux"></a>1.1 Thủ công Linux</h3><pre><code class="bash"># 1. Downloadwget http://mirror.downloadvn.com/apache/kafka/2.4.0/kafka_2.13-2.4.0.tgz# 2. Giải nén tar zxvf# 3. Gán giá trị PATH biến môi trường tới đường dẫn /bin của kafkaexport PATH=/home/vagrant/kafka_2.13-2.4.0/bin:$PATH# 4. Chạy zookeeper # sửa file config/zookeeper.propertie nếu cần thiết (ví dụ: thay đổi đường dẫn chứa data)zookeeper-server-start.sh config/producer.properties# 5. Chạy broker server (bootstrap)# sửa file config/server.properties để kafka client có thể access vào đượcadvertised.listeners=PLAINTEXT://192.168.60.4:9092# thay 192.168.60.4 thành địa chỉ IP của dải mạng dùng để access # có thể thay đổi các cấu hình khác, nếu cần (ví dụ thay đổi đường dẫn chứa log tại log.dirs=)# Run broker serverkafka-server-start.sh config/server.properties</code></pre><ul><li>Quick run by nohub</li></ul><pre><code class="bash">echo &quot;run zookeeper&quot;nohup bash zookeeper-server-start.sh config/zookeeper.properties &gt;&gt; /tmp/zookeeper.log 2&gt;&amp;1&amp;echo &quot;run bootstrap server / broker server&quot;nohup kafka-server-start.sh config/server.properties &gt;&gt; /tmp/kafkaserver.log 2&gt;&amp;1&amp;</code></pre><h3 id="1-2-Docker"><a href="#1-2-Docker" class="headerlink" title="1.2 Docker"></a>1.2 Docker</h3><pre><code class="bash"># docker hub: https://hub.docker.com/r/bitnami/kafka# 1. create networkdocker network create app-tier --driver bridge# 2. install zookeeperdocker run --name zookeeper-server -d \    --network app-tier \    -e ALLOW_ANONYMOUS_LOGIN=yes \    -p 2181:2181 \    bitnami/zookeeper:latest# 3. Install broker/boostrap# Lưu ý KAFKA_CFG_ADVERTISED_LISTENERSdocker run --name kafka-server -d \    --network app-tier \    -e ALLOW_PLAINTEXT_LISTENER=yes \    -e KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181 \    -e KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://tungexplorer.me:19092 \    -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092 \    -p 19092:9092 \    bitnami/kafka:latest</code></pre><h2 id="2-Topic"><a href="#2-Topic" class="headerlink" title="2.Topic"></a>2.Topic</h2><pre><code class="bash"># Create topickafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --create --partitions 3 --replication-factor 1# first_topic = tên của topic, tên này là unique, identity# --partitions 3  = chỉnh số partitions, không nhập sẽ báo lỗi, số partitions là tùy ý. # --replication-factor 1 = chỉnh số replication , không nhập sẽ báo lỗi, nếu chỉ chạy 1 broker thì nên nhập 1# Listkafka-topics.sh --zookeeper 127.0.0.1:2181 --list# Detailkafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --describe# Deletekafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --delete</code></pre><h2 id="3-Producer"><a href="#3-Producer" class="headerlink" title="3. Producer"></a>3. Producer</h2><pre><code class="bash"># Vào console producerkafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first_topic# sau khi vào mode, có thể gõ bất kỳ message nào, cứ enter 1 lần, thì message sẽ được gửi đi. # Truyền propertykafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first_topic --producer-property acks=all# Trường hợp nhập tên 1 topic mới, mà không được tạo trước đó, thì sẽ tự động tạo tạo topic vừa nhập (có WARN cảnh báo). # Topic vừa nhập, có các thông số như partition, replication được chỉnh default như ở trong file config/server.properties</code></pre><h2 id="4-Consumer"><a href="#4-Consumer" class="headerlink" title="4. Consumer"></a>4. Consumer</h2><pre><code class="bash"># Vào console consumerkafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic# Như command trên, không nhập groupID, thì cli sẽ tự động tạo ra 1 groupId mới, unique# Để show ra các message từ lúc begin (chưa được &quot;mark as read&quot;), thì truyền thêm param --from-beginningkafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --from-beginning# Để set groupId cho consumer kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-first-application# trường hợp các consumer cùng chung group, thì các consumer sẽ được nhận message từ topic 1 cách lần lượt, (round robin)# trường hợp các consumer khác group, thì với mỗi 1 message, tất cả các consumer đều nhận được message như nhau.</code></pre><h2 id="5-Resetting-Offset"><a href="#5-Resetting-Offset" class="headerlink" title="5. Resetting Offset"></a>5. Resetting Offset</h2><pre><code class="bash"># khi consumer đã nhận được message, thì offset trên mỗi partition được &quot;mark as read&quot; sẽ thay đổi (offset lên giá trị gần nhất)# Sử dụng reset offset, để khi consumer load lại, có thể call lại các message từ offset chưa được &quot;mark&quot;kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest --execute --topic first_topic# còn các option khác như:# --to-datetime# --by-period# --to-earliest# --to-latest# --shift-by# --from-file# --to-current</code></pre><h1 id="Kafka-Tool"><a href="#Kafka-Tool" class="headerlink" title="Kafka Tool"></a>Kafka Tool</h1><ul><li>Kafka không cung cấp WEBUI đi kèm (rabbitmq có cung cấp webui đi kèm)</li><li>Để truy xuất tới kafka, có thể dùng tool Kafka tool (có UI)<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/kafka_tool_1.JPG" alt="Kafkatool"></li></ul>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> command </tag>
            
            <tag> kafka </tag>
            
            <tag> kafka cli </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka - Use case</title>
      <link href="/2020/01/Kafka/Kafka_UseCase/"/>
      <url>/2020/01/Kafka/Kafka_UseCase/</url>
      
        <content type="html"><![CDATA[<h1 id="Kafka-Use-case"><a href="#Kafka-Use-case" class="headerlink" title="Kafka Use case"></a>Kafka Use case</h1><h2 id="1-Video-Analytics-MovieFlix"><a href="#1-Video-Analytics-MovieFlix" class="headerlink" title="1. Video Analytics - MovieFlix"></a>1. Video Analytics - MovieFlix</h2><ul><li>MovieFlix is a company that allows you to watch TV Shows and Movies on demand. The business wants the following<br>capabilities</li><li>Make sure the user can resume the video where they left it off</li><li>Build a user profile in real time</li><li>Recommend the next show to the user in real time</li><li>Store all the data in analytics store<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_usecase/movieflix.JPG" alt="MovieFlix"></li><li><code>show_position</code> topic:<ul><li>is a topic that can have multiple producers</li><li>should be highly distributed if high volume &gt; 30 partitions</li><li>if I were to choose a key, I would choose “user_id”</li></ul></li><li><code>recommendations</code> topic:<ul><li>the kafka streams recommendation engine may source data from the analytical store for historical training</li><li>may be a low volume topic</li><li>if I were to choose a key, I would choose <code>user_id</code></li></ul></li></ul><h2 id="2-IOT-Example-GetTaxi"><a href="#2-IOT-Example-GetTaxi" class="headerlink" title="2. IOT Example - GetTaxi"></a>2. IOT Example - GetTaxi</h2><ul><li>GetTaxi is a company that allows people to match with taxi drivers on demand, right-away. The business wants the<br>following capabilities:</li><li>The user should match with a close by driver</li><li>The pricing should <code>surge</code> if the number of drivers are low or the number of user is high</li><li>all the position data before and during the ride should be stored in an analytics store so that the cost can be<br>computed accurately<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_usecase/gettaxi.JPG" alt="gettaxi"></li><li><code>taxi_position, user_position</code> topics:<ul><li>are topics that can have multiple producers</li><li>should be highly distributed if high volume &gt; 30 partitions</li><li>if i were to choose a key, i would choose “user_id”,”taxi_id”</li><li>Data is ephemeral and probably doesn’t need to be kept for a long time</li></ul></li><li><code>surge_pricing</code> topic:<ul><li>the computation of surge pricing comes from the Kafka Streams application</li><li>surge pricing may be regional and therefore that topic may be high volume</li><li>other topics such as “weather” or “events” etc. can be included in the Kafka Streams application</li></ul></li></ul><h2 id="3-CQRS-MySocialMedia"><a href="#3-CQRS-MySocialMedia" class="headerlink" title="3. CQRS - MySocialMedia"></a>3. CQRS - MySocialMedia</h2><ul><li>MySocialMedia is a company that allows you people to post images and others to react by using “likes” and “comments”.<br>The business wants the following capabilities:</li><li>users should be able to post, like and comment</li><li>users should see the total number of likes and comments per post in real time</li><li>high volume of data is expected on the first day of launch</li><li>users should be able to see “trending” posts<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_usecase/cqrs.JPG" alt="cqrs"></li><li>responsibilites are <code>segregated</code> hence we can call the model CQRS (Command Query Responsibility Segregation)</li><li>Posts<ul><li>are topics that can have multiple producers</li><li>should be highly distributed if high volume &gt; 30 partitions</li><li>if i were to choose a key, i would choose “user_id”</li><li>we probably want a high retention period of data for this topic</li></ul></li><li>Likes, Comments<ul><li>are topics with multiple producers</li><li>should be highly distributed as the volume of data is expected to be much greater</li><li>if i were to choose a key, i would choose “post_id”</li></ul></li><li>The data itself in Kafka should be formatted as <code>events</code><ul><li>User_123 created a post_id 456 at 2 pm</li><li>User_234 liked post_id 456 at 3pm</li><li>User_123 deleted a post_id 456 at 6pm</li></ul></li></ul><h2 id="4-Finance-application-MyBank"><a href="#4-Finance-application-MyBank" class="headerlink" title="4. Finance application - MyBank"></a>4. Finance application - MyBank</h2><ul><li>MyBank is a company that allows real-time banking for its users. It wants to deploy a brand-new capability to alert user<br>in case of large transactions.</li><li>the transaction data already exists in a database</li><li>thresholds can be defined by the users</li><li>alerts must be sent in real time to the users<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_usecase/myBank.JPG" alt="MyBank"></li><li>Bank Transactions topics:<ul><li>Kafka Connect Source is a great way to expose data from existing database</li><li>there are tons of CDC (change data capture) connectors for technologies such as PostgreSQL, Oracle, MySQL,<br>SQLServer, MongoDB etc…</li></ul></li><li>Kafka Stream application:<ul><li>When a user changes their settings, alerts won’t be triggered for past transactions</li></ul></li><li>User thresholds topics:<ul><li>it is better to send <code>events</code> to the topic (User 123 enabled threshold at $1000 at 12pm on July 12th 2018)</li><li>Then sending the state of the user: (User 123: threshold $1000)</li></ul></li></ul><h2 id="5-BigData-Ingestion"><a href="#5-BigData-Ingestion" class="headerlink" title="5. BigData Ingestion"></a>5. BigData Ingestion</h2><ul><li>It is common to have <code>generic</code> connectors or solutions to offload data from Kafka to HDFS, Amazon S3, and<br>ElasticSearch for example</li><li>It is also every common to have Kafka serve a <code>speed layer</code> for real time applications, while having a <code>slow layer</code><br>which helps with data ingestion in to stores for later analytics</li><li>Kafka as a front to BigData Ingestion is a common pattern in Big Data to provide an <code>ingestion buffer</code> in front of<br>some stores<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_usecase/bigdata.JPG" alt="Bigdata"></li></ul><h2 id="6-Logging-Metrics-Aggregation"><a href="#6-Logging-Metrics-Aggregation" class="headerlink" title="6. Logging &amp; Metrics Aggregation"></a>6. Logging &amp; Metrics Aggregation</h2><ul><li><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_usecase/logging.JPG" alt="Logging"></li></ul>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
            <tag> use case </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ops - Vagrant Template</title>
      <link href="/2020/01/Ops/VagrantFile_Template/"/>
      <url>/2020/01/Ops/VagrantFile_Template/</url>
      
        <content type="html"><![CDATA[<h1 id="File-template-vagrant"><a href="#File-template-vagrant" class="headerlink" title="File template vagrant"></a>File template vagrant</h1><p>paste to VagrantFile</p><h2 id="1-Tao-1-server"><a href="#1-Tao-1-server" class="headerlink" title="1. Tạo 1 server"></a>1. Tạo 1 server</h2><pre><code class="shell"># -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(&quot;2&quot;) do |config|  config.vm.box = &quot;bento/ubuntu-16.04&quot;  config.ssh.insert_key = false  config.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;, id: &quot;vagrant&quot;  config.vm.synced_folder &quot;./apps&quot;, &quot;/home/vagrant/apps&quot;, id: &quot;apps&quot;  config.vm.provider :virtualbox do |v|    v.memory = 2048    v.linked_clone = true    v.cpus = 2  end  config.vm.define &quot;app&quot; do |app|    app.vm.hostname = &quot;app.test&quot;    app.vm.network :private_network, ip: &quot;192.168.60.4&quot;  endend</code></pre><ul><li>config.vm.box : thay bằng image của os muốn tạo</li><li>config.ssh.insert_key : trường hợp muốn tạo ssh publickey sau khi OS được tạo xong, nếu điền false, sẽ không cần path<br>tới file public key</li><li>config.vm.synced_folder : đồng bộ data giữa máy chủ thật, và server ảo được tạo</li><li>id : định danh trong vagrant cho image, mỗi 1 id sẽ được cung cấp bởi duy nhất 1 provider (virtualbox, hyperv, kvm)</li><li>config.vm.provider :virtualbox : lựa chọn provider là virtualbox</li><li>v.memory : khai báo RAM cho máy ảo</li><li>v.cpus : khai báo nhân CPU cho máy ảo</li><li>config.vm.define : khai báo tên của image (1 id có thể có nhiều tên). Có thể sử dụng tên này để ssh tới máy ảo.<br>Ex <code>vagrant ssh app</code></li><li>app.vm.hostname : hostname sau khi máy ảo được tạo</li><li>app.vm.network :private_network, ip : địa chỉ IP máy ảo</li></ul><h2 id="2-Tao-nhieu-server-cung-luc"><a href="#2-Tao-nhieu-server-cung-luc" class="headerlink" title="2. Tạo nhiều server cùng lúc"></a>2. Tạo nhiều server cùng lúc</h2><pre><code class="shell"># -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(&quot;2&quot;) do |config|  config.vm.box = &quot;bento/ubuntu-16.04&quot;  config.ssh.insert_key = false  config.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;, disabled: true  config.vm.provider :virtualbox do |v|    v.memory = 1024    v.linked_clone = true    v.cpus = 2  end  config.vm.define &quot;app1&quot; do |app|    app.vm.hostname = &quot;app1.test&quot;    app.vm.network :private_network, ip: &quot;192.168.60.4&quot;  end  config.vm.define &quot;app2&quot; do |app|    app.vm.hostname = &quot;app2.test&quot;    app.vm.network :private_network, ip: &quot;192.168.60.5&quot;  end  config.vm.define &quot;db&quot; do |db|    db.vm.hostname = &quot;db.test&quot;    db.vm.network :private_network, ip: &quot;192.168.60.6&quot;  endend</code></pre>]]></content>
      
      
      <categories>
          
          <category> cicd_ops </category>
          
      </categories>
      
      
        <tags>
            
            <tag> template </tag>
            
            <tag> vagrant </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka - Note</title>
      <link href="/2019/12/Kafka/Kafka_Note/"/>
      <url>/2019/12/Kafka/Kafka_Note/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Topics-partitions-and-offsets"><a href="#1-Topics-partitions-and-offsets" class="headerlink" title="1. Topics, partitions and offsets"></a>1. Topics, partitions and offsets</h2><ul><li><code>Topics</code>: a particular stream of data<ul><li>Similar to a table in a database (without all the constraints)</li><li>You can have as many topics as you want</li><li>A topic is identified by its <code>name</code></li></ul></li><li>Topics are split in <code>partitions</code><ul><li>Each partition is ordered</li><li>Each message within a partition gets an incremental ids, called <code>offset</code></li></ul></li><li><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/topics_1.JPG" alt="Kafka partitions"></li><li>Offset only have a meaning for a specific partition<ul><li>E.g. offset 3 in partition 0 doesn’t represent the same data as offset 3 in partition I</li></ul></li><li>Order is guaranteed only within a partition (not across partitions)</li><li>Data is kept only for a limited time (default is one week)</li><li>Once the data is written to a partition, <code>it can&#39;t be changed</code> (immutability)</li><li>Data is assigned randomly to a partition unless a key is provided (more on this later)</li><li>Thư mục chứa các partitions ở tại ..&#x2F;data&#x2F;kafka</li></ul><h2 id="2-Brokers"><a href="#2-Brokers" class="headerlink" title="2. Brokers"></a>2. Brokers</h2><ul><li>A Kafka cluster is composed of multiple brokers (servers)</li><li>Each broker is identified with its ID (integer)</li><li>Each broker contains certain topic partitions</li><li>After connecting to any broker (called a <code>bootstrap broker</code>), you will be connected to the entire cluster</li><li>A good number to get started is 3 brokers, but some big clusters have over 100 brokers<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/btoker_1.JPG" alt="Broker1"></li><li>Note: data is distributed and Broker 103 doesn’t have any Topic B data</li></ul><h2 id="3-Topic-replication-factor"><a href="#3-Topic-replication-factor" class="headerlink" title="3. Topic replication factor"></a>3. Topic replication factor</h2><ul><li>Topics should have a replication factor &gt; I (usually between 2 and 3)</li><li>This way if a broker is down, another broker can serve the data</li><li>Example: Topic-A with 2 partitions and replication factor of 2<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/topic_replication_factor_2.JPG" alt="Topic_replication_factor1"></li><li><code>At any time only ONE broker can be a leader for a given partition</code></li><li><code>Only that leader can receive and serve data for a partition</code></li><li>The other brokers will synchronize the data</li><li>Therefore, each partition has one leader and multiple ISR (in-sync replica)</li></ul><h2 id="4-Producers"><a href="#4-Producers" class="headerlink" title="4. Producers"></a>4. Producers</h2><ul><li>Producers write data to topics (which is made of partitions)</li><li>Producers automatically know to which broker and partition to write to</li><li>In case of Broker failures, Producers will automatically recover<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/producer_1.JPG" alt="Producer1"></li><li>Producers can choose to receive acknowledgment of data writes:<ul><li><code>acks=0</code>: Producer won’t wait for acknowledgment (possible data loss)</li><li><code>acks=l</code>: Producer will wait for leader acknowledgment (limited data loss)</li><li><code>acks=all</code>: Leader + replicas acknowledgment (no data loss)</li></ul></li><li>Producers can choose to send a <code>key</code> with the message (string, number, etc…)</li><li>If key&#x3D;null, data is sent round ro bin (broker 101 then 102 then 103…)</li><li>If a key is sent, then all messages for that key will always go to the same partition</li><li>A key is basically sent if you need message ordering for a specific field (ex: truck_id)<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/producer_message_key_1.JPG" alt="ProducerMessageKey1"></li></ul><h2 id="5-Consumers-Consumer-Groups"><a href="#5-Consumers-Consumer-Groups" class="headerlink" title="5. Consumers &amp; Consumer Groups"></a>5. Consumers &amp; Consumer Groups</h2><ul><li>Consumers read data from a topic (identified by name)</li><li>Consumers know which broker to read from</li><li>In case of broker failures, consumers know how to recover</li><li>Data is read in order <code>within each partitions</code><br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/btoker_1.JPG" alt="Consumer1"></li><li>Consumers read data in consumer groups</li><li>Each consumer within a group reads from exclusive partitions</li><li>If you have more consumers than partitions, some consumers will be inactive<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/consumer_2.JPG" alt="Consumer2"></li><li>If you have more consumers than partitions, some consumers will be inactive<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/consumer_3.JPG" alt="Consumer3"></li></ul><h2 id="6-Consumer-Offsets"><a href="#6-Consumer-Offsets" class="headerlink" title="6. Consumer Offsets"></a>6. Consumer Offsets</h2><ul><li>Kafka stores the offsets at which a consumer group has been reading</li><li>The offsets committed live in a Kafka topic named <code>__consumer_offsets</code></li><li>When a consumer in a group has processed data received from Kafka, it should be committing the offsets</li><li>If a consumer dies, it will be able to read back from where it left off thanks to the committed consumer offsets!<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/offset_1.JPG" alt="Offset1"></li><li>Consumers choose when to commit offsets.</li><li>There are 3 delivery semantics:<ul><li>At most once:<ul><li>offsets are committed as soon as the message is received.</li><li>If the processing goes wrong, the message will be lost (it won’t be read again)</li></ul></li><li>At least once (usually preferred):<ul><li>offsets are committed after the message is processed.</li><li>if the processing goes wrong, the message will be read again</li><li>This can result it duplicate processing of messages. Make sure your processing is <code>idempotent</code> (i.e.<br>processing again the messages won’t impact your systems)</li></ul></li><li>Exactly once:<ul><li>can be achieved for Kafka &#x3D;&gt; Kafka workflows using Kafka Streams API</li><li>for Kafka &#x3D;&gt; External System workflows, use an idempotent consumer.</li></ul></li></ul></li></ul><h2 id="7-Kafka-Broker-Discovery"><a href="#7-Kafka-Broker-Discovery" class="headerlink" title="7. Kafka Broker Discovery"></a>7. Kafka Broker Discovery</h2><ul><li>Every Kafka broker is also called a <code>bootstrap server</code></li><li>That means that <code>you only need to connect to one broker</code>, and you will be connected to the entire cluster.</li><li>Each broker knows about all brokers, topics and partitions (metadata)<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/broker_discovery_1.JPG" alt="Discovery1"></li></ul><h2 id="8-Zookeeper"><a href="#8-Zookeeper" class="headerlink" title="8. Zookeeper"></a>8. Zookeeper</h2><ul><li>Zookeeper manages brokers (keeps a list of them)</li><li>Zookeeper helps in performing leader election for partitions.</li><li>Zookeeper sends notifications to Kafka in case of changes (e.g. new topic, broker dies, broker comes up, delete<br>topics, etc…)</li><li><code>Kafka can&#39;t work without Zookeepr</code></li><li>Zookeeper by design operates with an odd number of servers (3,5,7)</li><li>Zookeeper has a leader (handle writes) the rest of the servers are followers (handle reads)</li><li>(Zookeepr does NOT store consumer offsets with Kafka &gt; v0.10)<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/zookeeper_1.JPG" alt="Zookeeper1"></li></ul><h2 id="9-Kafka-Guarantees"><a href="#9-Kafka-Guarantees" class="headerlink" title="9. Kafka Guarantees"></a>9. Kafka Guarantees</h2><ul><li>Messages are appended to a topic-partition in the order they are sent</li><li>Consumers read messages in the order stored in a topic-partition</li><li>With a replication factor of N, producers and consumers can tolerate up to N-I brokers being down</li><li>This is why a replication factor of 3 is a good idea:<ul><li>Allows for one broker to be taken down for maintenance</li><li>Allows for another broker to be taken down unexpectedly</li></ul></li><li>As long as the number of partitions remains constant for a topic (no new partitions), the same key will always go to<br>the same partition.</li></ul><h2 id="10-Theory-Roundup"><a href="#10-Theory-Roundup" class="headerlink" title="10. Theory Roundup"></a>10. Theory Roundup</h2><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/broker_discovery_1.JPG" alt="TheoryRoundUp1"></p><pre><code>ref: Udemy - Apache Kafka Series - Learn Apache Kafka for Beginners v2</code></pre><h1 id="Kafka-Segment-…"><a href="#Kafka-Segment-…" class="headerlink" title="Kafka - Segment &amp; …"></a>Kafka - Segment &amp; …</h1><h2 id="1-Partitions-and-Segments"><a href="#1-Partitions-and-Segments" class="headerlink" title="1. Partitions and Segments"></a>1. Partitions and Segments</h2><ul><li>Topics are made of partitions</li><li>Partitions are made of segments (files)<br><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/segment2.JPG" alt="Segment"></li><li>Only one segment is <code>ACTIVE</code> (the one data is being written to)</li><li>Two segment settings:<ul><li>log.segment.bytes: the max size of a single segment in bytes</li><li>log.segment.ms: the time Kafka will wait before committing t he segment if not full</li></ul></li><li>Segments come with two indexes (files):<ul><li>An offset to position index: allows Kafka where to read to find a message</li><li>A timestamp to offset index: allows Kafka to find messages with a timestamp</li></ul></li><li>Therefore, Kafka knows where to find data in a constant time</li></ul><h2 id="2-unclean-leader-election"><a href="#2-unclean-leader-election" class="headerlink" title="2. unclean.leader.election"></a>2. unclean.leader.election</h2><ul><li>if all your In Sync Replicas die (but you still have out of sync replicas up), you have the following option:<ul><li>Wait for an ISR to come back online (default)</li><li>Enable <code>unclean.leader.election=true</code> and start producing to non ISR partitions</li></ul></li><li>if you enable <code>unclean.leader.election=true</code>, you improve availability, but you will lose data because other messages<br>on ISR will be discarded.</li><li>Overall this is a very dangerous setting and its implication must be understood fully before enabling it</li><li>Use cases include: metrics collection, log collection, and other cases where data loss is somewhat acceptable, at the<br>trade-off of availability</li></ul><h2 id="3-min-insync-replicas"><a href="#3-min-insync-replicas" class="headerlink" title="3. min.insync.replicas"></a>3. min.insync.replicas</h2><ul><li>Acks&#x3D;all must be used in conjunction with <code>min.insync.replicas</code></li><li><code>min.insync.replicas</code> can be set at the broker or topic level (override).</li><li><code>min.insync.replicas=2</code> implies that at least 2 brokers that are ISR (including leader) must respond that they have<br>the data</li><li>That means if you use <code>replication.factor=3, min.insync=2, acks=all</code>, you can only tolerate I broker going down,<br>otherwise the producer will receive an exception on send.</li></ul><h2 id="4-Advertised-Host-Setting"><a href="#4-Advertised-Host-Setting" class="headerlink" title="4. Advertised Host Setting"></a>4. Advertised Host Setting</h2><ul><li><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/kafka_note/advertised.JPG" alt="Advertised"></li></ul><hr><ul><li>khi set <code>enable.auto.commit</code> &#x3D; false, thì <code>auto.commit.interval.ms</code> không được xét tới</li><li>Tham khảo các properties: <a href="https://jaceklaskowski.gitbooks.io/apache-kafka/kafka-properties.html">https://jaceklaskowski.gitbooks.io/apache-kafka/kafka-properties.html</a><br>&#x2F;&#x2F; end</li></ul>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
            <tag> segment </tag>
            
            <tag> total </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Code Template - AWS - S3</title>
      <link href="/2019/12/CodeTemplate/CodeTemplate_S3/"/>
      <url>/2019/12/CodeTemplate/CodeTemplate_S3/</url>
      
        <content type="html"><![CDATA[<ul><li>Upload file có option set quyền public&#x2F;private</li><li>Get link download (signed URL)</li><li>Delete file</li></ul><h2 id="1-Basic"><a href="#1-Basic" class="headerlink" title="1. Basic"></a>1. Basic</h2><pre><code class="java">import com.amazonaws.AmazonClientException;import com.amazonaws.HttpMethod;import com.amazonaws.auth.AWSStaticCredentialsProvider;import com.amazonaws.auth.BasicAWSCredentials;import com.amazonaws.regions.Regions;import com.amazonaws.services.s3.AmazonS3;import com.amazonaws.services.s3.AmazonS3ClientBuilder;import com.amazonaws.services.s3.model.*;import com.tale.bootstrap.Bootstrap;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.ByteArrayInputStream;import java.net.URL;import java.util.Date;public class S3UploadUtils &#123;    public static final Logger LOG = LoggerFactory.getLogger(S3UploadUtils.class);    private static AmazonS3 s3Client()&#123;        final String accessKey = &quot;aws access key&quot;;        final String secretKey = &quot;aws secret key&quot;;        // return AmazonS3ClientBuilder.standard().build();        return AmazonS3ClientBuilder.standard().withCredentials(                new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey))).withRegion(Regions.AP_SOUTHEAST_1).build();    &#125;;    //    The time that download link has not expired    private static final int TIME_MINUTES_EXPIRED = 5;    /**     * Upload file to AWS S3     * isPrivate = true for private file. Ex: csv report     * isPrivate = false for public file: Ex: avatar user     *     * @param awsS3Bucket AWS Bucket     * @param fileName    File&#39;s name     * @param bytes       Bytes     * @param isPrivate   Private file     * @return Url of file     */    public static String uploadFile(String awsS3Bucket, String fileName, byte[] bytes, boolean isPrivate) &#123;        CannedAccessControlList cannedAccessControlList = isPrivate ? CannedAccessControlList.Private : CannedAccessControlList.PublicRead;        try &#123;            ObjectMetadata metadata = new ObjectMetadata();            metadata.setContentLength(bytes.length);            PutObjectRequest putObjectRequest = new PutObjectRequest(awsS3Bucket,                    fileName, new ByteArrayInputStream(bytes), metadata);            s3Client().putObject(putObjectRequest.withCannedAcl(cannedAccessControlList));            return s3Client().getUrl(putObjectRequest.getBucketName(), putObjectRequest.getKey()).toString();        &#125; catch (AmazonClientException ace) &#123;            LOG.error(ace.getMessage(), ace);            return null;        &#125;    &#125;    /**     * Upload public file to AWS S3     *     * @param awsS3Bucket AWS Bucket     * @param fileName    File&#39;s name     * @param bytes       Bytes     * @return Url of file     */    public static String uploadFile(String awsS3Bucket, String fileName, byte[] bytes) &#123;        return uploadFile(awsS3Bucket, fileName, bytes, false);    &#125;    /**     * get download link for private file. That need token to download     * bucketName same as folder name     * fileName is unique per bucketName     *     * @param bucketName     * @param fileName     * @return download link     */    public static String getDownloadLink(String bucketName, String fileName) &#123;        String downloadLink = &quot;&quot;;        if (bucketName.isEmpty() || fileName.isEmpty()) return downloadLink;        Date expiration = new Date();        long expTimeMillis = new Date().getTime();        expiration.setTime(expTimeMillis + TIME_MINUTES_EXPIRED * 1000 * 60);        try &#123;            GeneratePresignedUrlRequest generatePresignedUrlRequest =                    new GeneratePresignedUrlRequest(bucketName, fileName)                            .withMethod(HttpMethod.GET)                            .withExpiration(expiration);            URL url = s3Client().generatePresignedUrl(generatePresignedUrlRequest);            downloadLink = url.toString();        &#125; catch (AmazonClientException ace) &#123;            LOG.error(ace.getMessage(), ace);        &#125;        return downloadLink;    &#125;    /**     * Delete file on S3     *     * @param bucketName     * @param fileName     */    public static void deleteFile(String bucketName, String fileName) &#123;        try &#123;            s3Client().deleteObject(new DeleteObjectRequest(bucketName, fileName));        &#125; catch (AmazonClientException ace) &#123;            LOG.error(ace.getMessage(), ace);        &#125;    &#125;&#125;</code></pre><h2 id="2-Su-dung-SSE-C-de-encrypt-decrypt-object"><a href="#2-Su-dung-SSE-C-de-encrypt-decrypt-object" class="headerlink" title="2. Sử dụng SSE-C để encrypt&#x2F;decrypt object"></a>2. Sử dụng SSE-C để encrypt&#x2F;decrypt object</h2><ul><li>Với cách này, chỉ có client chứa “key” mới có thể download&#x2F;retrivew metadata của object được. Cho dù tài khoản root<br>của AWS có full quyền, cũng không thể can thiệp, nếu không có key.<br>ref: <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/sse-c-using-java-sdk.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/sse-c-using-java-sdk.html</a></li></ul><h3 id="Code-example"><a href="#Code-example" class="headerlink" title="Code example"></a>Code example</h3><pre><code class="java">package tung.demo.ssec_s3;import com.amazonaws.AmazonServiceException;import com.amazonaws.SdkClientException;import com.amazonaws.auth.AWSStaticCredentialsProvider;import com.amazonaws.auth.BasicAWSCredentials;import com.amazonaws.regions.Regions;import com.amazonaws.services.s3.AmazonS3;import com.amazonaws.services.s3.AmazonS3ClientBuilder;import com.amazonaws.services.s3.model.*;import javax.crypto.KeyGenerator;import java.io.BufferedReader;import java.io.File;import java.io.IOException;import java.io.InputStreamReader;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;public class ServerSideEncryptionUsingClientSideEncryptionKey &#123;    private static SSECustomerKey SSE_KEY;    private static AmazonS3 S3_CLIENT;    private static KeyGenerator KEY_GENERATOR;    public static void main(String[] args) throws IOException, NoSuchAlgorithmException &#123;        Regions clientRegion = Regions.AP_SOUTHEAST_1;        String accessKey = &quot;&quot;;        String secretKey = &quot;ga+tu8vB&quot;;        String bucketName = &quot;&quot;;        String keyName = &quot;test001.png&quot;;        String uploadFileName = &quot;D:\\yamaha.png&quot;;        String targetKeyName = &quot;*** Target key name ***&quot;;        // Create an encryption key.        KEY_GENERATOR = KeyGenerator.getInstance(&quot;AES&quot;);        SecureRandom secureRandom = SecureRandom.getInstance(&quot;SHA1PRNG&quot;);        secureRandom.setSeed(&quot;TUNGTUNGTUNG&quot;.getBytes());        KEY_GENERATOR.init(256, secureRandom);        SSE_KEY = new SSECustomerKey(KEY_GENERATOR.generateKey());        try &#123;            S3_CLIENT = AmazonS3ClientBuilder.standard()                    .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey)))                    .withRegion(clientRegion)                    .build();            // Upload an object.            uploadObject(bucketName, keyName, new File(uploadFileName));            // Download the object.//            downloadObject(bucketName, keyName);            // Verify that the object is properly encrypted by attempting to retrieve it            // using the encryption key.            retrieveObjectMetadata(bucketName, keyName);            // Copy the object into a new object that also uses SSE-C.//            copyObject(bucketName, keyName, targetKeyName);        &#125; catch (AmazonServiceException e) &#123;            // The call was transmitted successfully, but Amazon S3 couldn&#39;t process            // it, so it returned an error response.            e.printStackTrace();        &#125; catch (SdkClientException e) &#123;            // Amazon S3 couldn&#39;t be contacted for a response, or the client            // couldn&#39;t parse the response from Amazon S3.            e.printStackTrace();        &#125;    &#125;    private static void uploadObject(String bucketName, String keyName, File file) &#123;//        PutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName, file);        PutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName, file).withSSECustomerKey(SSE_KEY);        S3_CLIENT.putObject(putRequest);        System.out.println(&quot;Object uploaded&quot;);    &#125;    private static void downloadObject(String bucketName, String keyName) throws IOException &#123;        GetObjectRequest getObjectRequest = new GetObjectRequest(bucketName, keyName).withSSECustomerKey(SSE_KEY);        S3Object object = S3_CLIENT.getObject(getObjectRequest);        System.out.println(&quot;Object content: &quot;);        displayTextInputStream(object.getObjectContent());    &#125;    private static void retrieveObjectMetadata(String bucketName, String keyName) &#123;        GetObjectMetadataRequest getMetadataRequest = new GetObjectMetadataRequest(bucketName, keyName)                .withSSECustomerKey(SSE_KEY);        ObjectMetadata objectMetadata = S3_CLIENT.getObjectMetadata(getMetadataRequest);        System.out.println(&quot;Metadata retrieved. Object size: &quot; + objectMetadata.getContentLength());    &#125;    private static void copyObject(String bucketName, String keyName, String targetKeyName)            throws NoSuchAlgorithmException &#123;        // Create a new encryption key for target so that the target is saved using SSE-C.        SSECustomerKey newSSEKey = new SSECustomerKey(KEY_GENERATOR.generateKey());        CopyObjectRequest copyRequest = new CopyObjectRequest(bucketName, keyName, bucketName, targetKeyName)                .withSourceSSECustomerKey(SSE_KEY)                .withDestinationSSECustomerKey(newSSEKey);        S3_CLIENT.copyObject(copyRequest);        System.out.println(&quot;Object copied&quot;);    &#125;    private static void displayTextInputStream(S3ObjectInputStream input) throws IOException &#123;        // Read one line at a time from the input stream and display each line.        BufferedReader reader = new BufferedReader(new InputStreamReader(input));        String line;        while ((line = reader.readLine()) != null) &#123;            System.out.println(line);        &#125;        System.out.println();    &#125;&#125;</code></pre><h2 id="3-Note"><a href="#3-Note" class="headerlink" title="3. Note"></a>3. Note</h2><ul><li>Trường hợp OS chạy app java, nếu thỏa mãn 2 điều kiện sau, thì không cần phải khai báo access key, secret key trong<br>code cho hàm s3Client()<ul><li>OS là máy chủ AWS, có gán ROLE access tới S3</li><li>OS có cài tool aws-cli, và có config accesskey, secret key.</li></ul></li><li>Với file được upload lên với giới hạn truy cập private, trường hợp muốn download thì cần generator link có token, khái<br>niệm này là SIGNED URL. Và link có token sẽ chỉ valid trong 1 khoảng thời gian được cấu hình bởi biến<br>TIME_MINUTES_EXPIRED</li></ul><h2 id="4-CopyObject-su-dung-multi-thread-CompletableFuture"><a href="#4-CopyObject-su-dung-multi-thread-CompletableFuture" class="headerlink" title="4. CopyObject sử dụng multi-thread - CompletableFuture"></a>4. CopyObject sử dụng multi-thread - CompletableFuture</h2><p>&#x2F;&#x2F; S3 SDK không hỗ trợ copyObject cho nhiều object cùng lúc</p><pre><code class="java">import org.springframework.boot.CommandLineRunner;import org.springframework.stereotype.Component;import org.springframework.util.CollectionUtils;import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;import software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;import software.amazon.awssdk.regions.Region;import software.amazon.awssdk.services.s3.S3AsyncClient;import software.amazon.awssdk.services.s3.model.CopyObjectRequest;import software.amazon.awssdk.services.s3.model.CopyObjectResponse;import java.util.ArrayList;import java.util.List;import java.util.concurrent.CompletableFuture;import java.util.stream.Collectors;@Componentpublic class Test implements CommandLineRunner &#123;    private static final String accessKey = &quot;1234567&quot;;    private static final String secretKey = &quot;1345678/N0S8onuhk&quot;;    final private S3AsyncClient s3AsyncClient;    public Test() &#123;        this.s3AsyncClient = S3AsyncClient.builder()                .credentialsProvider(StaticCredentialsProvider.create(AwsBasicCredentials.create(accessKey, secretKey)))                .region(Region.AP_EAST_1)                .build();    &#125;    @Override    public void run(String... args) throws Exception &#123;        System.out.println(123);        List&lt;CompletableFuture&lt;CopyObjectResponse&gt;&gt; listFuture = new ArrayList&lt;&gt;();        for (int i = 1; i &lt; 100; i++) &#123;            CopyObjectRequest copyObjectRequest = CopyObjectRequest.builder()                    .copySource(&quot;tungtv202-testversion&quot; + &quot;/&quot; + &quot;vnpayqr4.png&quot;)                    .destinationBucket(&quot;tungexplorer.me&quot;)                    .destinationKey(i + &quot;.png&quot;)                    .build();            listFuture.add(s3AsyncClient.copyObject(copyObjectRequest));        &#125;        CompletableFuture&lt;Void&gt; allFutures = CompletableFuture                .allOf(listFuture.toArray(new CompletableFuture[listFuture.size()]));        CompletableFuture&lt;List&lt;CopyObjectResponse&gt;&gt; allPageContentsFuture = allFutures.thenApply(v -&gt; &#123;            return listFuture.stream().map(CompletableFuture::join)                    .collect(Collectors.toList());        &#125;)//                .whenComplete((res, ex) -&gt; &#123;//                    if (ex != null) &#123;//                        System.out.println(&quot;Oops! We have an exception - &quot; + ex.getMessage());//                    &#125; else &#123;//                        System.out.println(&quot;done&quot;);//                    &#125;//                &#125;)                .exceptionally(ex -&gt; &#123;                    System.out.println(&quot;Oops! We have an exception2 - &quot; + ex.getMessage());                    return null;                &#125;);        var resultList = allPageContentsFuture.get();        if (CollectionUtils.isEmpty(resultList)) return;        for (CopyObjectResponse e : resultList) &#123;            if (!&quot;OK&quot;.equalsIgnoreCase(e.sdkHttpResponse().statusText().get())) &#123;                System.out.println(&quot;Copy object error&quot;);            &#125;        &#125;        System.out.println(456);    &#125;&#125;</code></pre><p>&#x2F;&#x2F; Từ Java 9 trở đi CompleteTableFutre support delay giữa các async Executer</p><pre><code class="java">//  CompletableFuture.delayedExecutor(5, TimeUnit.SECONDS) private CompletableFuture&lt;ProductList&gt; clientAsyncGetProduct(Store store, int page, String createdOnMax) &#123;        return CompletableFuture.supplyAsync(() -&gt; productClient.filter(Utils.getxyzBaseUrl(store.getStoreAlias()), store.getAccessToken(), page, Constants.BACKUP_CLIENT_LIMIT                , Constants.BACKUP_CLIENT_SORT_DEFAULT, createdOnMax), CompletableFuture.delayedExecutor(5, TimeUnit.SECONDS));    &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> code_template </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> s3 </tag>
            
            <tag> code template </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS - Policy Evaluation Logic</title>
      <link href="/2019/12/AWS/AWS_PolicyEvaluationLogic/"/>
      <url>/2019/12/AWS/AWS_PolicyEvaluationLogic/</url>
      
        <content type="html"><![CDATA[<h1 id="Policy-Evaluation-Logic"><a href="#Policy-Evaluation-Logic" class="headerlink" title="Policy Evaluation Logic"></a>Policy Evaluation Logic</h1><ul><li>If a request is denied, it is definitively denied, regardless of any allow rule for the object&#x2F;bucket.</li><li>If a request is not denied, it does not necessarily mean it is allowed; explicit allow must be defined for it to be granted.</li><li>Reference: <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html">AWS IAM Policy Evaluation Logic</a></li><li>IAM Policy is like a driver’s license that permits you to drive a car; whether or not you are allowed to enter a certain place (e.g., an S3 bucket) depends on the policies set for that place.</li></ul><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/PolicyEvaluationHorizontal.png" alt="PolicyEvaluate"></p><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/aws/policy2.JPG" alt="Policy2"></p><ul><li>Cross-Account Policy Evaluation Logic has its own evaluation logic.<br>Reference: <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic-cross-account.html">Cross-Account Policy Evaluation Logic</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> evaluation logic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Jackson - Deserialize Abstract class</title>
      <link href="/2019/12/Java/JackSon/Jackson_Deserialize_Abstractclass/"/>
      <url>/2019/12/Java/JackSon/Jackson_Deserialize_Abstractclass/</url>
      
        <content type="html"><![CDATA[<h1 id="Su-dung-Jackson-de-deserialize-abstract-class"><a href="#Su-dung-Jackson-de-deserialize-abstract-class" class="headerlink" title="Sử dụng Jackson để deserialize abstract class"></a>Sử dụng Jackson để deserialize abstract class</h1><h1 id="Inheritance-with-Jackson"><a href="#Inheritance-with-Jackson" class="headerlink" title="Inheritance with Jackson"></a>Inheritance with Jackson</h1><p>Tham khảo: <a href="https://www.baeldung.com/jackson-inheritance">https://www.baeldung.com/jackson-inheritance</a></p><h2 id="1-Code-mau"><a href="#1-Code-mau" class="headerlink" title="1. Code mẫu"></a>1. Code mẫu</h2><h3 id="Abtract-class"><a href="#Abtract-class" class="headerlink" title="Abtract class"></a>Abtract class</h3><pre><code class="java">@Getter@Setter@JsonTypeInfo(        use = JsonTypeInfo.Id.NAME,        include = JsonTypeInfo.As.PROPERTY,        property = &quot;type2&quot;)@JsonSubTypes(&#123;        @JsonSubTypes.Type(value = Car.class, name = &quot;car&quot;),        @JsonSubTypes.Type(value = Truck.class, name = &quot;truck&quot;)&#125;)public abstract class Vehicle &#123;    private String make;    private String model;    protected Vehicle(String make, String model) &#123;        this.make = make;        this.model = model;    &#125;    public Vehicle() &#123;    &#125;&#125;</code></pre><h3 id="Child-class"><a href="#Child-class" class="headerlink" title="Child class"></a>Child class</h3><pre><code class="java">@Getter@Setterpublic class Car extends Vehicle &#123;    private int seatingCapacity;    private double topSpeed;    public Car(String make, String model, int seatingCapacity, double topSpeed) &#123;        super(make, model);        this.seatingCapacity = seatingCapacity;        this.topSpeed = topSpeed;    &#125;    public Car(String make, String model) &#123;        super(make, model);    &#125;    public Car()&#123;    &#125;&#125;</code></pre><pre><code class="java">@Getter@Setterpublic class Truck extends Vehicle &#123;    private double payloadCapacity;    public Truck(String make, String model, double payloadCapacity) &#123;        super(make, model);        this.payloadCapacity = payloadCapacity;    &#125;    public Truck()&#123;    &#125;&#125;</code></pre><h3 id="using-class"><a href="#using-class" class="headerlink" title="using class"></a>using class</h3><pre><code class="java">@Getter@Setterpublic class Fleet &#123;    //    private List&lt;Vehicle&gt; vehicles;    private Vehicle vehicle;&#125;</code></pre><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><pre><code class="java">public static void main(String[] args) throws IOException &#123;        ObjectMapper mapper = new ObjectMapper();//        mapper.enableDefaultTyping();        mapper.configure(SerializationFeature.INDENT_OUTPUT, true);        Car car = new Car(&quot;Mercedes-Benz&quot;, &quot;S500&quot;, 5, 250.0);        Truck truck = new Truck(&quot;Isuzu&quot;, &quot;NQR&quot;, 7500.0);//        List&lt;Vehicle&gt; vehicles = new ArrayList&lt;&gt;();//        vehicles.add(car);//        vehicles.add(truck);        Fleet serializedFleet = new Fleet();        serializedFleet.setVehicle(car);        String jsonDataString = mapper.writeValueAsString(serializedFleet);        System.out.println(jsonDataString);        Fleet fleet = mapper.readValue(jsonDataString, Fleet.class);        System.out.println(&quot;Car ? &quot; + (fleet.getVehicle() instanceof Car));        System.out.println(&quot;Truck ? &quot; + (fleet.getVehicle() instanceof Truck));    &#125;</code></pre><p>output:</p><pre><code class="json">&#123;  &quot;vehicle&quot; : &#123;    &quot;type2&quot; : &quot;car&quot;,    &quot;make&quot; : &quot;Mercedes-Benz&quot;,    &quot;model&quot; : &quot;S500&quot;,    &quot;seatingCapacity&quot; : 5,    &quot;topSpeed&quot; : 250.0  &#125;&#125;</code></pre><h2 id="2-Luu-y"><a href="#2-Luu-y" class="headerlink" title="2. Lưu ý"></a>2. Lưu ý</h2><ul><li>Trường hợp nếu CÓ khai báo <code>mapper.enableDefaultTyping();</code> thì class <code>Vehicle</code> không cần khai báo các <code>@JsonTypeInfo</code><br>và <code>@JsonSubTypes</code></li><li>Lưu ý khai báo <code>property = &quot;type2&quot;</code> trường này sẽ quyết định xác định class con nào được deserialize</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> jackson </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> serialize </tag>
            
            <tag> jackson </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Jackson - Note</title>
      <link href="/2019/12/Java/JackSon/Jackson/"/>
      <url>/2019/12/Java/JackSon/Jackson/</url>
      
        <content type="html"><![CDATA[<h1 id="Jackson"><a href="#Jackson" class="headerlink" title="Jackson"></a>Jackson</h1><pre><code>com.fasterxml.jackson.core</code></pre><p><img src="https://tungexplorer.s3.ap-southeast-1.amazonaws.com/jackson/JackSon1.JPG" alt="JackSon Architect"></p><h2 id="1-annotation"><a href="#1-annotation" class="headerlink" title="1. annotation"></a>1. annotation</h2><h3 id="1-1-JsonProperty"><a href="#1-1-JsonProperty" class="headerlink" title="1.1 @JsonProperty"></a>1.1 @JsonProperty</h3><pre><code class="java">    public class Employee &#123;        @JsonProperty(&quot;employee-name&quot;)        private String name;        @JsonProperty(&quot;employee-code&quot;)        private String code;    &#125;    // Code demo at: DemoJsonProperty.java</code></pre><ul><li>Trong cả case serialization và deserialization:<ul><li>A &#x3D; “name”,”code” &#x3D;&gt; A1 &#x3D; “employee-name”, “employee-code”</li><li>B &#x3D; “name”, B1 &#x3D; “employee-name”, “employee-code”</li></ul></li></ul><h3 id="1-2-JsonGetter-JsonSetter"><a href="#1-2-JsonGetter-JsonSetter" class="headerlink" title="1.2 @JsonGetter @JsonSetter"></a>1.2 @JsonGetter @JsonSetter</h3><pre><code class="java">    public  class Employee &#123;        private String name;        @JsonGetter(&quot;employee-name-1&quot;)        public String getName() &#123;            return name;        &#125;        @JsonSetter(&quot;employee-name-2&quot;)        public void setName(String name) &#123;            this.name = name;        &#125;    &#125;    // code demo at: DemoJsonSetterGetter.java</code></pre><ul><li>Trong case serialization<ul><li>A &#x3D; “name” &#x3D;&gt; A1&#x3D; “employee-name-1”</li></ul></li><li>Trong case deserialization<ul><li>B1 &#x3D; “employee-name-2” &#x3D;&gt; B &#x3D; “name”</li></ul></li><li>Trường hợp @JsonGetter và @JsonSetter có value bằng nhau, thì tương đương dùng @JsonProperty</li><li>Trường hợp deserialization lỗi do không tìm thấy properties, có thể cần phải config ObjectMapper</li></ul><pre><code class="java">    objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);</code></pre><h3 id="1-3-JsonIgnore"><a href="#1-3-JsonIgnore" class="headerlink" title="1.3 @JsonIgnore"></a>1.3 @JsonIgnore</h3><pre><code class="java">    public static class Employee &#123;        @JsonIgnore        private String name;        @JsonProperty(&quot;employee-code&quot;)        private String code;    &#125;    // code demo at DemoJsonIgnore.java</code></pre><ul><li>Case serialization<ul><li>A &#x3D; name, code<ul><li>A1 &#x3D; employee-code  (note: A1 không có employee-name, chứ không phải employee-code &#x3D; null)</li></ul></li><li>Case deserialization<ul><li>B1 &#x3D; name, employee-code</li><li>B &#x3D; name , code  (note: name &#x3D; null)</li></ul></li><li>Trường hợp lỗi deserialization</li></ul></li></ul><pre><code class="java">    objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);</code></pre><h3 id="1-4-JsonIgnoreProperties"><a href="#1-4-JsonIgnoreProperties" class="headerlink" title="1.4 @JsonIgnoreProperties"></a>1.4 @JsonIgnoreProperties</h3><pre><code class="java">    @JsonIgnoreProperties(value = &#123;&quot;name&quot;,&quot;code&quot;&#125;, ignoreUnknown = true)    public static class Employee &#123;            private String name;            private String code;            @JsonProperty(&quot;employee-address&quot;)            private String address;        &#125;           // demo code at DemoJsonIgnoreProperties.java</code></pre><ul><li>Logic serialization và deserialization giống với @JsonIgnore</li><li>Dùng trong trường hợp muốn ignore nhiều trường cùng lúc</li><li>Khai báo “ignoreUnknown &#x3D; true” tương đương với config ObjectMapper:</li></ul><pre><code>    (DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)</code></pre><h3 id="1-5-JsonIgnoreType"><a href="#1-5-JsonIgnoreType" class="headerlink" title="1.5 @JsonIgnoreType"></a>1.5 @JsonIgnoreType</h3><pre><code class="java">private static class Employee &#123;        private String name;        private String dept;        private Address address;    &#125;    @JsonIgnoreType    private static class Address &#123;        private String street;        private String city;    &#125;    // code demo at DemoJsonIgnoreType.java</code></pre><ul><li>Dùng để ignore, giống với @JsonIgnore, nhưng thay vì đặt trên method, thì đặt trên class. Bên trên tương đương với</li></ul><pre><code class="java">private static class Employee &#123;        private String name;        private String dept;        @JsonIgnore        private Address address;    &#125;    private static class Address &#123;        private String street;        private String city;    &#125;</code></pre><h3 id="1-6-JacksonInject"><a href="#1-6-JacksonInject" class="headerlink" title="1.6 @JacksonInject"></a>1.6 @JacksonInject</h3><pre><code class="java"> @JacksonInject(&quot;lastUpdated&quot;)  private LocalDateTime lastUpdated;//   .......//   .......//   ....... InjectableValues iv = new InjectableValues.Std();      ((InjectableValues.Std) iv).addValue(&quot;lastUpdated&quot;, LocalDateTime.now());</code></pre><ul><li>Dùng để set giá trị cho đặc tính, mà không cần thông qua append string vào B1, khi deserialization</li></ul><h3 id="1-7-JsonPropertyOrder"><a href="#1-7-JsonPropertyOrder" class="headerlink" title="1.7 @JsonPropertyOrder"></a>1.7 @JsonPropertyOrder</h3><pre><code class="java">@JsonPropertyOrder(&#123;&quot;name&quot;, &quot;phoneNumber&quot;,&quot;email&quot;, &quot;salary&quot;, &quot;id&quot; &#125;)public class Employee &#123;  private String id;  private String name;  private int salary;  private String phoneNumber;  private String email;    .............&#125;</code></pre><ul><li>order theo alphabet</li></ul><pre><code class="java">@JsonPropertyOrder(alphabetic = true)public class Employee2 &#123;  private String id;  private String name;  private int salary;  private String phoneNumber;  private String email;    .............&#125;</code></pre><h3 id="1-8-JsonAlias-Annotation"><a href="#1-8-JsonAlias-Annotation" class="headerlink" title="1.8 @JsonAlias Annotation"></a>1.8 @JsonAlias Annotation</h3><ul><li>Chỉ có ý nghĩa khi deserialization</li><li>1 properties có thể khai báo nhiều Alias (JsonProperties thì chỉ khai báo được 1 giá trị)</li><li>Có thể có B1’, B1’’ &#x3D;&gt; B</li></ul><pre><code class="java">public class Employee &#123;  private String name;  @JsonAlias(&#123;&quot;department&quot;, &quot;employeeDept&quot; &#125;)  private String dept;    .............&#125;</code></pre><h3 id="1-9-JsonCreator"><a href="#1-9-JsonCreator" class="headerlink" title="1.9 @JsonCreator"></a>1.9 @JsonCreator</h3><pre><code class="java"> @JsonCreator        public Employee(@JsonProperty(&quot;name&quot;) String name, @JsonProperty(&quot;dept&quot;) String dept) &#123;            System.out.println(&quot;&#39;constructor invoked&#39;&quot;);            this.name = name;            this.dept = dept;        &#125;</code></pre><ul><li>Giống JsonProperties, khác ở điểm khai báo ở Constructor, không phải bên trên properties</li><li>Chỉ có ý nghĩa khi Deserialize</li></ul><h3 id="1-20-ConstructorProperties"><a href="#1-20-ConstructorProperties" class="headerlink" title="1.20 @ConstructorProperties"></a>1.20 @ConstructorProperties</h3><pre><code class="java">@ConstructorProperties(&#123;&quot;name&quot;, &quot;dept&quot;&#125;)  public Employee(String name, String dept) &#123;      System.out.println(&quot;Constructor invoked&quot;);      //Java 9 StackWalker to find out the caller      System.out.println(&quot;caller: &quot; + StackWalker.getInstance(              StackWalker.Option.RETAIN_CLASS_REFERENCE).getCallerClass());      this.name = name;      this.dept = dept;  &#125;</code></pre><ul><li>Giống JsonCreator, nhưng khai báo multi 1 lúc</li></ul><h3 id="1-21-JsonSerialize-JsonDeserialize"><a href="#1-21-JsonSerialize-JsonDeserialize" class="headerlink" title="1.21 @JsonSerialize @JsonDeserialize"></a>1.21 @JsonSerialize @JsonDeserialize</h3><pre><code class="java">@JsonSerialize(converter = LocalDateTimeToStringConverter.class)@JsonDeserialize(converter = StringToLocalDatetimeConverter.class)private LocalDateTime lastUpdated;</code></pre><pre><code class="java">public class LocalDateTimeToStringConverter extends StdConverter&lt;LocalDateTime, String&gt; &#123;  static final DateTimeFormatter DATE_FORMATTER = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.MEDIUM);  @Override  public String convert(LocalDateTime value) &#123;      return value.format(DATE_FORMATTER);  &#125;&#125;</code></pre><h3 id="1-22-JsonInclude"><a href="#1-22-JsonInclude" class="headerlink" title="1.22 @JsonInclude"></a>1.22 @JsonInclude</h3><pre><code class="java">@JsonInclude(JsonInclude.Include.NON_NULL)public class Employee &#123;  private String name;  private String dept;  private String address;&#125;</code></pre><ul><li>Trường nào khi serialize có properties &#x3D; null, thì trường đó sẽ không có trong json trả về (không có, chứ không phải<br>có mà bằng null)</li><li>Cách config khác tương tự:</li></ul><pre><code class="java">ObjectMapper om = new ObjectMapper();        om.setDefaultPropertyInclusion(JsonInclude.Include.NON_NULL);</code></pre><ul><li>Danh sách JsonInclude.Include…</li></ul><pre><code class="java">        ALWAYS         NON_NULL        NON_ABSENT        NON_EMPTY        NON_DEFAULT        CUSTOM        USE_DEFAULTS</code></pre><pre><code class="java">  @JsonInclude(value = JsonInclude.Include.NON_EMPTY, content = JsonInclude.Include.NON_EMPTY)  private AtomicReference&lt;String&gt; address;</code></pre><pre><code class="java">@JsonInclude(content = JsonInclude.Include.CUSTOM, contentFilter = PhoneFilter.class)  private Map&lt;String, String&gt; phones;//   public class PhoneFilter &#123;  private static Pattern phonePattern = Pattern.compile(&quot;\\d&#123;3&#125;-\\d&#123;3&#125;-\\d&#123;4&#125;&quot;);  @Override  public boolean equals(Object obj) &#123;      if (obj == null || !(obj instanceof String)) &#123;          return false;      &#125;      //phone must match the regex pattern      return !phonePattern.matcher(obj.toString()).matches();  &#125;&#125;</code></pre><h3 id="1-23-JsonFormat"><a href="#1-23-JsonFormat" class="headerlink" title="1.23 @JsonFormat"></a>1.23 @JsonFormat</h3><pre><code class="java">@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = &quot;yyyy/MM/dd&quot;, timezone = &quot;America/Chicago&quot; )  private Date customerSince;</code></pre><ul><li>format định dạng thời gian trả về</li></ul><pre><code class="java"> @JsonFormat(shape = JsonFormat.Shape.NUMBER)  private Dept dept;// public enum Dept &#123;  Admin, IT, Sales&#125;</code></pre><ul><li>format định dạng Enum trả về</li></ul><h3 id="1-24-JsonView"><a href="#1-24-JsonView" class="headerlink" title="1.24 @JsonView"></a>1.24 @JsonView</h3><ul><li>Tạo ra 1 interface chứa nhiều kiểu VIEW</li><li>trước mỗi properties khai báo properties đó sẽ được sử dụng ở View nào</li><li>Khi serialization thì cần khai báo kiểu View ở objectMapper với mode View tương ứng, chỉ các properties có khai báo<br>kiểu view mới hiện ra</li></ul><pre><code class="java">@JsonView(&#123;Views.DemoView.class&#125;)        private String address;        @JsonView(&#123;Views.QuickContactView.class&#125;)        private String phone;// public static class Views &#123;        interface QuickContactView &#123;&#125;        interface DetailedView&#123;&#125;        interface DemoView&#123;&#125;    &#125;// ObjectMapper om = new ObjectMapper();        String jsonString = om.writerWithView(Views.DemoView.class)                .writeValueAsString(customer);</code></pre><h3 id="1-25-JsonUnwrapped"><a href="#1-25-JsonUnwrapped" class="headerlink" title="1.25 @JsonUnwrapped"></a>1.25 @JsonUnwrapped</h3><ul><li>trường hợp serialize&#x2F;deserialize, không muốn class con được gọi bên trong class cha, có cấu trúc json cha&#x2F;con</li></ul><pre><code class="java">public class Employee &#123;  private String name;  @JsonUnwrapped  private Department dept;&#125;//public class Department &#123;  private String deptName;  private String location;    .............&#125;</code></pre><ul><li>Output trường hợp có dùng JsonUnwrapped</li></ul><pre><code class="json">&#123;&quot;name&quot;:&quot;Amy&quot;,&quot;deptName&quot;:&quot;Admin&quot;,&quot;location&quot;:&quot;NY&quot;&#125;</code></pre><ul><li>Output trường hợp không dùng JsonUnwrapped</li></ul><pre><code class="json">&#123;&quot;name&quot;:&quot;Amy&quot;,&quot;dept&quot;:&#123;&quot;deptName&quot;:&quot;Admin&quot;,&quot;location&quot;:&quot;NY&quot;&#125;&#125;</code></pre><ul><li>Có thể config thêm prefix-suffix</li></ul><pre><code class="java">@JsonUnwrapped(prefix = &quot;dept-&quot;)private Department dept;</code></pre><h3 id="1-26-JsonAnyGetter"><a href="#1-26-JsonAnyGetter" class="headerlink" title="1.26 @JsonAnyGetter"></a>1.26 @JsonAnyGetter</h3><ul><li>serialize</li></ul><pre><code class="java">@JsonAnyGetter    public Map&lt;String, Object&gt; getOtherInfo() &#123;        return otherInfo;    &#125;</code></pre><ul><li>Có dùng JsonAnyGetter</li></ul><pre><code class="json">&#123;&quot;id&quot;:&quot;TradeDetails&quot;,&quot;title&quot;:&quot;Trade Details&quot;,&quot;width&quot;:500,&quot;height&quot;:300,&quot;xLocation&quot;:400,&quot;yLocation&quot;:200&#125;</code></pre><ul><li>Không dùng</li></ul><pre><code class="json">&#123;&quot;id&quot;:&quot;TradeDetails&quot;,&quot;title&quot;:&quot;Trade Details&quot;,&quot;width&quot;:500,&quot;height&quot;:300,&quot;otherInfo&quot;:&#123;&quot;xLocation&quot;:400,&quot;yLocation&quot;:200&#125;&#125;</code></pre><h3 id="1-27-JsonEnumDefaultValue"><a href="#1-27-JsonEnumDefaultValue" class="headerlink" title="1.27 @JsonEnumDefaultValue"></a>1.27 @JsonEnumDefaultValue</h3><pre><code class="java">public enum EmployeeType &#123;  FullTime,  PartTime,  @JsonEnumDefaultValue  Contractor;&#125;//ObjectMapper om = new ObjectMapper();om.enable(DeserializationFeature.READ_UNKNOWN_ENUM_VALUES_USING_DEFAULT_VALUE);</code></pre><h3 id="1-28-JsonRootName"><a href="#1-28-JsonRootName" class="headerlink" title="1.28 @JsonRootName"></a>1.28 @JsonRootName</h3><pre><code class="java">@JsonRootName(&quot;Person&quot;)public class PersonEntity &#123;  private String name;  private int age;    .............&#125;</code></pre><ul><li>dùng để define root khi des&#x2F;se</li></ul><h3 id="1-29-JsonFilter"><a href="#1-29-JsonFilter" class="headerlink" title="1.29 @JsonFilter"></a>1.29 @JsonFilter</h3><h3 id="1-30-JsonMerge"><a href="#1-30-JsonMerge" class="headerlink" title="1.30 @JsonMerge"></a>1.30 @JsonMerge</h3><h2 id="2-Config-ObjectMapper-Example"><a href="#2-Config-ObjectMapper-Example" class="headerlink" title="2. Config ObjectMapper Example"></a>2. Config ObjectMapper Example</h2><pre><code class="java">@Component@Configurationpublic class ObjectMapperConfig &#123;    @Bean(name = &quot;json_main&quot;)    @Primary    public ObjectMapper main() &#123;        ObjectMapper objectMapper = new ObjectMapper();        objectMapper.configure(SerializationFeature.WRAP_ROOT_VALUE, true);        objectMapper.configure(DeserializationFeature.UNWRAP_ROOT_VALUE, true);        objectMapper.setPropertyNamingStrategy(PropertyNamingStrategy.SNAKE_CASE);        objectMapper.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);        objectMapper.setDateFormat(new ISO8601DateFormat());        SimpleModule module = new SimpleModule();        module.addDeserializer(String.class, new StringDeserializer());        SimpleFilterProvider filters = new SimpleFilterProvider();        filters.addFilter(&quot;empty&quot;, SimpleBeanPropertyFilter.serializeAllExcept(new HashSet&lt;&gt;()));        filters.addFilter(&quot;field&quot;, SimpleBeanPropertyFilter.serializeAllExcept(new HashSet&lt;&gt;()));        objectMapper.setFilterProvider(filters);        objectMapper.registerModule(module);        return objectMapper;    &#125;&#125;</code></pre><ul><li>ref: <a href="https://www.logicbig.com/tutorials/misc/jackson.html">https://www.logicbig.com/tutorials/misc/jackson.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> jackson </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> serialize </tag>
            
            <tag> jackson </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker - Command</title>
      <link href="/2019/12/Docker/Docker_Command/"/>
      <url>/2019/12/Docker/Docker_Command/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Install"><a href="#1-Install" class="headerlink" title="1. Install"></a>1. Install</h2><pre><code class="bash">#!/bin/bash# Set Docker version (you can modify this as needed)DOCKER_VERSION=24.0.7# Install Dockerecho &quot;Try to build with Docker version: $DOCKER_VERSION&quot;curl -fsSL https://get.docker.com -o get-docker.shsudo sh get-docker.sh --version $DOCKER_VERSION</code></pre><ul><li>Sau khi cài đặt docker, cần lưu ý gán quyền user cho docker. Nếu không gán quyền, thì command trong docker sẽ bị<br>permission</li></ul><pre><code class="sh">sudo usermod -aG docker $USER</code></pre><p>Logout sau đó login lại để có hiệu lực</p><h2 id="3-Docker-Swarm"><a href="#3-Docker-Swarm" class="headerlink" title="3 Docker Swarm"></a>3 Docker Swarm</h2><h3 id="3-1-Tao-Swarm"><a href="#3-1-Tao-Swarm" class="headerlink" title="3.1 Tạo Swarm"></a>3.1 Tạo Swarm</h3><pre><code class="bash"># Tại node leader &gt; khởi tạodocker swarm init --advertise-addr=192.168.99.117# Tại node worker &gt; joindocker swarm join --token SWMTKN-1-5xv7z2ijle1dhivalkl5cnwhoadp6h8ae0p7bs5tmanvkpbi3l-5ib6sjrd3w0wdhfsnt8ga7ybd 192.168.99.111:2377# Kiểm tra các node trong swarmdocker node ls</code></pre><h3 id="3-2-Tao-service"><a href="#3-2-Tao-service" class="headerlink" title="3.2 Tạo service"></a>3.2 Tạo service</h3><pre><code class="bash"># Tạo một servicedocker service create --replicas 5 -p 8085:8085 --name testservice ichte/swarmtest:node# Liệt kê các service trên swarmdocker service ls# Liệt kê các container cho dịch vụ có tên testservicedocker service ps testservice# Kiểm tra log cho dịch vụ testservicedocker service logs testservice# Thay đổi số lượng container cho dịch vụ testservicedocker service scale testservice=n# Cập nhật thiết lập cho dịch vụ testservice# Thay đổi Imagedocker service update --image=ichte/swarmtest:php testservice# Thay đổi tài nguyên CPU, MEMdocker service update --limit-cpu=&quot;0.5&quot; --limit-memory=150M testservice# Các cập nhật khác cho servicedocker service update --update-parallelism=2 --update-delay=10s testservice# Xóa dịch vụ testservicedocker service rm testservice# Xóa nhiều Docker images không có tên (dangling images)docker rmi $(docker images -f &quot;dangling=true&quot; -q)</code></pre>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> command </tag>
            
            <tag> script </tag>
            
            <tag> install </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS - S3</title>
      <link href="/2019/05/AWS/AWS_LAB_S3/"/>
      <url>/2019/05/AWS/AWS_LAB_S3/</url>
      
        <content type="html"><![CDATA[<h1 id="AWS-S3-mot-service-Cloud-Storage"><a href="#AWS-S3-mot-service-Cloud-Storage" class="headerlink" title="AWS S3 - một service Cloud Storage"></a>AWS S3 - một service Cloud Storage</h1><p>S3 là 1 service của AWS cho chức năng lưu trữ dữ liệu hướng Object</p><h2 id="1-Thu-vien"><a href="#1-Thu-vien" class="headerlink" title="1. Thư viện"></a>1. Thư viện</h2><p>Tại thời điểm bài viết này, AWS đã cho ra bản SDK 2.0 cho Java, tuy nhiên đây là bản preview cho developer.</p><h2 id="2-Tao-IAM"><a href="#2-Tao-IAM" class="headerlink" title="2. Tạo IAM"></a>2. Tạo IAM</h2><p>Cũng giống như nhiều SDK khác, Amazon AWS sử dụng các secret key để developer implement vô code.<br>Sử dụng secret key để access tới service của Amazon, thay vì sử dụng username + password như trên giao diện web.<br>Service quản lý việc này của Amazon là IAM. Để tạo và config role cho các secret key này bạn truy cập tại<br>đây: <a href="https://console.aws.amazon.com/iam">https://console.aws.amazon.com/iam</a></p><pre><code class="java">// ExampleString AWSAccessKeyId=&quot;AKIAIBGCWNEKIYZSKXTA&quot;;String AWSSecretKey=&quot;LZjMW4t/udiEu8UXupg++I0mQsaXsm8Jb99upJBi&quot;;</code></pre><h2 id="3-Demo"><a href="#3-Demo" class="headerlink" title="3. Demo"></a>3. Demo</h2><h3 id="Step-1-Khoi-tao-ket-noi"><a href="#Step-1-Khoi-tao-ket-noi" class="headerlink" title="Step 1. Khởi tạo kết nối"></a>Step 1. Khởi tạo kết nối</h3><p>Để thao tác được với AWS, Cần khởi tạo client trước.<br>Nôm na có thể hiểu nhanh là bước này để verify kết nối, xem access key, secret key có đúng không.</p><pre><code class="java">BasicAWSCredentials basicAWSCredentials = new BasicAWSCredentials(awsAccessKey, awsSecretKey);AWSStaticCredentialsProvider credentialsProvider = new AWSStaticCredentialsProvider(basicAWSCredentials);AmazonS3ClientBuilder s3ClientBuilder = AmazonS3ClientBuilder.standard().withRegion(awsRegion).withCredentials(credentialsProvider);AmazonS3 s3connect = s3ClientBuilder.build();</code></pre><p>Trong đó String awsRegion &#x3D; “ap-southeast-1”;<br>Amazon S3 cung cấp nhiều Region để chứa dữ liệu.<br>Có thể hiểu nhanh Region là vùng lưu trữ,Và AWS có hệ thống servers tại nhiều nơi trên thế giới.<br>Bạn có thể chọn lấy 1 Region, nơi mà bạn thích để chứa dữ liệu của bạn. (Dựa theo vị trí địa lý chẳng hạn). Đây là danh<br>sách các Regions AWS cung cấp:</p><pre><code>    GovCloud(&quot;us-gov-west-1&quot;, &quot;AWS GovCloud (US)&quot;),    US_EAST_1(&quot;us-east-1&quot;, &quot;US East (N. Virginia)&quot;),    US_EAST_2(&quot;us-east-2&quot;, &quot;US East (Ohio)&quot;),    US_WEST_1(&quot;us-west-1&quot;, &quot;US West (N. California)&quot;),    US_WEST_2(&quot;us-west-2&quot;, &quot;US West (Oregon)&quot;),    EU_WEST_1(&quot;eu-west-1&quot;, &quot;EU (Ireland)&quot;),    EU_WEST_2(&quot;eu-west-2&quot;, &quot;EU (London)&quot;),    EU_WEST_3(&quot;eu-west-3&quot;, &quot;EU (Paris)&quot;),    EU_CENTRAL_1(&quot;eu-central-1&quot;, &quot;EU (Frankfurt)&quot;),    AP_SOUTH_1(&quot;ap-south-1&quot;, &quot;Asia Pacific (Mumbai)&quot;),    AP_SOUTHEAST_1(&quot;ap-southeast-1&quot;, &quot;Asia Pacific (Singapore)&quot;),    AP_SOUTHEAST_2(&quot;ap-southeast-2&quot;, &quot;Asia Pacific (Sydney)&quot;),    AP_NORTHEAST_1(&quot;ap-northeast-1&quot;, &quot;Asia Pacific (Tokyo)&quot;),    AP_NORTHEAST_2(&quot;ap-northeast-2&quot;, &quot;Asia Pacific (Seoul)&quot;),    SA_EAST_1(&quot;sa-east-1&quot;, &quot;South America (Sao Paulo)&quot;),    CN_NORTH_1(&quot;cn-north-1&quot;, &quot;China (Beijing)&quot;),    CN_NORTHWEST_1(&quot;cn-northwest-1&quot;, &quot;China (Ningxia)&quot;),    CA_CENTRAL_1(&quot;ca-central-1&quot;, &quot;Canada (Central)&quot;);</code></pre><p>&#x2F;&#x2F; Có thể xem danh sách này trong com.amazonaws.regions .</p><h3 id="Step-2-Tao-bucketName"><a href="#Step-2-Tao-bucketName" class="headerlink" title="Step 2. Tạo bucketName"></a>Step 2. Tạo bucketName</h3><p>BucketName là gì? Có thể hiểu nhanh nó là tên 1 folder để chứa tất cả các dữ liệu của mình.<br>Lưu ý: bucketName là unique với toàn hệ thống AWS (nhắc lại là toàn hệ thống aws, chứ không phải unique trong 1 tài<br>khoản).</p><pre><code class="java">public static Bucket createBucket(AmazonS3 amazonS3, String bucketName) &#123;        Bucket bucket = null;        try &#123;            bucket = amazonS3.createBucket(bucketName);        &#125; catch (AmazonServiceException ase) &#123;            LOG.error(&quot;Caught an AmazonServiceException, which means your request made it &quot;                    + &quot;to Amazon S3, but was rejected with an error response for some reason.&quot;);            LOG.error(&quot;Error Message:    &quot; + ase.getMessage());            LOG.error(&quot;HTTP Status Code: &quot; + ase.getStatusCode());            LOG.error(&quot;AWS Error Code:   &quot; + ase.getErrorCode());            LOG.error(&quot;Error Type:       &quot; + ase.getErrorType());            LOG.error(&quot;Request ID:       &quot; + ase.getRequestId());        &#125; catch (AmazonClientException ace) &#123;            LOG.error(&quot;Caught an AmazonClientException, which means the client encountered &quot;                    + &quot;a serious internal problem while trying to communicate with S3, &quot;                    + &quot;such as not being able to access the network.&quot;);            LOG.error(&quot;Error Message: &quot; + ace.getMessage());        &#125;        return bucket;    &#125;</code></pre><p>Thực ra nguyên đoạn code dài ngoằng trên chỉ cô đọng trong 1 dòng:</p><pre><code class="java"> Bucket bucket = amazonS3.createBucket(bucketName);</code></pre><p>Cơ mà hãy cứ code sử dụng try catch như mình, để lấy được log lỗi cho chuẩn! Dễ debug.</p><h3 id="Step-3-Upload-file-to-S3"><a href="#Step-3-Upload-file-to-S3" class="headerlink" title="Step 3. Upload file to S3"></a>Step 3. Upload file to S3</h3><p>Thực ra thì dùng từ “file” nó không được đúng lắm, với 1 hệ thống lưu trữ Object Storage thì không có khái niệm là “<br>file”.<br>Họ sử dụng từ “object”. Cơ mà trong khuôn khổ demo code này mình viết văn theo cách mình nghĩ người khác dễ hiểu nhất.</p><pre><code class="java">/**     * upload file to s3     * isPrivate = true for private file. Ex: csv report     * isPrivate = false for public file: Ex: avatar user     *     * @param amazonS3     * @param putObjectRequest     * @param isPrivate     * @return link to access file on s3     */    public static String uploadFile(AmazonS3 amazonS3, PutObjectRequest putObjectRequest, Boolean isPrivate) &#123;        String urlResult = &quot;&quot;;        if (putObjectRequest == null) return urlResult;        try &#123;            if (isPrivate) &#123;                amazonS3.putObject(putObjectRequest.withCannedAcl(CannedAccessControlList.Private));            &#125; else &#123;                amazonS3.putObject(putObjectRequest.withCannedAcl(CannedAccessControlList.PublicRead));            &#125;            urlResult = amazonS3.getUrl(putObjectRequest.getBucketName(), putObjectRequest.getKey()).toString();        &#125; catch (AmazonServiceException ase) &#123;        &#125; catch (AmazonClientException ace) &#123;            LOG.error(&quot;Error Message: &quot; + ace.getMessage());        &#125;        return urlResult;    &#125;    </code></pre><p>Trong đó PutObjectRequest được khởi tạo bởi các thuộc tính sau:</p><pre><code class="java">PutObjectRequest(String bucketName, String key, File file)</code></pre><p>Hoặc</p><pre><code class="java">PutObjectRequest(String bucketName, String key, InputStream input, ObjectMetadata metadata)</code></pre><ul><li>bucketName: đã giải thích bên trên.</li><li>key: fileName, có thể hiểu nhanh là tên của file, nằm trong folder. Và nó cũng là unique trong mỗi bucket.</li><li>CannedAccessControlList.Private : khi 1 file upload lên S3, nếu không có config gì đặc biệt, default nó sẽ là private.</li></ul><h3 id="Step-4-Get-link-download-file-tu-S3-co-token"><a href="#Step-4-Get-link-download-file-tu-S3-co-token" class="headerlink" title="Step 4 Get link download file từ S3 (có token)"></a>Step 4 Get link download file từ S3 (có token)</h3><p>(Thực tế kỹ thuật này được gọi là Signer URL)<br>Với những file config policy là PUBLIC thì thật dễ dàng dể download, chỉ cần copy URL theo format na ná như sau:</p><p><a href="https://s3-us-west-2.amazonaws.com/my-tungtv202-avatar/MyObjectKey">https://s3-us-west-2.amazonaws.com/my-tungtv202-avatar/MyObjectKey</a><br>là có thể download được mọi lúc mọi nơi.</p><p>Tuy nhiên với những file có policy là PRIVATE thì khi truy cập như vậy, sẽ gặp thông báo sau:</p><pre><code class="xml">Error&gt;&lt;Code&gt;AccessDenied&lt;/Code&gt;&lt;Message&gt;Access Denied&lt;/Message&gt;&lt;RequestId&gt;AEE10DFAA27FEF26&lt;/RequestId&gt;&lt;HostId&gt;ipPFZDboIzOCohRl4/RPe9I/IBQVn3esK+8mnGQO3yDKIPcatbnbl41SxC2oMUKPpt7WcG+toqk=&lt;/HostId&gt;&lt;/Error&gt;</code></pre><p>Đoạn code dưới đây để getlink download (link có kèm token, token có thời hạn available).</p><pre><code class="java">/**     * get download link for private file. That need token to download     * bucketName same as folder name     * fileName is unique per bucketName     *     * @param amazonS3     * @param bucketName     * @param fileName     * @return download link     */    public static String getDownloadLink(AmazonS3 amazonS3, String bucketName, String fileName) &#123;        String downloadLink = &quot;&quot;;        if (bucketName.isEmpty() || fileName.isEmpty()) return downloadLink;        Date expiration = new Date();        long expTimeMillis = new Date().getTime();        expiration.setTime(expTimeMillis + TIME_MINUTES_EXPIRED * 1000 * 60);        try &#123;            GeneratePresignedUrlRequest generatePresignedUrlRequest =                    new GeneratePresignedUrlRequest(bucketName, fileName)                            .withMethod(HttpMethod.GET)                            .withExpiration(expiration);            URL url = amazonS3.generatePresignedUrl(generatePresignedUrlRequest);            downloadLink = url.toString();        &#125; catch (AmazonServiceException ase) &#123;        &#125; catch (AmazonClientException ace) &#123;            LOG.error(&quot;Error Message: &quot; + ace.getMessage());        &#125;        return downloadLink;    &#125;</code></pre><p>Example 1 download link có token:</p><pre><code>https://xinchaovietna222me.s3.ap-southeast-1.amazonaws.com/188?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20180710T065913Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=299&amp;X-Amz-Credential=AKIAIP7Y2FP3U3AJWLPQ%2F20180710%2Fap-southeast-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=fc46c9d6cef32a94ee120dac5ab6a33c08e245256b979be977232b76c32e6926</code></pre><h2 id="4-Su-dung-SSE-C-de-encrypt-decrypt-object"><a href="#4-Su-dung-SSE-C-de-encrypt-decrypt-object" class="headerlink" title="4. Sử dụng SSE-C để encrypt&#x2F;decrypt object"></a>4. Sử dụng SSE-C để encrypt&#x2F;decrypt object</h2><ul><li>Với cách này, chỉ có client chứa “key” mới có thể download&#x2F;retrivew metadata của object được. Cho dù tài khoản root<br>của AWS có full quyền, cũng không thể can thiệp, nếu không có key.<br>ref: <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/sse-c-using-java-sdk.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/sse-c-using-java-sdk.html</a></li></ul><h3 id="4-1-Code-example"><a href="#4-1-Code-example" class="headerlink" title="4.1 Code example"></a>4.1 Code example</h3><pre><code class="java">package tung.demo.ssec_s3;import com.amazonaws.AmazonServiceException;import com.amazonaws.SdkClientException;import com.amazonaws.auth.AWSStaticCredentialsProvider;import com.amazonaws.auth.BasicAWSCredentials;import com.amazonaws.regions.Regions;import com.amazonaws.services.s3.AmazonS3;import com.amazonaws.services.s3.AmazonS3ClientBuilder;import com.amazonaws.services.s3.model.*;import javax.crypto.KeyGenerator;import java.io.BufferedReader;import java.io.File;import java.io.IOException;import java.io.InputStreamReader;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;public class ServerSideEncryptionUsingClientSideEncryptionKey &#123;    private static SSECustomerKey SSE_KEY;    private static AmazonS3 S3_CLIENT;    private static KeyGenerator KEY_GENERATOR;    public static void main(String[] args) throws IOException, NoSuchAlgorithmException &#123;        Regions clientRegion = Regions.AP_SOUTHEAST_1;        String accessKey = &quot;&quot;;        String secretKey = &quot;ga+tu8vB&quot;;        String bucketName = &quot;&quot;;        String keyName = &quot;test001.png&quot;;        String uploadFileName = &quot;D:\\yamaha.png&quot;;        String targetKeyName = &quot;*** Target key name ***&quot;;        // Create an encryption key.        KEY_GENERATOR = KeyGenerator.getInstance(&quot;AES&quot;);        SecureRandom secureRandom = SecureRandom.getInstance(&quot;SHA1PRNG&quot;);        secureRandom.setSeed(&quot;TUNGTUNGTUNG&quot;.getBytes());        KEY_GENERATOR.init(256, secureRandom);        SSE_KEY = new SSECustomerKey(KEY_GENERATOR.generateKey());        try &#123;            S3_CLIENT = AmazonS3ClientBuilder.standard()                    .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey)))                    .withRegion(clientRegion)                    .build();            // Upload an object.            uploadObject(bucketName, keyName, new File(uploadFileName));            // Download the object.//            downloadObject(bucketName, keyName);            // Verify that the object is properly encrypted by attempting to retrieve it            // using the encryption key.            retrieveObjectMetadata(bucketName, keyName);            // Copy the object into a new object that also uses SSE-C.//            copyObject(bucketName, keyName, targetKeyName);        &#125; catch (AmazonServiceException e) &#123;            // The call was transmitted successfully, but Amazon S3 couldn&#39;t process            // it, so it returned an error response.            e.printStackTrace();        &#125; catch (SdkClientException e) &#123;            // Amazon S3 couldn&#39;t be contacted for a response, or the client            // couldn&#39;t parse the response from Amazon S3.            e.printStackTrace();        &#125;    &#125;    private static void uploadObject(String bucketName, String keyName, File file) &#123;//        PutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName, file);        PutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName, file).withSSECustomerKey(SSE_KEY);        S3_CLIENT.putObject(putRequest);        System.out.println(&quot;Object uploaded&quot;);    &#125;    private static void downloadObject(String bucketName, String keyName) throws IOException &#123;        GetObjectRequest getObjectRequest = new GetObjectRequest(bucketName, keyName).withSSECustomerKey(SSE_KEY);        S3Object object = S3_CLIENT.getObject(getObjectRequest);        System.out.println(&quot;Object content: &quot;);        displayTextInputStream(object.getObjectContent());    &#125;    private static void retrieveObjectMetadata(String bucketName, String keyName) &#123;        GetObjectMetadataRequest getMetadataRequest = new GetObjectMetadataRequest(bucketName, keyName)                .withSSECustomerKey(SSE_KEY);        ObjectMetadata objectMetadata = S3_CLIENT.getObjectMetadata(getMetadataRequest);        System.out.println(&quot;Metadata retrieved. Object size: &quot; + objectMetadata.getContentLength());    &#125;    private static void copyObject(String bucketName, String keyName, String targetKeyName)            throws NoSuchAlgorithmException &#123;        // Create a new encryption key for target so that the target is saved using SSE-C.        SSECustomerKey newSSEKey = new SSECustomerKey(KEY_GENERATOR.generateKey());        CopyObjectRequest copyRequest = new CopyObjectRequest(bucketName, keyName, bucketName, targetKeyName)                .withSourceSSECustomerKey(SSE_KEY)                .withDestinationSSECustomerKey(newSSEKey);        S3_CLIENT.copyObject(copyRequest);        System.out.println(&quot;Object copied&quot;);    &#125;    private static void displayTextInputStream(S3ObjectInputStream input) throws IOException &#123;        // Read one line at a time from the input stream and display each line.        BufferedReader reader = new BufferedReader(new InputStreamReader(input));        String line;        while ((line = reader.readLine()) != null) &#123;            System.out.println(line);        &#125;        System.out.println();    &#125;&#125;</code></pre><p>.</p>]]></content>
      
      
      <categories>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> s3 </tag>
            
            <tag> object storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Signed URL</title>
      <link href="/2019/05/Other/Signed_URL/"/>
      <url>/2019/05/Other/Signed_URL/</url>
      
        <content type="html"><![CDATA[<h1 id="Signed-URL"><a href="#Signed-URL" class="headerlink" title="Signed URL"></a>Signed URL</h1><h3 id="1-Khai-niem"><a href="#1-Khai-niem" class="headerlink" title="1. Khái niệm"></a>1. Khái niệm</h3><p>Là việc sử dụng URL đã được “signed” để được cấp quyền truy cập vào “resource” mà developer cấu hình trước và được giới<br>hạn trong 1 khoản thời gian nhất định.<br>Ví dụ:</p><ul><li>Đây là URL tới 1 resource file ảnh được lấy trên facebook</li></ul><pre><code>https://scontent.fhan2-1.fna.fbcdn.net/v/t1.0-9/59423202_442314843263996_7671343376326721536_n.jpg</code></pre><p>File ảnh này hiện tại được set private quyền, không thể xem được.<br>Và đây là Signed URL của file ảnh này:</p><pre><code>https://scontent.fhan2-1.fna.fbcdn.net/v/t1.0-9/59423202_442314843263996_7671343376326721536_n.jpg?_nc_cat=101&amp;_nc_oc=AQlnMDOmZYDHdq_ixK4jZzBLHzMdzb2AyEtZZQ4a4C-2E2YnH4LRaExofT0E86Xs-V1oNWDNO7tJ-3hvh-Y9JxX9&amp;_nc_ht=scontent.fhan2-1.fna&amp;oh=549bf01567ce4b588c9853b8b8783676&amp;oe=5D60E9C4</code></pre><p>Với Signed URL ta có thể “access” được vào “resource” file ảnh trên. (xem được).</p><p>Thường thì Signed URL hay đi kèm với các hệ thống lưu trữ dữ liệu Cloud Storage. Các service lớn đều cung cấp sẵn tính<br>năng này, như AWS S3, Google Cloud Storage, IBM Bluemix, Azure Blob Storage…</p><ul><li>Format Signed URL của AWS S3:</li></ul><pre><code>https://pres-url-test.s3-eu-west-1.amazonaws.com/test.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJQ6UAEQOACU54C3A%2F20180927%2Feu-west-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20180927T100139Z&amp;X-Amz-Expires=900&amp;X-Amz-Signature=f6fa35129753e7626c850a531379436a555447bfbd597c19e3177ae3d2c48387&amp;X-Amz-SignedHeaders=host</code></pre><ul><li>Format Signed URL của Google Cloud Storage</li></ul><pre><code>https://storage.googleapis.com/example-bucket/cat.jpeg?X-Goog-Algorithm= GOOG4-RSA-SHA256&amp;X-Goog-Credential=example%40example-project.iam.gserviceaccount .com%2F20181026%2Fus-central-1%2Fstorage%2Fgoog4_request&amp;X-Goog-Date=20181026T18 1309Z&amp;X-Goog-Expires=900&amp;X-Goog-SignedHeaders=host&amp;X-Goog-Signature=247a2aa45f16 9edf4d187d54e7cc46e4731b1e6273242c4f4c39a1d2507a0e58706e25e3a85a7dbb891d62afa849 6def8e260c1db863d9ace85ff0a184b894b117fe46d1225c82f2aa19efd52cf21d3e2022b3b868dc c1aca2741951ed5bf3bb25a34f5e9316a2841e8ff4c530b22ceaa1c5ce09c7cbb5732631510c2058 0e61723f5594de3aea497f195456a2ff2bdd0d13bad47289d8611b6f9cfeef0c46c91a455b94e90a 66924f722292d21e24d31dcfb38ce0c0f353ffa5a9756fc2a9f2b40bc2113206a81e324fc4fd6823 a29163fa845c8ae7eca1fcf6e5bb48b3200983c56c5ca81fffb151cca7402beddfc4a76b13344703 2ea7abedc098d2eb14a7</code></pre><p>Bản chất Signed URL thực tế chỉ là việc truyền thêm các param vào URL, và các param này sẽ được Server xử lý để<br>verified.</p><h3 id="2-Dac-diem"><a href="#2-Dac-diem" class="headerlink" title="2. Đặc điểm"></a>2. Đặc điểm</h3><ul><li>Signed URL là 1 tư tưởng kĩ thuật, và chỉ khi nó đi kèm với 1 service thì mới thành 1 danh từ riêng. Tức là Signed URL<br>chỉ chung chung việc 1 URL đã được ký. Còn việc nó được ký như thế nào, sử dụng thuật toán nào để ký, hệ thống xác<br>thực nó bằng cách nào. Thì việc này hoàn toàn tự quyết định bởi hệ thống cung cấp nó, mà không có 1 chuẩn chung nào<br>cả.</li><li>Signed URL không có tính định danh người truy cập. Bất cứ ai có được URL này đều có thể truy cập vào resource được với<br>quyền tương đương nhau.</li></ul><p>Format của Signed URL trong Google Cloud Storage:</p><pre><code>https://storage.googleapis.com/example-bucket/cat.jpeg?X-Goog-Algorithm=GOOG4-RSA-SHA256&amp;X-Goog-Credential=example%40example-project.iam.gserviceaccount.com%2F20181026%2Fus-central-1%2Fstorage%2Fgoog4_request&amp;X-Goog-Date=20181026T181309Z&amp;X-Goog-Expires=900&amp;X-Goog-SignedHeaders=host&amp;X-Goog-Signature=247a2aa45f169edf4d187d54e7cc46e4731b1e6273242c4f4c39a1d2507a0e58706e25e3a85a7dbb891d62afa8496def8e260c1db863d9ace85ff0a184b894b117fe46d1225c82f2aa19efd52cf21d3e2022b3b868dcc1aca2741951ed5bf3bb25a34f5e9316a2841e8ff4c530b22ceaa1c5ce09c7cbb5732631510c20580e61723f5594de3aea497f195456a2ff2bdd0d13bad47289d8611b6f9cfeef0c46c91a455b94e90a66924f722292d21e24d31dcfb38ce0c0f353ffa5a9756fc2a9f2b40bc2113206a81e324fc4fd6823a29163fa845c8ae7eca1fcf6e5bb48b3200983c56c5ca81fffb151cca7402beddfc4a76b133447032ea7abedc098d2eb14a7</code></pre><p>Có thể sử dụng công cụ <a href="https://www.freeformatter.com/url-parser-query-string-splitter.html">https://www.freeformatter.com/url-parser-query-string-splitter.html</a> để parse các param trong URL<br>trên như sau:</p><pre><code>&#39;X-Goog-Algorithm&#39;:GOOG4-RSA-SHA256&#39;X-Goog-Credential&#39;: example@example-project.iam.gserviceaccount .com/20181026/us-central-1/storage/goog4_request&#39;X-Goog-Date&#39;:20181026T18 1309Z&#39;X-Goog-Expires&#39;:  900&#39;X-Goog-SignedHeaders&#39;: host&#39;X-Goog-Signature&#39;: 247a2aa45f16 9edf4d187d54e7cc46e4731b1e6273242c4f4c39a1d2507a0e58706e25e3a85a7dbb891d62afa849 6def8e260c1db863d9ace85ff0a184b894b117fe46d1225c82f2aa19efd52cf21d3e2022b3b868dc c1aca2741951ed5bf3bb25a34f5e9316a2841e8ff4c530b22ceaa1c5ce09c7cbb5732631510c2058 0e61723f5594de3aea497f195456a2ff2bdd0d13bad47289d8611b6f9cfeef0c46c91a455b94e90a 66924f722292d21e24d31dcfb38ce0c0f353ffa5a9756fc2a9f2b40bc2113206a81e324fc4fd6823 a29163fa845c8ae7eca1fcf6e5bb48b3200983c56c5ca81fffb151cca7402beddfc4a76b13344703 2ea7abedc098d2eb14a7 </code></pre><p>Trong đó:</p><pre><code>X-Goog-Algorithm: giải thuật được sử dụng để ký URLX-Goog-Credential: thông tin về &quot;credentical&quot; được sử dụng để kýX-Goog-Date: Thời gian mà Signed URL được kýX-Goog-Expires: Hạn sử dụng của Signed URL, tính theo đơn vị giây, kể từ lúc kýX-Goog-SignedHeaders: headerX-Goog-Signature: chuỗi xác thực</code></pre><p>Signed URL có thể được sử dụng với các phương thức HTTP:</p><pre><code>DELETEGETHEADPUTPOST</code></pre><h3 id="3-Cac-use-case-hay-gap"><a href="#3-Cac-use-case-hay-gap" class="headerlink" title="3. Các use case hay gặp"></a>3. Các use case hay gặp</h3><ul><li>Ứng dụng chia sẻ resource cho người dùng khác trong 1 khoảng thời gian, chỉ với URL, mà không cần phải cung cấp<br>username&#x2F;password để access vào resource đó.</li><li>Bài toán private resource. Ví dụ chỉ có User đó mới có thể xem được các file ảnh, file data… từ hệ thống lưu trữ<br>storage.</li><li>Sử dụng Signed Url để cho phép upload trực tiếp file từ client lên hệ thống Cloud Storage, mà không cần phải trung<br>gian qua Server của applicant. Nhằm tăng hiệu năng, tiết kiệm tài nguyên cho Server Backend.</li></ul><h3 id="4-Demo"><a href="#4-Demo" class="headerlink" title="4. Demo"></a>4. Demo</h3><p>AWS S3 là một service cung cấp dịch vụ lưu trữ lớn. Và AWS S3 có tích hợp sẵn chức năng Signed URL.<br>Kịch bản là code Java sử dụng SDK của AWS, để generate Signed URL get data từ 1 file trên bucket.<br>Code Java:</p><pre><code class="java">public static String getDownloadLink(AmazonS3 amazonS3, String bucketName, String fileName) &#123;  Date expiration = new Date(System.currentTimeMillis() + TIME_MINUTES_EXPIRED * 60 * 1000);  try &#123;    GeneratePresignedUrlRequest generatePresignedUrlRequest = new GeneratePresignedUrlRequest(bucketName, fileName)            .withMethod(HttpMethod.GET)            .withExpiration(expiration);    URL url = amazonS3.generatePresignedUrl(generatePresignedUrlRequest);    return url.toString();  &#125; catch (AmazonServiceException ase) &#123;    LOG.error(&quot;AmazonServiceException: &quot; + ase.getMessage(), ase);  &#125; catch (AmazonClientException ace) &#123;    LOG.error(&quot;AmazonClientException: &quot; + ace.getMessage(), ace);  &#125;  return &quot;&quot;;&#125;</code></pre><p>downloadLink example:</p><pre><code>https://xinchaovietna222me.s3.ap-southeast-1.amazonaws.com/188?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20180710T065913Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=299&amp;X-Amz-Credential=AKIAIP7Y2FP3U3AJWLPQ%2F20180710%2Fap-southeast-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=fc46c9d6cef32a94ee120dac5ab6a33c08e245256b979be977232b76c32e6926</code></pre><p>parse</p><pre><code>&#39;X-Amz-Algorithm&#39;:AWS4-HMAC-SHA256&#39;X-Amz-Date&#39;:20180710T065913Z&#39;X-Amz-SignedHeaders&#39;:host&#39;X-Amz-Expires&#39;:299&#39;X-Amz-Credential&#39;:AKIAIP7Y2FP3U3AJWLPQ/20180710/ap-southeast-1/s3/aws4_request&#39;X-Amz-Signature&#39;:fc46c9d6cef32a94ee120dac5ab6a33c08e245256b979be977232b76c32e6926</code></pre>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> s3 </tag>
            
            <tag> object storage </tag>
            
            <tag> signed url </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Memory Leak</title>
      <link href="/2019/03/Java/MemoryLeak/"/>
      <url>/2019/03/Java/MemoryLeak/</url>
      
        <content type="html"><![CDATA[<h1 id="Memory-leak-trong-Java"><a href="#Memory-leak-trong-Java" class="headerlink" title="Memory leak trong Java"></a>Memory leak trong Java</h1><p><img src="https://www.baeldung.com/wp-content/uploads/2018/11/Memory-_Leak-_In-_Java.png" alt="Context"></p><h2 id="1-Co-che"><a href="#1-Co-che" class="headerlink" title="1. Cơ chế"></a>1. Cơ chế</h2><ul><li>1 là chủ động tự trigger, 2 là đợi bộ GC tự chạy. (không rõ khi nào nó được chạy, có thể khi gần hết bộ nhớ...@@)</li><li>Có 2 giá trị xms, và xmx, cấp size nhớ lúc start, và giá trị maximum.</li><li>Dựa vào xmx, và kernel, os ..vvv, mà JVM sẽ tự tính toán ra size cho 2 vùng Old và Young (Eden). GC sẽ scan ở Eden<br>trước, và thường xuyên hơn khi scan ở Old.</li><li>Ngoài old và eden, có thể có thêm s0, s1… để phục vụ cho cơ chế phát hiện được Object không được ref ở đâu, ít phải<br>scan hơn.</li><li>Có 2 phrase: Mark and sweep. Phrase Mark sẽ scan và đánh dấu các Object không được ref, Sweep sẽ scan các object không<br>được ref và release chúng.</li></ul><h2 id="2-Cac-code-vi-du"><a href="#2-Cac-code-vi-du" class="headerlink" title="2. Các code ví dụ"></a>2. Các code ví dụ</h2><pre><code>Các ví dụ sau chỉ ra các lỗi thường gặp hay đẫn tới memory leak. Và cách phòng tránh.</code></pre><h3 id="2-1-Khai-bao-static-cho-bien"><a href="#2-1-Khai-bao-static-cho-bien" class="headerlink" title="2.1, Khai báo static cho biến"></a>2.1, Khai báo static cho biến</h3><pre><code class="java">public class StaticTest &#123;    public static List&lt;Double&gt; list = new ArrayList&lt;&gt;();    public void populateList() &#123;        for (int i = 0; i &lt; 10000000; i++) &#123;            list.add(Math.random());        &#125;        Log.info(&quot;Debug Point 2&quot;);    &#125;    public static void main(String[] args) &#123;        Log.info(&quot;Debug Point 1&quot;);        new StaticTest().populateList();        Log.info(&quot;Debug Point 3&quot;);    &#125;&#125;</code></pre><p>Ngay khi “Debug Point 1” được start, memory đã được cấp phát cho list Object Double. Sau khi “Debug Point 2” kết thúc,<br>list object này không được sử dụng nữa, nhưng memory vẫn không được thu hồi.<br>Ảnh phân tích:<br><img src="https://www.baeldung.com/wp-content/uploads/2018/11/memory-with-static.png" alt="vidu1"></p><p>Sau khi xóa khai báo static:<br><img src="https://www.baeldung.com/wp-content/uploads/2018/11/memory-without-static.png" alt="vidu2"></p><h3 id="2-2-Su-dung-wrapper-class"><a href="#2-2-Su-dung-wrapper-class" class="headerlink" title="2.2, Sử dụng wrapper class"></a>2.2, Sử dụng wrapper class</h3><pre><code class="java">public class Adder &#123;    public long addIncremental(long l) &#123;        Long sum = 0L;        sum = sum + l;        return sum;    &#125;    public static void main(String[] args) &#123;        Adder adder = new Adder();        for (long ; i &lt; 1000; i++) &#123;            adder.addIncremental(i);        &#125;    &#125;&#125;</code></pre><p>Việc sử dụng kiểu Long trong đoạn code này là không cần thiết. Theo cơ chế autoboxing thì vòng lặp sẽ tạo ra 1000 Object<br>Long.<br>&#x3D;&gt; Nên sử dụng kiểu long<br>&#x3D;&gt; Cần phân biệt trường hợp sử dụng giữa primitive type và wrapper class. Hãy cố gắng sử dụng primitive type nhiều nhất<br>có thể.</p><h3 id="2-3-Unclosed-Resources"><a href="#2-3-Unclosed-Resources" class="headerlink" title="2.3, Unclosed Resources"></a>2.3, Unclosed Resources</h3><p>Khi mở kết nối tới database, hay mở file nhưng sau khi sử dụng xong lại không close kết nối chúng. Có thể do quên code<br>close, hoặc đặt code close sai vị trí. Việc này sẽ dẫn tới memory leak. Giải pháp là luôn luôn khai báo close resources<br>trong khối finally.</p><h3 id="2-4-Implement-phuong-thuc-equals-va-hashCode"><a href="#2-4-Implement-phuong-thuc-equals-va-hashCode" class="headerlink" title="2.4, Implement phương thức equals() và hashCode()"></a>2.4, Implement phương thức equals() và hashCode()</h3><pre><code class="java">public class Person &#123;    public String name;    public Person(String name) &#123;        this.name = name;    &#125;&#125;</code></pre><pre><code class="java">@Testpublic void givenMap_whenEqualsAndHashCodeNotOverridden_thenMemoryLeak()&#123;    Map&lt;Person, Integer&gt; map=new HashMap&lt;&gt;();    for(int i=0;i&lt;100;i++)&#123;    map.put(new Person(&quot;jon&quot;),1);    &#125;    Assert.assertFalse(map.size()==1);    &#125;</code></pre><p>Kkhi khai báo Map và Person làm key, lúc này Map sẽ không chấp nhận việc trùng key. Trong class Person này, đã không<br>implement 2 method là equals() và hashCode() &#x3D;&gt; dẫn tới Map không thể get ra được. làm tăng memory cho Map, khi insert<br>100 phần tử Object Person.</p><h3 id="2-5-Mot-vai-recommend"><a href="#2-5-Mot-vai-recommend" class="headerlink" title="2.5, Một vài recommend"></a>2.5, Một vài recommend</h3><ul><li>Nên sử dụng version Java mới nhất</li><li>Nếu phải làm việc với biến String lớn, cần khai báo tăng size cho PermGem để tránh báo lỗi OutOfMemoryErrors</li></ul><pre><code class="bash">-XX:MaxPermSize=512m</code></pre><ul><li>Sử dụng WeakHashMap để init cache thay vì HashMap như truyền thống. Với các cặp &lt;key,value&gt; trong WeakHashMap, nếu key<br>không bị tham chiếu bởi object nào thì cặp &lt;key,value&gt; đó sẽ được GC dọn dẹp.</li><li>CustomKey luôn đi kèm với tính Immutable</li></ul><h2 id="3-Tool-de-warning-monitor-memory-leak"><a href="#3-Tool-de-warning-monitor-memory-leak" class="headerlink" title="3. Tool để warning + monitor memory leak"></a>3. Tool để warning + monitor memory leak</h2><ul><li>Để cảnh báo: IDE eclipse…</li></ul><p><img src="https://www.baeldung.com/wp-content/uploads/2018/11/Eclipse-_Memor-_Leak-_Warnings.png" alt="tool1"></p><ul><li><p>Để phân tích: Java profilers<br>a) JProfiler<br><img src="https://www.baeldung.com/wp-content/uploads/2017/10/1-jprofiler-overview-probing.png" alt="tool2"></p><p>b) Java VisualVM<br><img src="https://www.baeldung.com/wp-content/uploads/2017/10/6-visualvm-overview.png" alt="tool3"></p><p>c) NetBeans Profiler<br><img src="https://www.baeldung.com/wp-content/uploads/2017/10/8-netbeans-telemetry-view.png" alt="tool4"></p></li></ul><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><ul><li><a href="https://reflectoring.io/create-analyze-heapdump/">https://reflectoring.io/create-analyze-heapdump/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> memory leak </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx - Config example</title>
      <link href="/2019/02/Other/Nginx_ConfigExample/"/>
      <url>/2019/02/Other/Nginx_ConfigExample/</url>
      
        <content type="html"><![CDATA[<h1 id="Tong-hop-cac-file-cau-hinh-NGINX-mau"><a href="#Tong-hop-cac-file-cau-hinh-NGINX-mau" class="headerlink" title="Tổng hợp các file cấu hình NGINX mẫu"></a>Tổng hợp các file cấu hình NGINX mẫu</h1><ul><li>File cấu hình mỗi site đặt tại <code>/etc/nginx/sites-enabled</code></li></ul><h3 id="1-File-normal"><a href="#1-File-normal" class="headerlink" title="1. File normal"></a>1. File normal</h3><pre><code class="bash"># /etc/nginx/sites-enabled/getlink.tungexplorer.meserver &#123;  listen 80;  server_name getlink.tungexplorer.me;  location / &#123;        proxy_pass http://127.0.0.1:8080;        proxy_http_version 1.1;        proxy_set_header Upgrade $http_upgrade;        proxy_set_header Connection $http_connection;        proxy_set_header Origin &#39;&#39;;  &#125;&#125;</code></pre><h3 id="2-File-co-basic-authen"><a href="#2-File-co-basic-authen" class="headerlink" title="2. File có basic authen"></a>2. File có basic authen</h3><ul><li>setting basic authen</li></ul><pre><code class="bash"># 1sudo apt-get install apache2-utils# 2# nginx = username, you can change it to user1, user2# /etc/nginx/.htpasswd = path store username + hash(pass)sudo htpasswd -c /etc/nginx/.htpasswd nginx# 3, verifycat /etc/nginx/.htpasswd</code></pre><ul><li>file server</li></ul><pre><code class="bash">server &#123;  listen 80;  server_name file.tungexplorer.me;  access_log off;  auth_basic            &quot;Restricted Access!&quot;;  auth_basic_user_file  /etc/nginx/.htpasswd;  location / &#123;    proxy_pass http://127.0.0.1:8082;    proxy_read_timeout 300;    proxy_connect_timeout 300;    proxy_redirect     off;    proxy_set_header   X-Forwarded-Proto $scheme;    proxy_set_header   Host              $http_host;    proxy_set_header   X-Real-IP         $remote_addr;  &#125;&#125;</code></pre><h3 id="3-File-regex"><a href="#3-File-regex" class="headerlink" title="3. File regex"></a>3. File regex</h3><pre><code>server &#123;  listen 80;  server_name   ~^(www\.)?[^.]+.tungexplorer.me$;  access_log off;  if ($host ~* ^(www\.)?([^.]+).tungexplorer.me$) &#123;    set $subdomain $2;  &#125;  resolver 8.8.8.8 valid=10s;  location / &#123;    proxy_pass http://$subdomain.fshare.vn;    proxy_read_timeout 300;    proxy_connect_timeout 300;    proxy_redirect     off;    proxy_set_header   X-Forwarded-Proto $scheme;    proxy_set_header   Host              $http_host;    proxy_set_header   X-Real-IP         $remote_addr;  &#125;&#125;</code></pre><h3 id="HTTPS"><a href="#HTTPS" class="headerlink" title="HTTPS"></a>HTTPS</h3><p><a href="https://viblo.asia/p/cau-hinh-ssl-https-voi-nginx-va-lets-enscrypt-3P0lP86blox">https://viblo.asia/p/cau-hinh-ssl-https-voi-nginx-va-lets-enscrypt-3P0lP86blox</a></p><pre><code>server &#123;  listen 443 ssl http2;  listen [::]:443 ssl http2;  server_name site.tungexplorer.me;  include snippets/tungexplorer.me.conf;  include snippets/ssl-params.conf;  location / &#123;        proxy_pass http://127.0.0.1:6258;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header Upgrade $http_upgrade;        proxy_set_header Connection &#39;upgrade&#39;;        proxy_cache_bypass $http_upgrade;  &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> template </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Annotation @GeneratedValue trong JPA</title>
      <link href="/2018/10/Java/JPA_GeneratedValue/"/>
      <url>/2018/10/Java/JPA_GeneratedValue/</url>
      
        <content type="html"><![CDATA[<h1 id="Annotation-GeneratedValue-trong-JPA"><a href="#Annotation-GeneratedValue-trong-JPA" class="headerlink" title="Annotation @GeneratedValue trong JPA"></a>Annotation @GeneratedValue trong JPA</h1><h2 id="1-GeneratedValue-la-gi"><a href="#1-GeneratedValue-la-gi" class="headerlink" title="1. @GeneratedValue là gì?"></a>1. @GeneratedValue là gì?</h2><p>Là 1 annotation của JPA, được sử dụng để đánh dấu cơ chế (cách thức) sinh ra ID trong database.</p><h2 id="2-Vay-co-bao-nhieu-co-che-cac-thuc"><a href="#2-Vay-co-bao-nhieu-co-che-cac-thuc" class="headerlink" title="2. Vậy có bao nhiêu cơ chế (các thức)?"></a>2. Vậy có bao nhiêu cơ chế (các thức)?</h2><p>Có 4 cơ chế:</p><ul><li>TABLE</li><li>SEQUENCE</li><li>IDENTITY</li><li>AUTO</li></ul><p>Trong đó:</p><ul><li>TABLE: sử dụng 1 table mặc định, hoặc developer tạo ra để làm cơ chế sinh ra giá trị ID. Hiện tại cơ chế này được<br>khuyến cáo là không nên sử dụng, vì cơ chế này đã cũ, và nó sẽ làm giảm performance của ứng dụng.</li><li>SEQUENCE: sử dụng 1 sequence mặc định, hoặc developer tạo ra 1 sequence trong database và khai báo, cho columnd ID<br>trong Entity. Từ sequence, developer có thể define được pool ID mà JPA có thể sinh ra, khi insert record vào db.</li><li>IDENTITY: đơn giản, dễ sử dụng nhất, và là cách nhanh nhất để developer tập chung cho business của ứng dụng, mà không<br>cần quan tâm nhiều. Tuy nhiên cơ chế này khuyến cáo là performance chưa thực sự hiệu quả. Khi sử dụng cơ chế IDENTITY,<br>thì giá trị của ID sẽ tự động tăng lên 1 đơn vị cho mỗi lần insert.</li><li>AUTO: tự động lựa chọn 1 trong 3 cơ chế bên trên.</li></ul><h2 id="3-Cu-phap-va-mot-so-dac-diem-cua-tung-co-che"><a href="#3-Cu-phap-va-mot-so-dac-diem-cua-tung-co-che" class="headerlink" title="3. Cú pháp, và một số đặc điểm của từng cơ chế"></a>3. Cú pháp, và một số đặc điểm của từng cơ chế</h2><h3 id="3-1-IDENTITY"><a href="#3-1-IDENTITY" class="headerlink" title="3.1 IDENTITY"></a>3.1 IDENTITY</h3><p>Đơn giản, dễ sử dụng nhất, chỉ cần khai báo</p><pre><code class="java">@Id@GeneratedValue(strategy = GenerationType.IDENTITY)private Long id;</code></pre><p>Mặc định giá trị ID của Entity được save vào DB, sẽ là giá trị ID lớn nhất đang có trong database cộng thêm một.<br>Có thể để ý query của IDE sinh ra khi chạy với IDENTITY, đó là query insert không truyền vào giá trị ID.</p><h3 id="3-2-SEQUENCE"><a href="#3-2-SEQUENCE" class="headerlink" title="3.2 SEQUENCE"></a>3.2 SEQUENCE</h3><p>Thường thì khi sử dụng cơ chế SEQUENCE, sẽ phải sử dụng kèm với 1 Annotation khác là @SequenceGenerator.<br>Tất nhiên không bắt buộc phải sử dụng kèm annotation này, khi đó JPA sẽ tự động sử dụng 1 sequence default của db. (cái<br>này là mặc định, hoặc developer có thể khai báo).<br>Khi sử dụng annotation @SequenceGenerator, sẽ phải quan tâm tới 6 thành phần sau:</p><ul><li>name</li><li>catalog</li><li>schema</li><li>sequenceName</li><li>initialValue</li><li>allocationSize</li></ul><p>Trong đó:</p><ul><li>initialValue: là giá trị khởi tạo ban đầu của sequence. Ví dụ có thể khai báo là 10001. Trong khi đó với IDENTITY thì<br>default giá trị khởi tạo ban đầu là 1.</li><li>allocationSize: là bước nhảy ID cho mỗi lần insert liền kề nhau.</li></ul><p>Cú pháp example:</p><pre><code class="java">@Id@GeneratedValue(strategy = GenerationType.SEQUENCE, generator = &quot;sequence_gen&quot;)@SequenceGenerator(name = &quot;sequence_gen&quot;, sequenceName = &quot;sequence&quot;, allocationSize = 2)private Long id;</code></pre><ul><li>sequenceName: tên của table muốn sử dụng.</li></ul><h3 id="3-3-TABLE"><a href="#3-3-TABLE" class="headerlink" title="3.3 TABLE"></a>3.3 TABLE</h3><ul><li>Tương tự như với SEQUENCE, thường đi kèm với annotation <code>@TableGenerator</code>. Vì cơ chế này khuyến cáo không nên sử dụng,<br>nên bài viết sẽ không đi chi tiết.</li></ul><h3 id="3-4-AUTO"><a href="#3-4-AUTO" class="headerlink" title="3.4 AUTO"></a>3.4 AUTO</h3><p>Đơn giản nhất, không phải khai báo gì</p><pre><code class="java">    @Id    @GeneratedValue    private Long id;</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jpa </tag>
            
            <tag> java </tag>
            
            <tag> GeneratedValue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java - Junit Test</title>
      <link href="/2018/10/Java/Junit_Test/"/>
      <url>/2018/10/Java/Junit_Test/</url>
      
        <content type="html"><![CDATA[<h1 id="Java-Junit-Test"><a href="#Java-Junit-Test" class="headerlink" title="Java - Junit Test"></a>Java - Junit Test</h1><h2 id="1-Junit-Annotation"><a href="#1-Junit-Annotation" class="headerlink" title="1. Junit Annotation"></a>1. Junit Annotation</h2><ul><li>Todo something before&#x2F;after something</li></ul><pre><code class="java">//  org.junit@BeforeClass@Before@After@AfterClass </code></pre><pre><code class="java">// org.junit.jupiter.api@BeforeEach@BeforeAll@AfterEach@AfterAll</code></pre><ul><li>Test</li></ul><pre><code class="java">@Test@RepeatTest(10)@ParameterizedTest</code></pre><ul><li>Group, Tag, Other…</li></ul><pre><code class="java">@Tag@Nested@Disabled@DisplayName@TempDir  </code></pre><h2 id="2-Assert-action"><a href="#2-Assert-action" class="headerlink" title="2. Assert action"></a>2. Assert action</h2><ul><li><p>Assert action</p><ul><li>assertEquals</li><li>assertFalse</li><li>assertNotNull</li><li>assertNotSame</li><li>assertNull</li><li>assertSame</li><li>assertTrue</li><li>isTrue</li><li>isFalse</li><li>isNotNull</li></ul></li><li><p>Assert for collection</p><ul><li>isEmpty</li><li>containsOnly</li><li>hasSameElementsAs</li><li>hasSize</li><li>containsExactlyInAnyOrder</li><li>contains</li><li>doesNotContain</li></ul></li><li><p>Assert Exception</p></li></ul><pre><code class="java">        assertThatCode(() -&gt; Mono.from(testee().clear(generateMailboxId())).block())            .doesNotThrowAnyException();</code></pre><pre><code class="java">        assertThatThrownBy(() -&gt; blobStore.read(DEFAULT_BUCKET, blobId))            .isInstanceOf(ObjectNotFoundException.class)</code></pre><pre><code class="java">        // expect message of exception        assertThatThrownBy(() -&gt; testee().addUser(TestFixture.INVALID_USERNAME, &quot;password&quot;))            .isInstanceOf(InvalidUsernameException.class)            .hasMessageContaining(&quot;should not contain any of those characters&quot;);</code></pre><ul><li>Assert softly</li></ul><pre><code class="java">        SoftAssertions.assertSoftly(softly-&gt;&#123;    softly.assertThat(imapUidDAO.retrieveAllMessages().collectList().block())    .containsExactlyInAnyOrder(MESSAGE_1);    softly.assertThat(Flux.from(pop3MetadataStore.listAllEntries()).collectList().block())    .containsExactlyInAnyOrder(new Pop3MetadataStore.FullMetadata(MAILBOX_ID,STAT_METADATA_1));    &#125;);</code></pre><h1 id="3-Mockito"><a href="#3-Mockito" class="headerlink" title="3. Mockito"></a>3. Mockito</h1><ul><li>Manual way</li></ul><pre><code class="java">BlobStoreDAO blobStoreDAO = mock(BlobStoreDAO.class);when(blobStoreDAO.listBlobs(DEFAULT_BUCKET)).thenReturn(Flux.just(blobId));when(blobStoreDAO.delete(DEFAULT_BUCKET, blobId)).thenThrow(new RuntimeException(&quot;test&quot;));</code></pre><ul><li>Annotation way</li></ul><pre><code class="java">@InjectMocks@Mock//@InjectMocks: require MockitoJUnitRunner </code></pre><h2 id="4-RestAPI-Testing"><a href="#4-RestAPI-Testing" class="headerlink" title="4. RestAPI Testing"></a>4. RestAPI Testing</h2><h3 id="Using-io-rest-assured"><a href="#Using-io-rest-assured" class="headerlink" title="Using io.rest-assured"></a>Using <code>io.rest-assured</code></h3><ul><li><a href="https://mvnrepository.com/artifact/io.rest-assured/rest-assured">io.rest-assured</a></li></ul><pre><code class="xml">&lt;dependency&gt;    &lt;groupId&gt;io.rest-assured&lt;/groupId&gt;    &lt;artifactId&gt;rest-assured&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><ol><li>Build base</li></ol><pre><code class="java">    public static RestAssuredConfig defaultConfig() &#123;        return newConfig().encoderConfig(encoderConfig().defaultContentCharset(StandardCharsets.UTF_8));    &#125;    RestAssured.requestSpecification  = new RequestSpecBuilder()                .setContentType(ContentType.JSON)                .setAccept(ContentType.JSON)                .setConfig(defaultConfig())                .setPort(port)                .setBasePath(&quot;/&quot;)                .log(LogDetail.ALL)                .build();</code></pre><ol start="2"><li>Using</li></ol><ul><li>Check error (ex: 404)</li><li>Error response is Json</li></ul><pre><code class="java">import static org.assertj.core.api.Assertions.assertThat;Map&lt;String, Object&gt; errors =             when()                .get(&quot;/user/myuser1&quot;)            .then()                .statusCode(NOT_FOUND_404)                .contentType(JSON)                .extract()                .body()                .jsonPath()                .getMap(&quot;.&quot;);            assertThat(errors)                .containsEntry(&quot;statusCode&quot;, NOT_FOUND_404)                .containsEntry(&quot;type&quot;, ERROR_TYPE_NOTFOUND)                .containsEntry(&quot;message&quot;, &quot;Invalid get on user&quot;);</code></pre><ul><li>Check value of body</li></ul><pre><code class="java">                given()                    .basePath(&quot;/basepath/&quot;)                .when()                    .get(taskId + &quot;/await&quot;)                .then()                    .statusCode(HttpStatus.OK)                    .body(&quot;status&quot;, Matchers.is(&quot;completed&quot;))                    .body(&quot;taskId&quot;, Matchers.is(notNullValue()))                    .body(&quot;type&quot;, Matchers.is(ClearMailboxContentTask.TASK_TYPE.asString()))                    .body(&quot;startedDate&quot;, Matchers.is(notNullValue()))                    .body(&quot;submitDate&quot;, Matchers.is(notNullValue()))                    .body(&quot;completedDate&quot;, Matchers.is(notNullValue()))                    .body(&quot;additionalInformation.username&quot;, Matchers.is(USERNAME.asString()));</code></pre><ul><li>Get value of one property</li></ul><pre><code class="java">            String taskId = when()                .delete(MAILBOX_NAME + &quot;/messages&quot;)            .then()                .statusCode(CREATED_201)                .extract()                .jsonPath()                .get(&quot;taskId&quot;);            assertThat(taskId)                .isNotEmpty();</code></pre><ul><li>Compare json</li></ul><pre><code class="java">val response: String = `given`      .body(&quot;json value here&quot;)    .when()      .post()    .`then`      .statusCode(HttpStatus.SC_OK)      .contentType(JSON)      .extract()      .body()      .asString()    assertThatJson(response)      .isEqualTo(        s&quot;&quot;&quot;           |&#123;           |    &quot;sessionState&quot;: &quot;$&#123;SESSION_STATE.value&#125;&quot;,           |    &quot;methodResponses&quot;: [           |        [           |            &quot;Email/send&quot;,           |            &#123;           |                &quot;accountId&quot;: &quot;$ACCOUNT_ID&quot;,           |                &quot;newState&quot;: &quot;$$&#123;json-unit.ignore&#125;&quot;,           |                &quot;created&quot;: &#123;           |                    &quot;K87&quot;: &#123;           |                        &quot;emailSubmissionId&quot;: &quot;$$&#123;json-unit.ignore&#125;&quot;,           |                        &quot;emailId&quot;: &quot;$$&#123;json-unit.ignore&#125;&quot;,           |                        &quot;blobId&quot;: &quot;$$&#123;json-unit.ignore&#125;&quot;,           |                        &quot;threadId&quot;: &quot;$$&#123;json-unit.ignore&#125;&quot;,           |                        &quot;size&quot;: &quot;$$&#123;json-unit.ignore&#125;&quot;           |                    &#125;           |                &#125;           |            &#125;,           |            &quot;c1&quot;           |        ]           |    ]           |&#125;&quot;&quot;&quot;.stripMargin)</code></pre><pre><code>- `$$&#123;json-unit.ignore&#125;` for ignore compare value</code></pre><ul><li>Set queryParam for request</li></ul><pre><code class="java">        given()            .queryParam(&quot;task&quot;, &quot;purge&quot;)            .queryParam(&quot;olderThan&quot;, &quot;15days&quot;)            .post()        .then()            .statusCode(HttpStatus.CREATED_201)            .body(&quot;taskId&quot;, notNullValue());</code></pre><h2 id="5-ConditionFactory"><a href="#5-ConditionFactory" class="headerlink" title="5. ConditionFactory"></a>5. ConditionFactory</h2><pre><code class="java">    ConditionFactory CALMLY_AWAIT=Awaitility    .with().pollInterval(ONE_HUNDRED_MILLISECONDS)    .and().pollDelay(ONE_HUNDRED_MILLISECONDS)    .await()    .atMost(TEN_SECONDS);    @Test    void testConditionFactory()&#123;        CALMLY_AWAIT.untilAsserted(()-&gt;&#123;        Random random=new Random();        assertThat(random.nextInt(10))        .isEqualTo(7);        &#125;);        &#125;    </code></pre><h2 id="6-RegisterExtension"><a href="#6-RegisterExtension" class="headerlink" title="6. RegisterExtension"></a>6. RegisterExtension</h2><ul><li>Pojo</li></ul><pre><code class="java">class CombinedTestSystem &#123;    private final boolean supportVirtualHosting;    private final SimpleDomainList domainList;    private final Username userAlreadyInLDAP;    private final Username userAlreadyInLDAP2;    private final Username userWithUnknowDomain;    private final Username invalidUsername;    public CombinedTestSystem(boolean supportVirtualHosting) throws Exception &#123;        // todo    &#125;    private Username toUsername(String login) &#123;        return toUsername(login, DOMAIN);    &#125;&#125;</code></pre><ul><li>Extension</li></ul><pre><code class="java">class CombinedUserRepositoryExtension implements BeforeEachCallback, ParameterResolver &#123;    private final boolean supportVirtualHosting;    private CombinedTestSystem combinedTestSystem;    private CombinedUserRepositoryExtension(boolean supportVirtualHosting) &#123;        this.supportVirtualHosting = supportVirtualHosting;    &#125;    @Override    public void beforeEach(ExtensionContext extensionContext) throws Exception &#123;        combinedTestSystem = new CombinedTestSystem(supportVirtualHosting);    &#125;    @Override    public boolean supportsParameter(ParameterContext parameterContext, ExtensionContext extensionContext) throws ParameterResolutionException &#123;        return parameterContext.getParameter().getType() == CombinedTestSystem.class;    &#125;    @Override    public Object resolveParameter(ParameterContext parameterContext, ExtensionContext extensionContext) throws ParameterResolutionException &#123;        return combinedTestSystem;    &#125;    public boolean isSupportVirtualHosting() &#123;        return supportVirtualHosting;    &#125;&#125;</code></pre><ul><li>Using</li></ul><pre><code class="java">@RegisterExtensionCombinedUserRepositoryExtension combinedExtension=CombinedUserRepositoryExtension...@BeforeEach  void setUp(CombinedTestSystem testSystem)&#123;      //todo      &#125;@Test  void test1(CombinedTestSystem testSystem)&#123;      // todo      &#125;</code></pre><h2 id="7-ParameterizedTest"><a href="#7-ParameterizedTest" class="headerlink" title="7. ParameterizedTest"></a>7. ParameterizedTest</h2><pre><code class="java">    static Stream&lt;Arguments&gt; storageStrategies()&#123;    return Stream.of(    Arguments.of(DEDUPLICATION_STRATEGY),    Arguments.of(PASSTHROUGH_STRATEGY)    );    &#125;@ParameterizedTest@MethodSource(&quot;storageStrategies&quot;)    void test(BlobStoreConfiguration blobStoreConfiguration)&#123;        // todo something        &#125;</code></pre><h2 id="8-Compare-Json"><a href="#8-Compare-Json" class="headerlink" title="8. Compare Json"></a>8. Compare Json</h2><pre><code class="java">import net.javacrumbs.jsonunit.assertj.JsonAssertions.assertThatJson assertThatJson(response).isEqualTo(     s&quot;&quot;&quot;&#123;         |  &quot;sessionState&quot;:&quot;$&#123;SESSION_STATE.value&#125;&quot;,         |  &quot;methodResponses&quot;: [         |    [&quot;error&quot;, &#123;         |      &quot;type&quot;: &quot;unknownMethod&quot;,         |      &quot;description&quot;: &quot;Missing capability(ies)&quot;         |    &#125;,&quot;c1&quot;]         |  ]         |&#125;&quot;&quot;&quot;.stripMargin)     //     assertThatJson(response)     .whenIgnoringPaths(&quot;methodResponses[0][1].description&quot;)     .isEqualTo(...)     //     assertThatJson(response)     .inPath(&quot;methodResponses[0][1]&quot;)     .isEqualTo(...)</code></pre><h3 id="Assert-List"><a href="#Assert-List" class="headerlink" title="Assert List"></a>Assert List</h3><pre><code class="java"> assertThat(loggingEvents.list).hasSize(1)                .allSatisfy(loggingEvent -&gt; &#123;                    assertThat(loggingEvent.getLevel()).isEqualTo(Level.ERROR);                    assertThat(loggingEvent.getFormattedMessage()).contains(&quot;Trace record&quot;, &quot;LeakAwareTest#leakDetectionShouldLogTraceRecordWhenLevelIsAdvanced&quot;);                &#125;);</code></pre><h3 id="Timeout"><a href="#Timeout" class="headerlink" title="Timeout"></a>Timeout</h3><pre><code>  @Test  @Timeout(180)  def apiRequestsShouldBeProcessed</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> junit </tag>
            
            <tag> mockito </tag>
            
            <tag> unit test </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
